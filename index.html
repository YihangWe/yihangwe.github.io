<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yihangwe.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
<meta property="og:type" content="website">
<meta property="og:title" content="EthanWeee">
<meta property="og:url" content="https://yihangwe.github.io/index.html">
<meta property="og:site_name" content="EthanWeee">
<meta property="og:description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Yihang Wei">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yihangwe.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>EthanWeee</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EthanWeee</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/04/DynamoDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/04/DynamoDB/" class="post-title-link" itemprop="url">DynamoDB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-04 00:00:00" itemprop="dateCreated datePublished" datetime="2024-10-04T00:00:00-07:00">2024-10-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-30 19:49:34" itemprop="dateModified" datetime="2025-05-30T19:49:34-07:00">2025-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h1><p>Dynamo has the ability of incremental scalability and predictable high performance, but it carries the operational complexity of self-managed large database systems.</p>
<p>SimpleDB is easy to administrate a cloud service, consistency, and a table-based data model, but it has limitations that tables have a small capacity in terms of storage and of request throughput, and that a unpredictable query and write latency.</p>
<p>DynamoDB &#x3D; Dynamo + SImpleDB</p>
<p><strong>Architecture:</strong></p>
<p><img src="/../images/dynamodb_arch.png" alt="img"></p>
<ol>
<li><strong>DynamoDB Tables and Primary Keys</strong>: A DynamoDB table is a collection of items, each uniquely identified by a primary key. The primary key can be a partition key or a composite key (partition key + sort key).</li>
<li><strong>Secondary Indexes</strong>: DynamoDB supports secondary indexes, allowing queries using alternate keys in addition to the primary key, enhancing query capabilities.</li>
<li><strong>Partitions and Replication Groups</strong>: DynamoDB tables are divided into multiple partitions, with each partition managing a contiguous key range. Each partition has multiple replicas across different Availability Zones for high availability and durability. A <strong>Multi-Paxos</strong> consensus is used for leader election within the replication group, and the leader handles writes and strongly consistent reads.</li>
<li><strong>Write-Ahead Logs and Consistency</strong>: The leader replica generates a write-ahead log for write requests. A write is acknowledged once a quorum of replicas persists the log. DynamoDB supports both strongly consistent and eventually consistent reads.</li>
<li><strong>Failure Detection and Leader Election</strong>: If the current leader is detected as unhealthy, other replicas initiate a new election. The new leader can only start serving writes or consistent reads after the old leader’s lease expires.</li>
<li><strong>Autoadmin Service</strong>: The autoadmin service monitors the health of the fleet and partitions, scaling tables and replacing unhealthy replicas or hardware to maintain system stability. It automatically detects and resolves issues, ensuring a stable and healthy infrastructure.</li>
</ol>
<p><strong>Journey from provisioned to on-demand:</strong></p>
<ul>
<li><p><strong>Bursting:</strong> To address the issue of uneven workload distribution across partitions, DynamoDB introduced the concept of <strong>bursting</strong>. Bursting allows an application to utilize unused capacity at the partition level when its provisioned throughput is exhausted, helping to <strong>handle short-term spikes</strong> in workload. DynamoDB retains unused capacity in a partition for <strong>up to 300 seconds</strong>, which can be tapped into when the consumed capacity exceeds the provisioned capacity. This reserved capacity is referred to as <strong>burst capacity</strong>.</p>
<p><strong>How It Works</strong>: DynamoDB manages throughput using multiple <strong>token buckets</strong>:</p>
<ul>
<li><p><strong>Each partition has two token buckets</strong>: one for allocated capacity and another for burst capacity. <strong>Each storage node has a token bucket</strong> that controls the overall load across partitions hosted on that node.</p>
</li>
<li><p>When a read or write request arrives at a storage node, the system first checks the partition’s token bucket. If the allocated capacity has been exhausted, burst capacity can be used, but only if there are available tokens at both the burst token level and the node level.</p>
</li>
<li><p><strong>Additional Check for Write Requests</strong>: When using burst capacity for write requests, an additional check is performed to ensure that other replica nodes for the partition also have sufficient capacity. This ensures that the write operation can be completed safely and consistently across all replicas. The leader replica periodically gathers information about the node-level capacity of other members in the replication group to facilitate this process.</p>
</li>
</ul>
</li>
<li><p><strong>Adaptive (deprecated):</strong> DynamoDB launched adaptive capacity to better absorb <strong>longlived</strong> spikes that cannot be absorbed by the burst capacity. Adaptive capacity allowed DynamoDB to better absorb workloads that had heavily skewed access patterns across partitions. Adaptive capacity actively monitored the provisioned and consumed capacity of all the tables.</p>
<ul>
<li><p>If a table experienced throttling and the table level throughput was not exceeded, then it would automatically increase (boost) the allocated throughput of the partitions of the table using a proportional control algorithm.</p>
</li>
<li><p>If the table was consuming more than its provisioned capacity then capacity of the partitions which received the boost would be decreased. The autoadmin system ensured that partitions receiving boost were relocated to an appropriate node that had the capacity to serve the increased throughput.</p>
</li>
</ul>
</li>
<li><p>GAC, how does it work:</p>
<ul>
<li><p><strong>Global Throughput Tracking and Management</strong></p>
<ul>
<li><p><strong>Global Token Management</strong>: GAC uses a <strong>token bucket system</strong> to manage the overall throughput (RCUs and WCUs) of a DynamoDB table.</p>
</li>
<li><p><strong>Token Buckets</strong>: Each request router maintains a <strong>local token bucket</strong> to handle requests. When tokens are depleted locally, the router requests more tokens from GAC, which manages the global distribution of these tokens across partitions.</p>
</li>
</ul>
</li>
<li><p><strong>Dynamic Token Allocation</strong></p>
<ul>
<li><p><strong>Periodic Replenishment</strong>: GAC regularly communicates with the request routers every few seconds to replenish their token buckets. The amount of tokens allocated is based on the overall resource consumption of the table, particularly when certain partitions are experiencing high traffic.</p>
</li>
<li><p><strong>Handling Hot Partitions</strong>: When specific partitions become hot, GAC dynamically allocates additional tokens to those partitions.</p>
</li>
</ul>
</li>
<li><p><strong>Capacity Limits and Isolation</strong></p>
<ul>
<li><p><strong>Global Throughput Limits</strong>: GAC ensures that the total number of tokens allocated to partitions does not exceed the <strong>provisioned capacity</strong> for the entire table.</p>
</li>
<li><p><strong>Node-Level Limits</strong>: Although GAC allocates tokens globally, each partition is subject to the <strong>maximum throughput capacity of its storage node</strong>. This ensures that no single partition can consume more than its node’s allowable resources.</p>
</li>
</ul>
</li>
<li><p><strong>Stateless and Distributed Design</strong></p>
<ul>
<li><p><strong>Stateless Operation</strong>: GAC operates in a <strong>stateless manner</strong>, meaning it calculates token allocations in real-time based on incoming client requests. It doesn’t rely on long-term stored states, so GAC servers can be restarted or stopped without affecting the system’s overall operation.</p>
</li>
<li><p><strong>Distributed Architecture</strong>: GAC uses a distributed architecture, where multiple GAC instances coordinate using a <strong>hash ring</strong>. This allows GAC to scale horizontally and handle requests from multiple routers efficiently.</p>
</li>
</ul>
</li>
<li><p><strong>Defense-in-Depth with Partition-Level Token Buckets</strong></p>
<ul>
<li><p><strong>Partition-Level Control</strong>: Even though GAC manages tokens globally, DynamoDB still retains <strong>partition-level token buckets</strong> for additional protection. These buckets ensure that no single partition consumes excessive resources, offering a secondary layer of isolation and control.</p>
</li>
<li><p><strong>Resource Isolation</strong>: Partition-level token buckets prevent any single application or partition from monopolizing the resources of the storage node.</p>
</li>
</ul>
</li>
<li><p><strong>Token Consumption and Replenishment Process</strong></p>
<ul>
<li><p>When a request is made, the request router checks its local token bucket for available tokens. If enough tokens are present, the request is processed.</p>
</li>
<li><p>If the local tokens are depleted, the request router asks GAC for more tokens.</p>
</li>
<li><p>GAC calculates the global consumption of tokens for the table and allocates more tokens to the router based on overall resource usage.</p>
</li>
<li><p>Once tokens are used up or expire, the process repeats, with the router requesting new tokens from GAC.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Proactive load balancing mechanism</p>
<ul>
<li><p>Independent Monitoring: Each storage node independently monitors the total throughput (read&#x2F;write requests) and data size of all the partition replicas it hosts.</p>
</li>
<li><p>Threshold Detection: When the throughput or data size of a partition replica approaches or exceeds a predefined threshold of the node’s capacity, that partition replica is flagged as a candidate for migration.</p>
</li>
<li><p>Reporting to Autoadmin Service: The storage node reports the list of over-utilized partition replicas to the Autoadmin service, which manages the load balancing process.</p>
</li>
<li><p>Automatic Migration: The Autoadmin service finds a new storage node, usually located in a different Availability Zone, that can accommodate the migrating partition replica. This new node must have enough spare capacity to handle the increased load.</p>
</li>
<li><p>GAC 和 bursting 都擅长处理 短期或临时的高负载，但如果某个分区长期处于高负载状态（例如一个分区持续有热点键），这些机制可能无法完全消除该分区对特定节点的影响。在这种情况下，自动迁移分区副本是更长期有效的解决方案。</p>
</li>
</ul>
</li>
<li><p>Even with GAC and partition bursting capacity, DynamoDB tables may still experience throttling if traffic is heavily concentrated on a specific set of items. When the throughput for a partition exceeds a certain threshold, the system splits the partition according to the observed key distribution, rather than simply splitting the key range in the middle. These smaller partitions are typically distributed to different storage nodes. However, some workloads may not benefit from this mechanism, such as:</p>
<ul>
<li><p>A partition where traffic is concentrated on a single item.</p>
</li>
<li><p>A partition where the key range is accessed sequentially.</p>
</li>
</ul>
</li>
<li><p>DynamoDB’s <strong>on-demand tables</strong> eliminate the need for customers to manually set throughput. The system automatically adjusts resources based on actual read and write requests, enabling it to quickly adapt to sudden traffic increases. Specifically:</p>
<ul>
<li><p>DynamoDB automatically scales up to <strong>twice the previous peak traffic</strong> to handle more requests instantly.</p>
</li>
<li><p>If traffic continues to increase, DynamoDB further allocates more resources to prevent throttling and maintain performance.</p>
</li>
</ul>
<p>The scaling mechanism for on-demand tables is achieved through <strong>partition splitting</strong>, where partitions are split based on traffic patterns to ensure each partition has sufficient resources. At the same time, <strong>GAC (Global Admission Control)</strong> monitors the system to prevent any single application from consuming too many resources, maintaining overall system stability.</p>
</li>
</ul>
<p><strong>机制之间的关联总结：</strong></p>
<ul>
<li><strong>On-demand tables</strong> 依赖 <strong>GAC</strong> 和 <strong>bursting</strong> 来动态扩展资源，处理流量波动。</li>
<li><strong>GAC</strong> 管理整个系统的全局资源分配，确保突发和按需扩展时不影响其他应用，同时在必要时与 <strong>proactive load balancing</strong> 机制配合，进行分区迁移。</li>
<li><strong>Bursting</strong> 提供短期解决方案，而当负载持续增加时，系统会通过 <strong>主动负载平衡</strong> 来长期调整资源分配，防止系统瓶颈。</li>
</ul>
<p><strong>Durability and correctness</strong></p>
<p><strong>Hardware failures</strong></p>
<p>The write-ahead logs in DynamoDB are crucial for ensuring data durability and crash recovery. Each partition has three replicas that store the write-ahead logs. To enhance durability, the logs are periodically archived to Amazon S3. The unarchived logs typically amount to a few hundred megabytes.</p>
<p>In large-scale systems, hardware failures such as memory or disk failures are common. When a node fails, all replication groups hosted on that node are reduced to two copies. The process of repairing a storage replica can take several minutes, as it involves copying both the B-tree and the write-ahead logs.</p>
<p>When the system detects an unhealthy replica, the leader of the replication group adds a log replica to ensure data durability is not compromised. Since only the recent write-ahead logs need to be copied without the B-tree, adding the log replica takes just a few seconds. This quick addition helps restore the affected replication group, ensuring that the most recent writes remain highly durable.</p>
<p><strong>Silent data errors</strong></p>
<p>Hardware failures can cause incorrect data storage: In DynamoDB, errors may occur due to issues with storage media, CPU, or memory, and these errors are often difficult to detect.</p>
<p>Extensive use of checksums: DynamoDB maintains checksums for every log entry, message, and log file to detect silent errors and ensure data integrity during each data transfer. When messages are transmitted between nodes, checksums verify whether errors occurred during transmission.</p>
<p>Log archiving and validation: Each log file archived to S3 has a manifest that records details such as the table, partition, and data markers. Before uploading, the archiving agent performs various checks, including checksum validation, verifying that the log belongs to the correct table and partition, and ensuring that there are no gaps in the sequence numbers.</p>
<p>Multiple replica log archiving: Log archiving agents run on all three replicas. If one agent finds that a log file has already been archived, it downloads the file and compares it with the local write-ahead log to verify data integrity.</p>
<p>Checksum validation during S3 upload: Every log file and manifest file is uploaded to S3 with a content checksum. S3 verifies this checksum during the upload process to catch any errors in data transmission.</p>
<p><strong>Continuous verification</strong></p>
<p>Continuous Data Integrity Verification: DynamoDB continuously verifies data at rest to detect silent data errors and bit rot, which can occur due to hardware failures or data corruption. This is a critical defense mechanism for maintaining data reliability.</p>
<p>Scrub Process: The scrub process is central to detecting unforeseen errors. It checks two main aspects:</p>
<ul>
<li><strong>Replica Consistency</strong>: Ensures that all three replicas in a replication group have identical data.</li>
<li><strong>Archived Log Reconstruction</strong>: Rebuilds an offline replica using archived write-ahead logs from S3 and verifies that it matches the live replica.</li>
</ul>
<p>Verification Mechanism: Scrub computes checksums for the live replicas and compares them with those generated from replicas built using archived logs.</p>
<p>Defense in Depth: This mechanism ensures that live storage replicas and those rebuilt from historical logs remain consistent, providing confidence in the system’s integrity and reliability.</p>
<p><strong>Backups and restores</strong></p>
<p>Backup and Restore Mechanism: DynamoDB supports backup and restore to protect against logical corruption caused by bugs in customer applications. Backups and restores are built using write-ahead logs stored in S3 and do not affect table performance or availability.</p>
<p>Backup Consistency: Backups are full copies of DynamoDB tables, consistent across multiple partitions to the nearest second, and stored in Amazon S3. Data can be restored to a new DynamoDB table at any time.</p>
<p>Point-in-Time Restore: DynamoDB supports point-in-time restore, allowing customers to restore a table to any point within the last 35 days. This feature creates periodic snapshots of table partitions and stores them in S3.</p>
<p>Snapshots and Write-Ahead Logs: For point-in-time restore, DynamoDB identifies the closest snapshots to the requested time, applies the corresponding write-ahead logs, and restores the table to the desired state.</p>
<p><strong>Availability</strong></p>
<p><strong>Write and consistent read availability</strong></p>
<p>Write Availability: DynamoDB partition write availability depends on having a healthy leader and a healthy write quorum. A write quorum in DynamoDB requires two out of three replicas across different Availability Zones (AZs) to be healthy.</p>
<p>Handling Write Quorum Failures: <strong>If one replica becomes unresponsive, the leader adds a log replica, which is the fastest way to meet the quorum requirement and minimize write disruptions caused by an unhealthy quorum.</strong></p>
<p>Consistent Reads: Consistent reads are served by the leader replica. <strong>If the leader fails, other replicas detect the failure and elect a new leader to minimize disruptions to consistent read availability.</strong></p>
<p>Impact of Log Replicas: The introduction of log replicas was a significant system change. The use of the formally proven Paxos protocol provided confidence to safely implement this change, increasing system availability. DynamoDB can run millions of Paxos groups with log replicas in a single region.</p>
<p>Eventually Consistent Reads: Eventually consistent reads can be served by any of the replicas.</p>
<p><strong>Failure detection</strong></p>
<p>New Leader Waits for Lease Expiry: A newly elected leader must wait for the old leader’s lease to expire before handling traffic, causing a few seconds of disruption where no new writes or consistent reads can be processed.</p>
<p>Importance of Leader Failure Detection: Quick and robust leader failure detection is crucial for minimizing disruptions. False positives in failure detection can lead to unnecessary leader elections, further disrupting availability.</p>
<p>Impact of Gray Network Failures: Gray network failures, such as communication issues between nodes or routers, can result in false or missed failure detections. These failures can trigger unnecessary leader elections, causing availability interruptions.</p>
<p>Improved Failure Detection Algorithm: To address the availability issues caused by gray failures, DynamoDB’s failure detection algorithm was improved. <strong>When a follower attempts to trigger a failover, it first checks with other replicas to see if they can still communicate with the leader. If they report the leader is healthy, the follower cancels the failover attempt.</strong> This change significantly reduced false leader elections and minimized availability disruptions.</p>
<p><strong>Metadata availability</strong></p>
<p>Metadata Needs for Request Routers: DynamoDB’s request routers require metadata mapping between table primary keys and storage nodes. Initially, this metadata was stored in DynamoDB, and the routers cached it locally. Although the cache hit rate was high, cache misses or cold starts caused metadata lookup traffic spikes, potentially destabilizing the system.</p>
<p>Caching Challenges: When caches failed or during cold starts, request routers frequently queried the metadata service, putting immense pressure on it and leading to cascading failures in other parts of the system.</p>
<p>Introduction of MemDS: <strong>To reduce reliance on local caches, DynamoDB introduced MemDS, a distributed in-memory data store for storing and replicating metadata.</strong> MemDS scales horizontally to handle all incoming requests and stores data in a compressed format. It uses a Perkle tree structure, combining Patricia and Merkle tree features for efficient key lookups and range queries.</p>
<p>Perkle Tree Operations: MemDS supports efficient key lookups, range queries, and special operations like floor (find the largest key ≤ given key) and ceiling (find the smallest key ≥ given key) for metadata retrieval.</p>
<p>New Partition Map Cache: DynamoDB implemented a new cache on request routers, addressing the issues of bimodal behavior. Even when a cache hit occurs, an asynchronous call is made to MemDS to refresh the cache. This ensures that MemDS consistently handles a steady volume of traffic, preventing reliance on cache hit ratios and avoiding cascading failures when caches become ineffective.</p>
<p>Partition Membership Updates: DynamoDB storage nodes, the authoritative source of partition membership data, push updates to MemDS. If a request router queries an incorrect storage node due to outdated information, the node provides updated membership data or triggers a new MemDS lookup.</p>
<p><strong>Transactions:</strong></p>
<p><img src="/../images/dynamodb_txn.png" alt="img"></p>
<p><strong>Request Router (RR)</strong>:</p>
<ul>
<li>The <strong>Request Router</strong> is the first major component that handles incoming requests after they pass through the network.</li>
<li><strong>Authentication and Authorization</strong>: RR typically interacts with an <strong>Authentication System</strong> to ensure that the request is valid and the user has the proper permissions to access or modify the data.</li>
<li><strong>Routing Requests</strong>: Once a request is authenticated, the RR determines which <strong>Storage Nodes</strong> the request should be forwarded to. It uses the <strong>Metadata System</strong> to map the key(s) involved in the request to the correct storage nodes, as the data is distributed across many nodes.</li>
<li><strong>Forwarding Requests</strong>: Depending on whether the operation is a simple read&#x2F;write or part of a larger transaction, the RR may route the request directly to storage nodes or to the Transaction Coordinator.</li>
</ul>
<p><strong>Transaction Coordinator (TC)</strong>:</p>
<ul>
<li>The <strong>Transaction Coordinator</strong> plays a central role in handling transactions that span multiple items or storage nodes.</li>
<li><strong>Transaction Management</strong>: For requests that involve multiple storage nodes or require consistency (e.g., multi-item writes in a transaction), the RR forwards the request to the TC. The TC is responsible for breaking down the transaction into individual operations and coordinating these operations across the necessary storage nodes.</li>
<li><strong>Distributed Transaction Execution</strong>: The TC ensures that the operations follow the appropriate protocol (e.g., two-phase commit) to guarantee atomicity and consistency, ensuring that all parts of the transaction are either completed successfully or rolled back.</li>
<li><strong>Timestamp Assignment and Conflict Resolution</strong>: In a timestamp-based system like DynamoDB, the TC may assign timestamps to ensure the correct ordering of operations and manage any potential conflicts between concurrent transactions.</li>
</ul>
<p>In summary:</p>
<ul>
<li><strong>Request Router (RR)</strong> handles initial authentication and routing of requests to the appropriate storage nodes or transaction coordinator.</li>
<li><strong>Transaction Coordinator (TC)</strong> manages distributed transactions, ensuring data consistency and handling multi-node operations.</li>
</ul>
<p><strong>Programming Interface:</strong></p>
<ol>
<li><p><strong>Key-Value Store</strong></p>
<p>DynamoDB allows users to create tables that can grow almost indefinitely. Each table is a collection of items, and each item is a collection of attributes. Each item is uniquely identified by a primary key, ensuring uniqueness within the table. DynamoDB provides a simple interface to store or retrieve items from a table or an index.</p>
</li>
<li><p><strong>Read and Write Operations</strong></p>
<p>DynamoDB operates as a key-value store, and the most common operations used by applications involve reading and writing data. These operations include:</p>
<ul>
<li><p><strong>GetItem</strong>: Retrieves an item with a given primary key.</p>
</li>
<li><p><strong>PutItem</strong>: Inserts a new item or replaces an existing one.</p>
</li>
<li><p><strong>UpdateItem</strong>: Updates an existing item, or adds it if it doesn’t exist.</p>
</li>
<li><p><strong>DeleteItem</strong>: Deletes an item from the table based on the primary key.</p>
</li>
</ul>
<p>These last three operations (PutItem, UpdateItem, and DeleteItem) are collectively referred to as writes. A write operation can optionally include conditions that must be satisfied for the operation to be executed successfully. For instance, you could specify that a PutItem operation should only succeed if the item doesn’t already exist.</p>
</li>
<li><p><strong>Transactional Operations</strong></p>
<p>DynamoDB supports transactions through two key operations:</p>
<ul>
<li><p><strong>TransactGetItems</strong>: Used for reading multiple items atomically. It retrieves the latest versions of items from one or more tables at a single point in time, ensuring consistency. If any conflicting operation is modifying an item that’s being read, the transaction will be rejected.</p>
</li>
<li><p><strong>TransactWriteItems</strong>: This is used for performing atomic writes across multiple items and tables. It allows you to create, update, or delete multiple items in one or more tables within a single atomic transaction. This ensures that either all changes happen, or none do. The operation is synchronous and idempotent (meaning it can be retried without causing duplicate effects). TransactWriteItems can include conditions on the current values of the items, and the operation is rejected if these conditions aren’t met.</p>
</li>
</ul>
</li>
</ol>
<p>Transaction execution:</p>
<ol>
<li><p><strong>Transaction Routing</strong></p>
<p>All operation requests first reach a set of frontend hosts known as request routers. These routers are responsible for authenticating the requests and routing them to the appropriate storage nodes. Storage nodes are mapped based on key ranges. For transaction management, the routers forward transaction operations to transaction coordinators.</p>
<p>Transaction coordinators break down the transaction into multiple operations targeting different items and coordinate the execution of these operations across the storage nodes using a distributed protocol.</p>
</li>
<li><p><strong>Timestamp Ordering</strong></p>
<p>Each transaction is assigned a timestamp that defines its logical execution order. Multiple transaction coordinators operate in parallel, and different coordinators assign timestamps to different transactions. As long as transactions execute in the assigned order, serializability is maintained.</p>
<p>The storage nodes are responsible for ensuring that the operations on the items they manage are executed in the correct order and rejecting transactions that cannot be properly ordered.</p>
</li>
<li><p><strong>Write Transaction Protocol</strong></p>
<p>DynamoDB uses a two-phase commit protocol to ensure that the write operations within a transaction are atomic and executed in the correct order. In the prepare phase, the coordinator prepares all the write operations. If all storage nodes accept the operations, the transaction is committed; otherwise, it is canceled.</p>
<p>The storage nodes record the timestamp and metadata of each item involved in the transaction to ensure the transaction is handled correctly.</p>
</li>
<li><p><strong>Read Transaction Protocol</strong></p>
<p>Read transactions also use a two-phase protocol, but it differs from the write transaction protocol. DynamoDB designed a two-phase read protocol without write operations to avoid adding latency and costs to reads.</p>
<p>In the first phase, the coordinator reads all the items involved in the transaction, along with their Log Sequence Numbers (LSN). In the second phase, if the LSN has not changed, the read is successful; otherwise, the read is rejected.</p>
</li>
<li><p><strong>Recovery and Fault Tolerance</strong></p>
<p>If a storage node fails, the leadership role transfers to another storage node within the same replication group, with transaction metadata persistently stored and replicated across the nodes.</p>
<p>Transaction coordinator failures are more complex. Coordinators maintain a persistent record of each transaction to ensure atomicity and completeness. Recovery managers periodically scan these transaction records, looking for incomplete transactions, and reassign them to new coordinators to resume execution.</p>
</li>
</ol>
<p><strong>Two-phase commit (2PC)</strong></p>
<ol>
<li><p><strong>Prepare Phase</strong></p>
<p>In the prepare phase, the transaction coordinator (TC) is responsible for sending the transaction’s write operations to all the participating storage nodes. The coordinator breaks down the transaction into individual operations targeting specific data items and sends a prepare message to each storage node involved. This message includes:</p>
<ul>
<li><p>The transaction’s timestamp.</p>
</li>
<li><p>The transaction’s unique identifier (ID).</p>
</li>
<li><p>The specific operation to be performed on the data item (such as insert, update, or delete).</p>
</li>
</ul>
<p>Upon receiving the prepare message, each storage node evaluates whether it can accept the transaction. The storage node will accept the transaction’s write operation if all of the following conditions are met:</p>
<ul>
<li><p><strong>Preconditions</strong> are satisfied (e.g., a condition might be that the item must exist, or that it has a certain value).</p>
</li>
<li><p>The write operation does not violate any <strong>system restrictions</strong> (e.g., exceeding the maximum item size).</p>
</li>
<li><p>The transaction’s timestamp is <strong>greater than</strong> the item’s last write timestamp, indicating that this operation is the most recent.</p>
</li>
<li><p>There are no <strong>ongoing transactions</strong> attempting to write to the same item.</p>
</li>
</ul>
<p>If all participating storage nodes accept the transaction during the prepare phase, the coordinator moves to the commit phase. If any node rejects the transaction (e.g., due to a failed precondition or timestamp conflict), the transaction is canceled.</p>
</li>
<li><p><strong>Commit Phase</strong></p>
<p>Once the transaction has been accepted by all storage nodes during the prepare phase, the coordinator enters the commit phase. During this phase, the coordinator sends a commit message to all the storage nodes, instructing them to apply the write operations. Each storage node then:</p>
<ul>
<li><p>Applies the prepared write operations to the local items.</p>
</li>
<li><p>Updates the <strong>timestamp</strong> of the item to reflect the transaction’s timestamp.</p>
</li>
<li><p>Updates the timestamps of any items where preconditions were checked, even if no write operation was performed.</p>
</li>
</ul>
<p>If any node rejects the transaction during the prepare phase, the coordinator sends a cancel message to all storage nodes, instructing them to discard any prepared changes. No writes are applied, ensuring atomicity.</p>
</li>
</ol>
<p><strong>Adapting timestamp ordering for keyvalue operations</strong></p>
<ol>
<li><p><strong>Individual Item Read Operations</strong></p>
<p>In DynamoDB, even if there is a prepared transaction attempting to read to a particular data item, the system still allows read operations on that item. Specifically:</p>
<ul>
<li><p><strong>Bypassing the transaction coordinator</strong>: Non-transactional <code>GetItem</code> operations are routed directly to the storage node responsible for the item, bypassing the transaction coordinator. This avoids potential transaction locks or delays.</p>
</li>
<li><p><strong>Returning the latest data immediately</strong>: The storage node immediately returns the latest committed value of the item, regardless of whether a prepared transaction may later update it.</p>
</li>
<li><p><strong>Timestamp assignment</strong>: This read operation is assigned a timestamp that is after the last write operation’s timestamp but before the prepared transaction’s commit timestamp. This ensures the read operation is serializable, meaning it is placed between the last completed write and the pending write.</p>
</li>
</ul>
</li>
<li><p><strong>Individual Item Write Operations</strong></p>
<p>In most cases, DynamoDB allows individual item write operations to be executed immediately, often before prepared transactions:</p>
<ul>
<li><p><strong>Directly routed to the storage node</strong>: Non-transactional <code>PutItem</code> and other modification operations are routed directly to the storage node, bypassing the transaction coordinator.</p>
</li>
<li><p><strong>Timestamp ordering</strong>: The storage node assigns a timestamp to the write operation that is typically earlier than any prepared transactions (since those have not yet written).</p>
</li>
<li><p><strong>Exceptions</strong>: If a prepared transaction includes a condition check on the item (e.g., checking a bank account balance), the system will not allow a new write to bypass the prepared transaction. For example, if a transaction is checking that there are enough funds to withdraw $100, a new transaction cannot make a withdrawal or delete the item during that check.</p>
</li>
</ul>
</li>
<li><p><strong>Delayed Execution of Write Operations</strong></p>
<p>In certain scenarios, the system can delay write operations instead of rejecting them:</p>
<ul>
<li><p><strong>Buffering writes</strong>: If a new write operation conflicts with a prepared transaction’s conditions (e.g., by modifying the item’s state), the storage node can buffer the write operation in a queue until the prepared transaction is complete. This prevents the need to reject the write and require the client to resubmit it.</p>
</li>
<li><p><strong>Processing buffered writes after the transaction completes</strong>: Once the prepared transaction completes (committed or canceled), the buffered write can be assigned a new timestamp and executed. Typically, the delay caused by waiting for the transaction to complete is short, so this strategy doesn’t significantly increase latency.</p>
</li>
<li><p><strong>Unconditional writes</strong>: If the storage node receives a <code>PutItem</code> or <code>DeleteItem</code> operation without any preconditions, these operations can be executed immediately. They are assigned a timestamp later than any prepared transactions, ensuring the correctness of transactions. If a previously prepared transaction is committed with an earlier timestamp, its write operations will be ignored.</p>
</li>
</ul>
</li>
<li><p><strong>Write Transactions with Older Timestamps</strong></p>
<p>DynamoDB supports accepting write transactions with older timestamps:</p>
<ul>
<li><p><strong>Handling after already committed writes</strong>: If a write transaction with an older timestamp arrives at a storage node where a later write has already been processed, the node can still accept the older transaction and mark it as prepared. If the transaction is eventually committed, its write will be ignored, as the earlier write has already been overwritten by the newer one.</p>
</li>
<li><p><strong>Exceptions for partial updates</strong>: This rule applies to full overwrites of data items (like <code>PutItem</code>), but not to partial updates (like <code>UpdateItem</code>). If the last write was a partial update, the operations must be executed in strict timestamp order to ensure correctness.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple Transactions Writing to the Same Item</strong></p>
<p>DynamoDB allows multiple transactions to simultaneously prepare to write the same data item:</p>
<ul>
<li><p><strong>Simultaneous transaction preparation</strong>: For a given item, a series of transactions can enter the prepared state simultaneously, without waiting for the previous transaction to commit. This increases concurrency and allows multiple transactions to proceed in parallel.</p>
</li>
<li><p><strong>Order of transaction commits</strong>: If the write operations are full item overwrites (like <code>PutItem</code> or <code>DeleteItem</code>), the transactions can be committed in any order, as long as the last <code>PutItem</code> or <code>DeleteItem</code> operation (with the latest timestamp) is the final one executed.</p>
</li>
<li><p><strong>Restrictions for partial updates</strong>: For transactions performing partial updates (like <code>UpdateItem</code>), the transactions must be executed in timestamp order, as the final state of the item depends on the sequence of updates.</p>
</li>
</ul>
</li>
<li><p><strong>Optimized Single-Phase Read Transactions</strong></p>
<p>DynamoDB introduces optimizations for read transactions, allowing certain read transactions to be completed in a single phase without requiring a two-phase commit protocol:</p>
<ul>
<li><p><strong><code>GetItemWithTimestamp</code></strong>: Assuming storage nodes support the <code>GetItemWithTimestamp</code> operation, it allows a read timestamp to be passed as a parameter. This operation returns the latest value of the item, provided its last write timestamp is earlier than the given read timestamp and any prepared transactions have timestamps later than the read timestamp; otherwise, the request is rejected.</p>
</li>
<li><p><strong>Single-phase completion of read transactions</strong>: When a read transaction involves multiple items, the transaction coordinator issues <code>GetItemWithTimestamp</code> requests for each item and buffers the returned values. If all storage nodes accept the requests without conflict, the coordinator can return the buffered values to the client, completing the transaction. If any node rejects the request, the read transaction fails.</p>
</li>
<li><p><strong>Serialization issues</strong>: This optimization is optimistic but can lead to potential serialization issues. If a storage node later accepts a write with a timestamp earlier than a previously executed read transaction, it may cause the transaction to be non-serializable. To avoid this, storage nodes need to track both the last read and write timestamps for each item. Future write transactions must ensure that their timestamps are later than the last read&#x2F;write timestamps of all the items they modify.</p>
</li>
</ul>
</li>
<li><p><strong>Optimizations for Single-Partition Write Transactions</strong></p>
<p>DynamoDB further optimizes write transactions that involve multiple items within a single partition, allowing them to be completed in a single phase without a two-phase commit protocol:</p>
<ul>
<li><p><strong>Single-partition transaction processing</strong>: If all the items being written in a transaction reside within the same partition (and thus are stored on the same storage node), there is no need for separate prepare and commit phases. The storage node can perform all the necessary precondition checks and immediately execute the write operations.</p>
</li>
<li><p><strong>Reduced communication overhead</strong>: This approach significantly reduces the communication overhead between the transaction coordinator and storage nodes, especially in highly concurrent environments, improving system performance.</p>
</li>
</ul>
</li>
</ol>
<p>Questions:</p>
<p><strong>Why does DynmoDB not use the two-phase locking protocol?</strong> </p>
<p>While two-phase locking is used traditionally to prevent concurrent transactions from reading and writing the same data items, it has drawbacks. Locking <strong>restricts concurrency</strong> and can lead to <strong>deadlocks</strong>. Moreover, it requires <strong>a recovery mechanism</strong> to release locks when an application fails after acquiring locks as part of a transaction but before that transaction commits. To simplify the design and take advantage of low-contention workloads, DynamoDB uses an optimistic concurrency control scheme that avoids locking altogether.</p>
<p><strong>With DynamoDB, what is the role of a transaction coordinator?</strong></p>
<ul>
<li>The <strong>Transaction Coordinator</strong> plays a central role in handling transactions that span multiple items or storage nodes. The TC is responsible for</li>
<li><ul>
<li>breaking down the transaction into individual operations and coordinating these operations across the necessary storage nodes.</li>
<li>ensuring that the operations follow two-phase commit and all parts of the transaction are either completed successfully or rolled back.</li>
<li>assigning timestamps to ensure the correct ordering of operations and managing any potential conflicts between concurrent transactions.</li>
</ul>
</li>
</ul>
<p><strong>Is DynamoDB a relational database management system?</strong></p>
<p>No, DynamoDB is not a relational database management system (RDBMS). It is a NoSQL database, specifically a key-value and document store. Here’s how it differs from an RDBMS:</p>
<ol>
<li><strong>Data Model</strong>: DynamoDB does not use tables with fixed schemas like relational databases. Instead, it stores data as key-value pairs or documents (JSON-like structure). Each item can have different attributes, and there’s no need for predefined schemas.</li>
<li><strong>Relationships</strong>: Relational databases focus on managing relationships between data (using joins, foreign keys, etc.), while DynamoDB is optimized for storing large amounts of data without complex relationships between the data items.</li>
<li><strong>Querying</strong>: RDBMSs typically use <strong>SQL</strong> for querying data, which allows for complex joins and aggregations. DynamoDB uses its own API for querying and does not support SQL natively. While it allows querying by primary key and secondary indexes, it doesn’t support joins.</li>
<li><strong>Consistency and Transactions</strong>: DynamoDB supports <strong>eventual consistency</strong> or <strong>strong consistency</strong> for reads, while traditional relational databases typically ensure strong consistency through ACID transactions. DynamoDB has introduced <strong>transactions</strong>, but they work differently compared to those in relational databases.</li>
<li><strong>Scalability</strong>: DynamoDB is designed for horizontal scalability across distributed systems, allowing it to handle very large amounts of traffic and data by automatically partitioning data. In contrast, RDBMSs are typically vertically scaled and are not as naturally distributed.</li>
</ol>
<p><strong>How is DynamoDB’s transaction coordinator different than Gamma’s scheduler?</strong> </p>
<ul>
<li>DynamoDB’s transaction coordinator uses Optimistic Concurrency Control (OCC) to manage distributed transactions, ensuring atomicity without 2PC, focusing on scalability and performance in a globally distributed system.</li>
<li>Gamma’s scheduler, on the other hand, uses the traditional Two-Phase Locking (2PL) protocol to guarantee strong consistency in a distributed environment, prioritizing strict coordination across nodes.</li>
</ul>
<p><strong>Name one difference between FoundationDB and DynamoDB?</strong></p>
<p>FoundationDB: FoundationDB is a multi-model database that offers a core key-value store as its foundation, but it allows you to build other data models (such as documents, graphs, or relational) on top of this key-value layer. It’s highly flexible and provides transactional support for different types of data models via layers.</p>
<p>DynamoDB: DynamoDB is a NoSQL key-value and document store with a fixed data model designed specifically for highly scalable, distributed environments. It does not offer the flexibility of building different models on top of its architecture and is focused on high-performance operations with automatic scaling.</p>
<p><strong>What partitioning strategy does FoundationDB use to distribute key-value pairs across its StorageServers?</strong></p>
<p>FoundationDB uses a range-based partitioning strategy to distribute key-value pairs across its StorageServers.</p>
<p>Here’s how it works:</p>
<ol>
<li><strong>Key Ranges</strong>: FoundationDB partitions the key-value pairs by dividing the key space into <strong>contiguous ranges</strong>. Each range of keys is assigned to a specific <strong>StorageServer</strong>.</li>
<li><strong>Dynamic Splitting</strong>: The key ranges are <strong>dynamically split</strong> and adjusted based on data distribution and load. If a particular range grows too large or becomes a hotspot due to frequent access, FoundationDB will automatically split that range into smaller sub-ranges and distribute them across multiple <strong>StorageServers</strong> to balance the load.</li>
<li><strong>Data Movement</strong>: When a key range is split or needs to be rebalanced, the corresponding data is migrated from one <strong>StorageServer</strong> to another without manual intervention, ensuring even distribution of data and load across the system.</li>
</ol>
<p><strong>Why do systems such as Nova-LSM separate storage of data from its processing?</strong> </p>
<ul>
<li><strong>Independent Scaling</strong>: Storage and processing resources can scale independently to meet varying load demands.</li>
<li><strong>Resource Optimization</strong>: Storage nodes focus on data persistence and I&#x2F;O performance, while processing nodes handle computation, improving overall resource efficiency.</li>
<li><strong>Fault Tolerance</strong>: Data remains safe in storage even if processing nodes fail, ensuring high availability.</li>
</ul>
<p>Reference: </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/atc23-idziorek.pdf">https://www.usenix.org/system/files/atc23-idziorek.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/atc22-elhemali.pdf">https://www.usenix.org/system/files/atc22-elhemali.pdf</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/13/HRPS%20Background/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/13/HRPS%20Background/" class="post-title-link" itemprop="url">HRPS Background</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-13 00:00:00" itemprop="dateCreated datePublished" datetime="2024-09-13T00:00:00-07:00">2024-09-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-30 19:42:46" itemprop="dateModified" datetime="2025-05-30T19:42:46-07:00">2025-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="HRPS-Background"><a href="#HRPS-Background" class="headerlink" title="HRPS Background"></a>HRPS Background</h1><p><img src="/../images/index_and_page.png" alt="img"></p>
<p>Each page consits of the header, the record space and a pointer array. Each slot in this array points to a record within the page.</p>
<p>A record is located by providing its page address and the slot number. The combination is called RID.</p>
<p>Life Cycle of a record:</p>
<p>Insert: Find an empty space to put it and set a slot at the very end of the page to point to it.</p>
<p>Delete: Remove the record and reclaim this space, set its slot number to null. When there are too many garbage slots, the system will drop the index structure, do reorganization on this disk page by removing all null slots, and reconstruct the index structure from scratch.</p>
<p>Non-clustered Index: the data of the disk page is independent of the bucket or leaf node of index.</p>
<p>Clustered Index: the data of the disk page resides within the bucket or leaf node of index.</p>
<p>Hash Index: the item in buckets is not ordered by the attribute value of index.</p>
<p>B+Tree Index: the item in leaf nodes is ordered by the attribute value of index.</p>
<ul>
<li>Primary Index: the data in the disk page is ordered.</li>
<li>Secondary Index: the data in the disk page is not ordered.</li>
</ul>
<p>Reference: <a target="_blank" rel="noopener" href="https://www.vldb.org/conf/1990/P481.PDF">https://www.vldb.org/conf/1990/P481.PDF</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/27/FoundationDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/27/FoundationDB/" class="post-title-link" itemprop="url">FoundationDB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-27 00:00:00" itemprop="dateCreated datePublished" datetime="2024-09-27T00:00:00-07:00">2024-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-30 19:45:29" itemprop="dateModified" datetime="2025-05-30T19:45:29-07:00">2025-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="FoundationDB"><a href="#FoundationDB" class="headerlink" title="FoundationDB"></a>FoundationDB</h1><p>FoundationDB的研究意义在于，它成功地将NoSQL的灵活性与ACID事务的强大功能结合在一起，提供了一种模块化的架构，使得各个子系统可以独立配置和扩展。这种设计不仅提高了系统的可扩展性和可用性，还增强了故障容忍能力。此外，FoundationDB采用了严格的模拟测试框架，确保了系统的稳定性和高效性，使得开发者能够快速引入和发布新特性。FoundationDB的快速恢复机制显著提高了系统的可用性，简化了软件升级和配置变更的过程，通常在几秒钟内完成。</p>
<p>The main design principles are:</p>
<ol>
<li>Divide-and-Conquer (or separation of concerns). FDB decouples the transaction management system (write path) from the distributed storage (read path) and scales them independently. Within the transaction management system, processes are assigned various roles representing different aspects of transaction management. Furthermore, cluster-wide orchestrating tasks, such as overload control and load balancing are also divided and serviced by additional heterogeneous roles.</li>
<li>Make failure a common case. For distributed systems, failure is a norm rather than an exception. To cope with failures in the transaction management system of FDB, we handle all failures through the recovery path: the transaction system proactively shuts down when it detects a failure. Thus, all failure handling is reduced to a single recovery operation, which becomes a common and well-tested code path. To improve availability, FDB strives to minimize Mean-Time-To-Recovery (MTTR). In our production clusters, the total time is usually less than five seconds.</li>
<li>Simulation testing. FDB relies on a randomized, deterministic simulation framework for testing the correctness of its distributed database. Simulation tests not only expose deep bugs, but also boost developer productivity and the code quality of FDB.</li>
</ol>
<p><strong>Architecture:</strong></p>
<p><img src="/../images/FDB_arch.png" alt="img"></p>
<ul>
<li><p>The control plane is responsible for persisting critical system metadata, that is, the configuration of transaction systems, on Coordinators.</p>
<ul>
<li><p>These <strong>Coordinators</strong> form a Paxos group and elect a ClusterController.</p>
</li>
<li><p>The <strong>ClusterController</strong> monitors all servers in the cluster and recruits three processes, Sequencer, DataDistributor, and Ratekeeper, which are re-recruited if they fail or crash.</p>
</li>
<li><p>The <strong>DataDistributor</strong> is responsible for monitoring failures and balancing data among StorageServers.</p>
</li>
<li><p><strong>Ratekeeper</strong> provides overload protection for the cluster.</p>
</li>
</ul>
</li>
<li><p>The data plane is responsible for transaction processing and data storage. FDB chooses an unbundled architecture:</p>
<ul>
<li><p>A distributed transaction management system (TS) consists of a Sequencer, Proxies, and Resolvers, all of which are stateless processes.</p>
<ul>
<li><p>The Sequencer assigns a read and a commit version to each transaction.</p>
</li>
<li><p>Proxies offer MVCC read versions to clients and orchestrate transaction commits.</p>
</li>
<li><p>Resolvers check for conflicts among transactions.</p>
</li>
</ul>
</li>
<li><p>A log system (LS) stores Write-Ahead-Log (WAL) for TS, and a separate distributed storage system (SS) is used for storing data and servicing reads. The LS contains a set of LogServers and the SS has a number of StorageServers. LogServers act as replicated, sharded, distributed persistent queues, each queue storing WAL data for a StorageServer.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Clients read from sharded StorageServers, so reads scale linearly with the number of StorageServers.</strong></p>
<p><strong>Writes are scaled by adding more Proxies, Resolvers, and LogServers.</strong></p>
<p>The control plane’s singleton processes (e.g., ClusterController and Sequencer) and Coordinators are not performance bottlenecks; they only perform limited metadata operations. 因为元数据操作少且简单，且与两者无关的数据读写是并行扩展的（如上面两行加粗字体所述）。</p>
<p><strong>Bootstrapping:</strong></p>
<p>FDB has no dependency on external coordination services. All user data and most system metadata (keys that start with 0xFF prefix) are stored in StorageServers. The metadata about StorageServers is persisted in LogServers, and the LogServers configuration data is stored in all Coordinators.</p>
<ol>
<li>The Coordinators are a disk Paxos group; servers attempt to become the ClusterController if one does not exist.</li>
<li>A newly elected ClusterController reads the old LS configuration from the Coordinators and spawns a new TS and LS.</li>
<li>Proxies recover system metadata from the old LS, including information about all StorageServers.</li>
<li>The Sequencer waits until the new TS finishes recovery, then writes the new LS configuration to all Coordinators. The new transaction system is then ready to accept client transactions.</li>
</ol>
<p><strong>Reconfiguration:</strong></p>
<p>The Sequencer process monitors the health of Proxies, Resolvers, and LogServers. Whenever there is a failure in the TS or LS, or the database configuration changes, the Sequencer terminates. The ClusterController detects the Sequencer failure, then recruits and bootstraps a new TS and LS. In this way, transaction processing is divided into epochs, where each epoch represents a generation of the transaction management system with its own Sequencer.</p>
<p><strong>End-to-end transaction processing:</strong></p>
<ol>
<li><p><strong>Transaction Start and Read Operations:</strong></p>
<ul>
<li><p>A client starts a transaction by contacting a <strong>Proxy</strong> to obtain a read version (timestamp).</p>
</li>
<li><p>The <strong>Proxy</strong> requests a read version from the <strong>Sequencer</strong> that is greater than all previously issued commit versions and sends it to the client.</p>
</li>
<li><p>The client then reads from <strong>StorageServers</strong> at this specific read version.</p>
</li>
</ul>
</li>
<li><p><strong>Buffered Write Operations</strong>:</p>
<ul>
<li><p>Client writes are buffered locally and not sent to the cluster immediately.</p>
</li>
<li><p>Read-your-write semantics are preserved by combining the database lookups with the client’s uncommitted writes.</p>
</li>
</ul>
</li>
<li><p><strong>Transaction Commit</strong>:</p>
<ul>
<li><p>When the client commits, it sends the transaction data (read and write sets) to a <strong>Proxy</strong>, waiting for either a commit or abort response.</p>
</li>
<li><p>The <strong>Proxy</strong> commits a transaction in three steps:</p>
<ol>
<li><p><strong>Obtain Commit Version</strong>: The Proxy requests a commit version from the <strong>Sequencer</strong> that is larger than all current read or commit versions.</p>
</li>
<li><p><strong>Conflict Check</strong>: The Proxy sends transaction data to the partitioned <strong>Resolvers</strong>, which check for read-write conflicts. If no conflicts are found, the transaction proceeds; otherwise, it is aborted.</p>
</li>
<li><p><strong>Persist to Log Servers</strong>: The transaction is sent to <strong>LogServers</strong> for persistence, and after all LogServers acknowledge, the transaction is considered committed. The Proxy then reports the committed version to the <strong>Sequencer</strong> and sends the response back to the client.</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Applying Writes</strong>:</p>
<ul>
<li><strong>StorageServers</strong> continuously pull mutation logs from <strong>LogServers</strong> and apply the committed changes to disk.</li>
</ul>
</li>
<li><p><strong>Read-Only Transactions and Snapshot Reads</strong>:</p>
<ul>
<li><p>Read-only transactions are <strong>serializable</strong> (at the read version) and <strong>high-performance</strong> (thanks to MVCC), allowing the client to commit locally without contacting the database, which is particularly important since most transactions are read-only.</p>
</li>
<li><p><strong>Snapshot reads</strong> relax the isolation property of a transaction, reducing conflicts by allowing concurrent writes without conflicting with snapshot reads.</p>
</li>
</ul>
</li>
</ol>
<p><strong>FoundationDB (FDB) using Serializable Snapshot Isolation (SSI) by combining Optimistic Concurrency Control (OCC) with Multi-Version Concurrency Control (MVCC).</strong></p>
<p>Transaction Versions:</p>
<ul>
<li>Each transaction receives a <strong>read version</strong> and a <strong>commit version</strong> from the <strong>Sequencer</strong>.</li>
<li>The read version ensures that the transaction observes the results of all previously committed transactions, and the commit version is greater than all current read or commit versions, establishing a serial order for transactions.</li>
</ul>
<p>Log Sequence Number (LSN):</p>
<ul>
<li>The <strong>commit version</strong> serves as the <strong>LSN</strong>, defining a serial history of transactions.</li>
<li>To ensure no gaps between LSNs, the Sequencer also returns the previous LSN with each commit. Both the LSN and previous LSN are sent to <strong>Resolvers</strong> and <strong>LogServers</strong> to enforce serial processing of transactions.</li>
</ul>
<p>Conflict Detection:</p>
<ul>
<li>FDB uses a lock-free conflict detection algorithm similar to <strong>write-snapshot isolation</strong>, but the commit version is chosen before conflict detection, enabling efficient batch processing of version assignments and conflict detection.</li>
<li>The key space is divided among multiple <strong>Resolvers</strong>, allowing conflict detection to be parallelized. A transaction can commit only if all Resolvers confirm no conflicts.</li>
</ul>
<p>Handling Aborted Transactions:</p>
<ul>
<li>If a transaction is aborted, some Resolvers may have already updated their history, leading to possible “false positive” conflicts for other transactions. However, this is rare because most transactions’ key ranges fall within one Resolver, and the effects of false positives are limited to a short MVCC window (5 seconds).</li>
</ul>
<p>Efficiency of OCC:</p>
<ul>
<li>The OCC design avoids the complexity of acquiring and releasing locks, simplifying interactions between the <strong>Transaction System (TS)</strong> and <strong>Storage Servers (SS)</strong>.</li>
<li>While OCC may result in some wasted work due to aborted transactions, FDB’s conflict rate in production is low (less than 1%), and clients can simply restart aborted transactions.</li>
</ul>
<p><strong>Logging protocol:</strong></p>
<p>Commit Logging:</p>
<ul>
<li>Once a <strong>Proxy</strong> decides to commit a transaction, it sends the transaction’s changes (mutations) to the <strong>LogServers</strong> responsible for the modified key ranges. Other LogServers receive an empty message.</li>
<li>The log message includes the current and previous <strong>Log Sequence Number (LSN)</strong> from the <strong>Sequencer</strong> and the largest known committed version (KCV) of the Proxy.</li>
<li>The <strong>LogServers</strong> reply to the Proxy once the log data is durably stored. The Proxy updates its KCV if all replica LogServers acknowledge and the LSN is larger than the current KCV.</li>
</ul>
<p>Shipping Redo Logs:</p>
<ul>
<li>Shipping the redo log from LogServers to <strong>StorageServers</strong> happens in the background and is not part of the commit path, improving performance.</li>
</ul>
<p>Applying Redo Logs:</p>
<ul>
<li><strong>StorageServers</strong> apply non-durable redo logs from LogServers to an in-memory index. In most cases, this happens before any client reads are processed, ensuring low-latency multi-version reads.</li>
<li>If the requested data is not yet available on a StorageServer, the client either waits or retries at another replica. If both reads time out, the client can restart the transaction.</li>
</ul>
<p>I&#x2F;O Efficiency:</p>
<ul>
<li>Since log data is already durable on LogServers, StorageServers can buffer updates in memory and write batches to disk periodically, improving input&#x2F;output (I&#x2F;O) efficiency.</li>
</ul>
<p><strong>What if a StorageServer is lagging behind on applying the redo logs and a client requests a version of a key pair it does not have?</strong></p>
<ol>
<li>Wait for a threshold for when known-committed-version is greater than or equal to the read version</li>
<li>If timeout, the client asks another StorageServer that stores the key</li>
<li>Return error “request for a future version” (FDB error code 1009)</li>
</ol>
<p><strong>What if there is no further transaction logs to redo?</strong></p>
<ul>
<li>Without new transactions issued from the client, proxies still generate empty transactions to advance the known-committed-version</li>
<li>Known-committed-version and LSN of each transaction are sent to all LogServers (limit scalability on writes)</li>
</ul>
<p><strong>Transaction system recovery:</strong></p>
<p>Simplified Recovery:</p>
<ul>
<li>Unlike traditional databases that require <strong>undo log processing</strong>, FoundationDB avoids this step by making the <strong>redo log processing</strong> the same as the normal log forward path. StorageServers pull logs from LogServers and apply them in the background.</li>
</ul>
<p>Failure Detection and New Transaction System (TS):</p>
<ul>
<li>Upon detecting a failure, a new TS is recruited. The new TS can start accepting transactions even before all old logs are fully processed. Recovery focuses on finding the end of the redo log, allowing StorageServers to asynchronously replay the logs from that point.</li>
</ul>
<p>Epoch-based Recovery:</p>
<ul>
<li>The recovery process is handled per <strong>epoch</strong>. The <strong>ClusterController</strong> locks the old TS configuration, stops old LogServers from accepting new transactions, recruits a new set of transaction components (Sequencer, Proxies, Resolvers, and LogServers), and writes the new TS configuration to the <strong>Coordinators</strong>.</li>
<li>Stateless components like Proxies and Resolvers don’t require special recovery, but LogServers, which store committed transaction logs, must ensure all data is durable and retrievable by StorageServers.</li>
</ul>
<p>Recovery Version (RV):</p>
<ul>
<li>The recovery focuses on determining the <strong>Recovery Version (RV)</strong>, which is essentially the end of the redo log. The <strong>Sequencer</strong> collects data from the old LogServers, specifically the <strong>Durable Version (DV)</strong> (maximum LSN persisted) and <strong>KCV</strong> (maximum committed version) from each.</li>
<li>Once enough LogServers have responded, the <strong>Previous Epoch Version (PEV)</strong> is established (the maximum of all KCVs). The start version of the new epoch is <code>PEV + 1</code>, and the minimum DV becomes the <strong>RV</strong>.</li>
</ul>
<p>Log Copying and Healing:</p>
<ul>
<li>Logs between <code>PEV + 1</code> and RV are copied from old LogServers to the new ones to restore replication in case of LogServer failures. This copying process is lightweight since it only covers a few seconds of logs.</li>
</ul>
<p>Rollback and Transaction Processing:</p>
<ul>
<li>The first transaction after recovery is a special <strong>recovery transaction</strong> that informs StorageServers of the RV, so they can discard in-memory multi-versioned data beyond the RV. StorageServers then pull data larger than the PEV from the new LogServers.</li>
<li>The rollback process simply discards in-memory multi-versioned data, as persistent data is only written to disk once it leaves the MVCC window.</li>
</ul>
<p><strong>Replication:</strong></p>
<ol>
<li><p><strong>Metadata Replication</strong>:</p>
<ul>
<li><strong>System metadata</strong> related to the control plane is stored on <strong>Coordinators</strong> using the <strong>Active Disk Paxos</strong> protocol. As long as a majority (quorum) of Coordinators are operational, the metadata can be recovered in case of failure.</li>
</ul>
</li>
<li><p><strong>Log Replication</strong>:</p>
<ul>
<li>When a <strong>Proxy</strong> writes logs to <strong>LogServers</strong>, each log record is replicated synchronously across <strong>k &#x3D; f + 1</strong> LogServers (where <strong>f</strong> is the number of allowed failures). The Proxy only sends a commit response to the client after all <strong>k</strong> LogServers have successfully persisted the log. If a LogServer fails, a transaction system recovery is triggered.</li>
</ul>
</li>
<li><p><strong>Storage Replication</strong>:</p>
<ul>
<li>Each <strong>key range (shard)</strong> is asynchronously replicated across <strong>k &#x3D; f + 1 StorageServers</strong>. These StorageServers form a <strong>team</strong>. A StorageServer typically hosts multiple shards, distributing its data across several teams. If a StorageServer fails, the <strong>DataDistributor</strong> moves the data from teams with the failed server to other healthy teams.</li>
</ul>
</li>
</ol>
<p>To prevent data loss in case of simultaneous failures, FoundationDB ensures that no more than one process in a replica group is placed within the same fault domain (e.g., a host, rack, or availability zone). As long as one process in each team is operational, no data is lost, provided at least one fault domain remains available.</p>
<p><strong>Simulation testing:</strong></p>
<ol>
<li><p><strong>Deterministic Simulation</strong>:</p>
<ul>
<li><p>FoundationDB uses <strong>deterministic discrete-event simulation</strong> to test its distributed system. This simulation runs real database code along with <strong>randomized synthetic workloads</strong> and <strong>fault injection</strong> to uncover bugs.</p>
</li>
<li><p>Determinism ensures that bugs are reproducible and can be investigated thoroughly.</p>
</li>
</ul>
</li>
<li><p><strong>Fault Injection</strong>:</p>
<ul>
<li><p>The simulation tests system resilience by injecting various faults, such as <strong>machine, rack, or data center failures</strong>, network issues, disk corruption, and delays.</p>
</li>
<li><p>Randomization of these faults increases the diversity of tested states, allowing for a wide range of potential issues to be examined.</p>
</li>
<li><p><strong>“Buggification”</strong> is a technique used to deliberately introduce rare or unusual behaviors (e.g., unnecessary delays, errors) in the system to stress-test its handling of non-standard conditions.</p>
</li>
</ul>
</li>
<li><p><strong>Swarm Testing</strong>:</p>
<ul>
<li><p><strong>Swarm testing</strong> increases simulation diversity by using random cluster sizes, configurations, workloads, and fault injection parameters.</p>
</li>
<li><p>This ensures that a broad range of scenarios is covered in testing, allowing for the discovery of rare bugs.</p>
</li>
</ul>
</li>
<li><p><strong>Test Oracles</strong>:</p>
<ul>
<li><p><strong>Test oracles</strong> are built into the system to verify key properties like <strong>transaction atomicity</strong>, <strong>isolation</strong>, and <strong>recoverability</strong>. Assertions check these properties to detect failures during simulation.</p>
</li>
<li><p>They help confirm that the system’s expected behaviors are maintained, even under stressful conditions.</p>
</li>
</ul>
</li>
<li><p><strong>Bug Detection Efficiency</strong>:</p>
<ul>
<li><p>The simulation runs faster than real-time, allowing FoundationDB to quickly discover and trace bugs. The <strong>parallel</strong> nature of testing accelerates the process of finding bugs, particularly before major releases.</p>
</li>
<li><p>This approach uncovers bugs that may not appear during real-time testing, especially for issues that require long-running operations.</p>
</li>
</ul>
</li>
<li><p><strong>Limitations</strong>:</p>
<ul>
<li><p>Simulation cannot reliably detect <strong>performance issues</strong> (like imperfect load balancing).</p>
</li>
<li><p>It cannot test <strong>third-party libraries</strong> or <strong>external dependencies</strong>, focusing mainly on FoundationDB’s internal code and behaviors.</p>
</li>
</ul>
</li>
</ol>
<p><strong>Lessons learned:</strong></p>
<ol>
<li><p><strong>Architecture Design</strong></p>
<ul>
<li><p><strong>Divide-and-Conquer Principle</strong>: Separating the transaction system from the storage layer allows for independent scaling and deployment of resources, enhancing both flexibility and performance.</p>
</li>
<li><p><strong>LogServers as Witness Replicas</strong>: In multi-region deployments, LogServers reduce the need for full StorageServer replicas while maintaining high availability.</p>
</li>
<li><p><strong>Role Specialization</strong>: The design enables the creation of specialized roles, like separating DataDistributor and Ratekeeper from the Sequencer, and separating Proxies into Get-Read-Version and Commit Proxies, which improves performance and makes the system extensible.</p>
</li>
<li><p><strong>Decoupling Enhances Extensibility</strong>: This design pattern allows features like replacing SQLite with RocksDB and adding new roles or functions without overhauling the entire system.</p>
</li>
</ul>
</li>
<li><p><strong>Simulation Testing</strong></p>
<ul>
<li><p><strong>High Productivity</strong>: FDB’s deterministic simulation testing enables bugs to be found and reproduced quickly. This approach has improved developer productivity and system reliability by reducing debugging time and improving test coverage.</p>
</li>
<li><p><strong>Reliability</strong>: FDB has operated without any data corruption over several years of deployment (e.g., CloudKit), thanks to rigorous simulation testing. Simulation has allowed ambitious rewrites and improvements to be made safely.</p>
</li>
<li><p><strong>Eliminating Dependencies</strong>: Simulation testing helped find bugs in external dependencies, leading to FDB replacing Apache Zookeeper with its own Paxos implementation. This change resulted in no further production bugs.</p>
</li>
</ul>
</li>
<li><p><strong>Fast Recovery</strong></p>
<ul>
<li><p><strong>Simplifies Upgrades</strong>: FDB allows fast recovery by restarting all processes simultaneously, typically within seconds, simplifying software upgrades and configuration changes. This method has been extensively tested and used in Apple’s production clusters.</p>
</li>
<li><p><strong>Bug Healing</strong>: Fast recovery can automatically resolve certain latent bugs, similar to software rejuvenation, by resetting system states.</p>
</li>
</ul>
</li>
<li><p><strong>5-Second MVCC Window</strong></p>
<ul>
<li><p><strong>Memory Efficiency</strong>: FDB uses a 5-second MVCC (Multi-Version Concurrency Control) window to limit memory usage in transaction systems and storage servers. This time window is long enough for most OLTP workloads, exposing inefficiencies if the transaction exceeds 5 seconds.</p>
</li>
<li><p><strong>TaskBucket Abstraction</strong>: Long-running processes, like backups, are broken into smaller transactions that fit within the 5-second window. FDB implements this through an abstraction called TaskBucket, which simplifies splitting large transactions into manageable jobs.</p>
</li>
</ul>
</li>
</ol>
<p>Questions:</p>
<p><strong>With FDB, what operations does a transaction commit perform when the transaction only reads the value of data items?</strong></p>
<ul>
<li><strong>Read Version Retrieval</strong>: The client requests a read version from a <strong>Proxy</strong> via the <strong>Sequencer</strong>, which guarantees the read version is greater than or equal to any committed version.</li>
<li><strong>Read Operation</strong>: The client reads the requested data at this specific read version from the <strong>StorageServers</strong>. The reads are served by the StorageServers, which are guaranteed to provide data consistent with the requested version.</li>
<li><strong>No Writes or Conflicts</strong>: Since the transaction is read-only, there is no write set or conflicts to check. The transaction simply ends, and no data is written or modified, meaning it does not interact with LogServers or commit any changes.</li>
<li><strong>Commit</strong>: Even though no actual commit occurs (because there’s no data change), the transaction is marked as successfully completed after the reads are done.</li>
</ul>
<p><strong>With FDB, is it possible for multiple resolvers to participate in the decision whether to commit or abort a write transaction?</strong> </p>
<p>Yes, multiple Resolvers can participate in the decision to commit or abort a write transaction in FDB. Here’s how it works:</p>
<ul>
<li><strong>Conflict Detection</strong>: When a transaction writes data, the write set (the keys it wants to write) is sent to a set of <strong>Resolvers</strong>. Each Resolver is responsible for a specific portion of the key space. Multiple Resolvers can be involved in checking the transaction’s read and write sets to detect <strong>conflicts</strong> (read-write conflicts or write-write conflicts).</li>
<li><strong>Parallel Conflict Checking</strong>: Since the key space is partitioned, different Resolvers check different key ranges in parallel. A transaction can only commit if <strong>all</strong> Resolvers agree that there are no conflicts.</li>
</ul>
<p><strong>With FDB, what if a StorageServer is lagging behind on applying the redo logs and a client requests a version of a key pair it does not have?</strong></p>
<ul>
<li><strong>Client Waits</strong>: The client can choose to wait for the StorageServer to catch up by applying the redo logs. Once the StorageServer finishes replaying the logs and reaches the required version, it can serve the requested data.</li>
<li><strong>Retry at Another Replica</strong>: If the StorageServer does not have the requested version yet, the client can try to read from another <strong>replica</strong> of the key. FDB typically stores multiple replicas of data across different StorageServers, so the client can retry the request from a replica that is up to date.</li>
<li><strong>Transaction Restart</strong>: If neither replica has the requested version or the delay is too long, the client may restart the transaction. Since FoundationDB uses <strong>MVCC (Multi-Version Concurrency Control)</strong>, restarting the transaction allows it to obtain a fresh version of the key from an up-to-date StorageServer.</li>
</ul>
<p><strong>Consider a database for students enrolling in courses and professors teaching those courses. Provide a SDM model of this database?</strong></p>
<p>Students: base concrete object class.</p>
<p>member property: student_id, name, age, email, department_id.</p>
<p>identifier: student_id.</p>
<p>Professors: base concrete object class.</p>
<p>member property: professor_id, name, age, email, department_id.</p>
<p>identifier: professor_id.</p>
<p>Courses: base concrete object class</p>
<p>member property: course_id, name, location, start_time, end_time, department_id.</p>
<p>derived member property: professor as Professors.professor_id.</p>
<p>identifier: course_id.</p>
<p>Enrollment: base duration event class.</p>
<p>member property: enrollment_id, date_of_enrollment.</p>
<p>member participant: student in Students, course in Courses.</p>
<p>identifier: enrollment_id.</p>
<p>Departments: abstract Students and Professors on common value of department_id.</p>
<p>derived member property: department_id as distinct value of (Students.department_id union Professors.department_id).</p>
<p><strong>What is the difference between a monolithic database management system and a disaggregated database management system?</strong> </p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Monolithic DBMS</th>
<th>Disaggregated DBMS</th>
</tr>
</thead>
<tbody><tr>
<td>Architecture</td>
<td>All components tightly integrated into a single system</td>
<td>Components like storage, computation, and query processing are separated</td>
</tr>
<tr>
<td>Scalability</td>
<td>Scales through vertical scaling (adding resources to the single server)</td>
<td>Scales through horizontal scaling (independent scaling of storage and compute)</td>
</tr>
<tr>
<td>Performance Bottlenecks</td>
<td>May face bottlenecks as the system grows</td>
<td>Components are independently optimized, reducing bottlenecks</td>
</tr>
<tr>
<td>Resource Management</td>
<td>Storage and compute resources are tightly coupled, hard to manage separately</td>
<td>Storage and compute resources can be managed independently, offering flexibility</td>
</tr>
<tr>
<td>Complexity</td>
<td>Easier to deploy and manage initially, but complexity increases with scale</td>
<td>More complex to manage and coordinate different components</td>
</tr>
<tr>
<td>Cost</td>
<td>Pay for all resources, even if they are not fully utilized</td>
<td>Can optimize resource usage and costs by scaling components independently</td>
</tr>
<tr>
<td>Consistency</td>
<td>Strong data consistency due to tight integration</td>
<td>Requires additional mechanisms to ensure consistency across components</td>
</tr>
</tbody></table>
<p><strong>With Gamma and its data flow execution paradigm, how does the system know when the execution of a parallel query involving multiple operators is complete?</strong></p>
<p>Data Dependency Graph: The query execution is modeled as a directed acyclic graph (DAG), where each node represents an operator (e.g., selection, join). Data flows between operators, and the system tracks the completion of each operator based on this graph.</p>
<p>Completion Signals: Each parallel operator sends a “done” signal once it finishes processing its data partition. The system monitors these signals to determine when all operators have finished.</p>
<p>Coordinator: A central coordinator tracks the progress of parallel tasks. When all tasks report completion, the system declares the query execution as complete.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://sigmodrecord.org/publications/sigmodRecord/2203/pdfs/08_fdb-zhou.pdf">https://sigmodrecord.org/publications/sigmodRecord/2203/pdfs/08_fdb-zhou.pdf</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/13/HRPS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/13/HRPS/" class="post-title-link" itemprop="url">HRPS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-13 00:00:00" itemprop="dateCreated datePublished" datetime="2024-09-13T00:00:00-07:00">2024-09-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-30 19:35:44" itemprop="dateModified" datetime="2025-05-30T19:35:44-07:00">2025-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="HRPS"><a href="#HRPS" class="headerlink" title="HRPS"></a>HRPS</h1><p>The HRPS declusters a relation into fragment based on the following criteria:</p>
<ul>
<li>Each fragment contains approximately FC tuples.</li>
<li>Each fragment contains a unique range of values of the partitioning attribute.</li>
</ul>
<p>The variable FC is determined based on the processing capability of the system and the resource requirements of the queries that access the relation (rather than the number of processors in the configuration).</p>
<p>A major underlying assumption of this partitioning strategy is that the selection operators which access the database retrieve and process the selected tuples using either a range predicate or an equality predicate.</p>
<p>For each query Qi, the workload defines the CPU processing time (CPUi), the Disk Processing Time (Diski), and the Network Processing time (Neti) of that query. Observe that these times are determined based on the resource requirements of each individual query and the processing capability of the system. Each query retrieves and processes (TuplesPerQi) tuples from the database. Furthermore, we assume that the workload defines the frequency of occurrence of each query (FreqQi).</p>
<p>Rather than describing the HRPS with respect to each query in the workload, we deline an average query (Qavg) that is representative of all the queries in the workload. The CPU, disk and network processing quanta for this query are:</p>
<p><img src="/../images/CPU_Disk_Net_TPQ.png" alt="截屏2025-05-30 18.28.03"></p>
<p>Assume that a single processor cannot overlap the use of two resources for an individual query. Thus, the execution time of Qavg on a single processor in a single user environment is:</p>
<p><img src="/../images/exe_time.png" alt="截屏2025-05-30 18.28.21"></p>
<p>As more processors are used for query execution, the response time decreases. However, this also incurs additional overhead, represented by the variable CP, which refers to the cost of coordinating the query execution across multiple processors (e.g., messaging overhead). The response time of the query on M processors can be described by the following formula:</p>
<p><img src="/../images/RT_M.png" alt="RT_M"></p>
<p>In a single-user environment, both HRPS and range partitioning perform similarly because they both efficiently execute the query on the required processor. However, in a multi-user environment, the range partitioning strategy is likely to perform better because it can distribute the workload across multiple processors, improving system throughput. In contrast, HRPS might not utilize all available processors as effectively, potentially leading to lower throughput.</p>
<p>Instead of M representing the number of processors over which a relation should be declustered, M is used instead to represent the number of processors that should participate in the execution of Qavg. Since Qavg processes TuplesPerQavg tuples, each fragment of the relation should contain FC &#x3D; TuplesPerQavg &#x2F; M tuples.</p>
<p>The process of fragmenting and distributing data in HRPS:</p>
<ol>
<li><strong>Sorting the relation</strong>: The relation is first sorted based on the partitioning attribute to ensure each fragment contains a distinct range of values.</li>
<li><strong>Fragmentation</strong>: The relation is then split into fragments, each containing approximately <strong>FC</strong> tuples.</li>
<li><strong>Round-robin distribution</strong>: These fragments are distributed to processors in a <strong>round-robin fashion</strong>, ensuring that adjacent fragments are assigned to different processors (unless the number of processors <strong>N</strong> is less than the required processors <strong>M</strong>).</li>
<li><strong>Storing fragments</strong>: All the fragments for a relation on a given processor are stored in the same physical file.</li>
<li><strong>Range table</strong>: The mapping of fragments to processors is maintained in a <strong>one-dimensional directory</strong> called the range table.</li>
</ol>
<p>This method ensures that at least M processors and at most M + 1 processors participate in the execution of a query.</p>
<p><strong>M &#x3D; N</strong>：系统和查询需求匹配，HRPS 调度所有处理器，达到最大并行度和最优性能。</p>
<p><strong>M &lt; N</strong>：HRPS 只调度一部分处理器执行查询，减少通信开销，但部分处理器资源可能闲置。</p>
<p><strong>M &gt; N</strong>：HRPS 将多个片段分配给处理器，尽量利用所有处理器，但每个处理器负担加重，查询执行速度可能受到影响。</p>
<p>HRPS in this paper supports only homogeneous nodes.</p>
<p>Questions:</p>
<p><strong>How does HRPS decide the ideal degree of parallelism for a query?</strong></p>
<p>HRPS (Hybrid-Range Partitioning Strategy) decides the ideal degree of parallelism by analyzing the resource requirements of the query, such as CPU, disk I&#x2F;O, and communication costs. It calculates the optimal number of processors (denoted as M) based on these factors. The strategy strikes a balance between minimizing query response time and avoiding excessive overhead from using too many processors.</p>
<p><strong>Why is it not appropriate to direct a query that fetches one record using an index structure to all the nodes of a system based on the shared-nothing architecture?</strong> </p>
<p>Fetching one record should only involve the node that contains the relevant data, as querying all nodes wastes resources and increases response time.</p>
<p><strong>How to extend HRPS to support heterogeneous nodes?</strong></p>
<ol>
<li>More powerful nodes would receive more fragments, while weaker nodes would handle fewer fragments.</li>
<li>The system could monitor node performance and dynamically adjust the degree of parallelism and fragment allocation based on current load and node availability.</li>
<li>Heavier tasks may be directed to more powerful nodes, while smaller or simpler queries could be executed on less powerful nodes.</li>
</ol>
<p>Reference: <a target="_blank" rel="noopener" href="https://www.vldb.org/conf/1990/P481.PDF">https://www.vldb.org/conf/1990/P481.PDF</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/08/30/SDM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/08/30/SDM/" class="post-title-link" itemprop="url">SDM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-08-30 00:00:00" itemprop="dateCreated datePublished" datetime="2024-08-30T00:00:00-07:00">2024-08-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-29 19:45:39" itemprop="dateModified" datetime="2025-05-29T19:45:39-07:00">2025-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="SDM"><a href="#SDM" class="headerlink" title="SDM"></a>SDM</h1><p>Data Models: Conceptual -&gt; Logical -&gt; Physical.</p>
<p>SDM is a conceptual data modeling tool, at the intersection of conceptual and logical. It facilitates <strong>an understaning of the meaning of the data</strong>.</p>
<ul>
<li>Identify and classify principal intensional (semantic) structures of an application.</li>
</ul>
<p>A set of constructs that express the essential meaning and structure of different problem domains.</p>
<p><strong>Basic Structure:</strong></p>
<ul>
<li>Classes: a collection of entities. Each class has:</li>
<li><ul>
<li>A name.</li>
<li>A collection of members: Its entities. 3 member types:</li>
<li><ul>
<li>Objects:</li>
<li><ul>
<li>Concrete.</li>
<li>Abstraction: a generalization of another entity.</li>
<li>Aggregate: a collection of another type of entity.</li>
</ul>
</li>
<li>Events: Action or activities in the application. Point and duration events.</li>
<li>Names are deginators for objects or events.</li>
</ul>
</li>
<li>Attributes.</li>
<li>A description: nature, purpose, and uses of the class.</li>
<li>Identified as either base or nonbase.</li>
</ul>
</li>
<li>Schema: a collection of classes.</li>
</ul>
<p>Class Attributes:</p>
<ol>
<li><strong>Member attributes</strong> link the member to one or more related entities in the same or another class.</li>
<li><strong>Class determined attribute</strong> is associated with the whole class and has the same value for all members of that class.</li>
<li><strong>Class attribute</strong> describes a property of a class taken as a whole.</li>
</ol>
<p>An attribute value is either a primitive (user defined) or derived (a value calculated from other information in the database).</p>
<p><strong>Base class</strong></p>
<p>It is independent of other classes. In SDM, it may be a concrete object class, a point event class, a duration event, a name class.</p>
<p>It is specified as either containing duplicates or not containing duplicates. The latter models a multiset&#x2F;bag of entities.</p>
<p>It has an associated list of groups of member attributes. One or more may serve as the unique identifier of a member.</p>
<p><strong>Nonbase class</strong></p>
<p>Subclass of a parent class. Members of the subclass inherit all attributes of the parent class. Subclass may add new member attributes. 2 types:</p>
<ol>
<li>Restrict: a predicate identifies which members of the parent belong to a subclass.</li>
<li>Subset: A human user decides entities in the subclass as long as the subclass is a subset of its parent.</li>
</ol>
<p>Attribute value: either an entity or a class of entities. It can be UNKNOWN.</p>
<p>Semantic types:</p>
<ol>
<li>A componenet models a physical part of an object.</li>
<li>A participant of the event entity models an entity that plays a role in an event.</li>
<li>A property of an attribute is an attribute that provides further information on the relationship between the entity and the value of one of its attributes.</li>
</ol>
<p>Questions:</p>
<p><strong>Why is it important for a relational schema to satisfy the 5 normal forms?</strong></p>
<p>To ensure the data integrity and consistency, and minimize the loss and redundancy of information.</p>
<p>1NF: all occurences of a record type must contain the same number of fields.</p>
<p>2 and 3NF: a non-key attribute is a fact about only the whole key.</p>
<p>4NF: a record should not contain two or more independent multi-valued fact about an entity.</p>
<p>5NF: decompose a table into smaller ones to eliminate multi-valued dependencies, while ensuring that the original data can be losslessly reconstructed through join operation.</p>
<p><strong>With SDM, what is the unique identifier of a class containing duplicates?</strong></p>
<p>There is no unique identifier of a class containing duplicates since some of the members of this class are indistinguishable.</p>
<p><strong>Is SDM a competitor to the relational data model?</strong> </p>
<p>Yes. But SDM is not intended to be a direct competitor to the relational data model. The goal of SDM is to provide a more semantic way to model complex application environments, expressing the structure and meaning of data more effectively than tradtional relational models. It is designed to enhance the relational data model.</p>
<p><strong>A database represents a snapshot of the state of an application and the changes to the database over time. What is the change in a 3D display that illuminates animations using FLSs? Does an FLS display represent a database using drones?</strong> </p>
<p>A 3D FLS display illuminates animations by computing the flight paths of the FLS drones based on the dynamic attributes of objects, such as geometry, color, and movement over time. Changes in the display represent transitions in the object’s states or visual properties.</p>
<p>Yes, the FLS display act as a dynamic visualization tool that represents a database where each drone corresponds to data points or entities, displaying multimedia content in a 3D space.</p>
<p><strong>Section 1 of the SDM paper states: “SDM is not dependent on the successful implementation of a new database management system that directly supports the SDM. The latter approach would be impractical for a variety of reasons.” Why is it impractical to implement a new database management system that supports the SDM? Do you know of a system?</strong></p>
<p>SDM emphasizes the <strong>meaning</strong> and <strong>relationship</strong> of data, requiring sophisticated handling of semantics, which adds complexity. Integrating SDM into existing systems will cause incompatibility with current DBMS architectures.</p>
<p>While there is not a widely adopted DBMS that fully supports SDM, some graph databases or knowledge graph systems such as <strong>Neo4j</strong>, <strong>RDF stores</strong> partially align with SDM principles.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/509252.509264">https://dl.acm.org/doi/pdf/10.1145/509252.509264</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/20/Nova-LSM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/20/Nova-LSM/" class="post-title-link" itemprop="url">Nova-LSM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-20 00:00:00" itemprop="dateCreated datePublished" datetime="2024-09-20T00:00:00-07:00">2024-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-30 19:44:35" itemprop="dateModified" datetime="2025-05-30T19:44:35-07:00">2025-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Nova-LSM"><a href="#Nova-LSM" class="headerlink" title="Nova-LSM"></a>Nova-LSM</h1><p>LSM-Tree（Log-Structured Merge Tree）的核心思想是将大量的随机写入转换为更高效的顺序写入。简单来说，它通过以下方式来实现：</p>
<ol>
<li><strong>写入内存</strong>：当有新的数据写入时，LSM-Tree首先将这些数据存储在内存中的缓冲区（称为MemTable）。这是一个有序的结构，数据按键排序。</li>
<li><strong>批量写入磁盘</strong>：当内存中的数据积累到一定程度时，整个MemTable会被一次性地写入磁盘，这个过程是<strong>顺序写入</strong>，非常高效。写入磁盘后，这个数据成为一个不可修改的文件，称为SSTable（Sorted String Table）。</li>
<li><strong>合并和压缩</strong>：随着时间的推移，磁盘上会产生多个SSTable。为了优化读取性能，系统会周期性地将这些SSTable进行合并和压缩，使得数据保持有序并减少冗余。</li>
</ol>
<p>这样，LSM-Tree通过将频繁的随机写操作缓存在内存中，最后批量顺序写入磁盘，大大提高了写入性能。这种方式适合写入密集型的工作负载，同时还能保证数据查询的效率。</p>
<p><strong>LSM-Tree的基础结构</strong>，特别是数据如何从内存（memtable）移动到磁盘，并经过多级的归并排序（compaction）过程来进行存储。</p>
<p><img src="/../images/nova_lsm_basic_structure.png" alt="img"></p>
<ol>
<li><p>MemTable（内存表）</p>
<ul>
<li><p>数据的写入首先进入到内存中的memtable，通常是一个有序的数据结构（比如跳表或B+树），这使得数据在内存中是有序的，便于快速写入和查询。</p>
</li>
<li><p>当memtable满了或者系统需要将数据持久化时，memtable中的数据会被flush（刷新）到磁盘，形成第一层的SSTable。</p>
</li>
</ul>
</li>
<li><p>Level-0（磁盘上的第一层）</p>
<ul>
<li><p>数据从内存写入磁盘后，存储在Level-0层的SSTable中。此时，SSTable的数据顺序与memtable一致，但可能存在多个SSTable，且它们之间的键值范围可能重叠。</p>
</li>
<li><p>Level-0的SSTable是逐渐积累的，并不会自动排序或整理，直到执行compaction（归并操作）。</p>
</li>
</ul>
</li>
<li><p>Compaction（归并操作）</p>
<ul>
<li><p>当Level-0层的数据达到一定量时，系统会执行归并操作，将Level-0层的多个SSTable合并，并将合并后的有序数据移到Level-1层。</p>
</li>
<li><p>Level-1开始，所有的SSTable都是有序且互不重叠的。也就是说，每个SSTable都有自己独立的键值范围，不会与其他SSTable的键值范围重叠，这使得查询时能够快速定位到目标SSTable。</p>
</li>
</ul>
</li>
<li><p>逐级沉降</p>
<ul>
<li><p>数据会随着系统运行，从Level-0层逐步沉降到更深的层级（如Level-1、Level-2等）。在每一层，数据都通过归并操作变得更加有序且结构紧凑。</p>
</li>
<li><p>每次合并后，数据被重新整理，分配到新的不重叠的SSTable中，从而保持物理上的键值有序性。</p>
</li>
</ul>
</li>
</ol>
<p><strong>LSM-Tree查询</strong></p>
<p>基于LSM-Tree的查询可分为点查与范围查询两大类，对应的执行方式如下：</p>
<ul>
<li>点查（point lookup）：从上往下进行查询，先查memtable，再到L0层、L1层。因为上层的数据永远比下层版本新，所以在第一次发生匹配后就会停止查询。</li>
<li>范围查询（range lookup）：每一层都会找到一个匹配数据项的范围，再将该范围进行<strong>多路归并</strong>，归并过程中同一key只会保留最新版本。</li>
</ul>
<p><strong>LSM-Tree性能的衡量</strong>主要考虑三个因素：空间放大、读放大和写放大。</p>
<p>一是空间放大（space amplification）。LSM-Tree的所有写操作都是顺序追加写，对数据的更新并不会立即反映到数据既有的值里，而是通过分配新的空间来存储新的值，即out-place update。因此冗余的数据或数据的多版本，仍会在LSM-Tree系统里存在一定时间。这种实际的占用空间大于数据本身的现象我们称之为空间放大。因为空间有限，为了减少空间放大，LSM-Tree会从L1往L2、L3、L4不断做compaction，以此来清理过期的数据以及不同数据的旧版本，从而将空间释放出来。</p>
<p>二是读放大（read amplification）。假设数据本身的大小为1k，由于存储结构的设计，它所读到的值会触发多次IO操作，一次IO意味着一条读请求，这时它所读取到的则是在后端所需要做大的磁盘读的实际量，已经远大于目标数据本身的大小，从而影响到了读性能。这种现象我们称之为读放大。为了减轻读放大，LSM-Tree采用布隆过滤器来避免读取不包括查询键值的SST文件。</p>
<p>三是写放大（write amplification）。在每层进行compaction时，我们会对多个SST文件进行反复读取再进行归并排序，在删掉数据的旧版本后，再写入新的SST文件。从效果上看，每条key在存储系统里可能会被多次写入，相当于一条key在每层都会写入一次，由此带来的IO性能损失即写放大。</p>
<p>LSM-Tree最初的理念是用空间放大和读放大来换取写放大的降低，从而实现较好的写性能，但也需要做好三者的平衡。以下是两种假设的极端情况。</p>
<p>第一种极端情况是：如果完全不做compaction，LSM-Tree基本等同于log文件，当memtable不断刷下来时，由于不做compaction，只做L0层的文件，这时如果要读一条key，读性能会非常差。因为如果在memtable里找不到该条key，就要去扫描所有的SST文件，但与此同时写放大现象也将不存在。</p>
<p>第二种极端情况是：如果compaction操作做到极致，实现所有数据全局有序，此时读性能最优。因为只需要读一个文件且该文件处于有序状态，在读取时可以很快找到对应的key。但要达到这种效果，需要做非常多的compaction操作，要不断地把需要删的SST文件读取合并再来写入，这会导致非常严重的写放大。</p>
<p><strong>Nova-LSM架构设计</strong></p>
<p><img src="/../images/nova_lsm_arch.png" alt="img"></p>
<p>第一部分是写日志的组件，将WAL写成功后再往LSM-Tree的memtable中查询新的数据。</p>
<p>第二部分是本身处理LSM-Tree写入的线程，其缩写为LTC(LSM-Tree Component)，代表着将该线程单独组件化。</p>
<p>第三部分则是底层的存储，负责把接收到的上层LTC组件下发下来，并提供标准的文件接口。</p>
<p><strong>Nova-LSM所解决的核心问题</strong></p>
<p>第一个核心问题是：基于LSM-Tree结构的存储系统，例如LevelDB、RocksDB等，都会不可避免地遇到缓写或者停写的问题。比如内存里的memtable，在配置时最多可以写8个，因为写入多，需要全部flush到磁盘上。与此同时，当前L0层的SST文件非常多，L0层即将开始做compaction。但compaction会涉及到磁盘IO，在还没做完时，就会阻塞内存中的memtable对L0层SST进行flush的过程。当flush无法进行时，就会发生缓写，随着阈值的推进，在实在写不进时甚至会停写，这种现象体现在客户端就是请求掉零。</p>
<p>为了解决LSM-Tree结构存储系统中的缓写、停写问题，该文章提出了两个解决办法：</p>
<ul>
<li>第一种方法是设计了<strong>存算分离</strong>的架构体系，具体如上图所示。该架构的重要作用之一，是把处理写入和处理磁盘IO的两大主力模块拆分，计算存储分离，<strong>哪个部分慢就为哪个部分增加节点</strong>以此来提高该部分的能力，这是比较亮眼的突破。</li>
<li>第二种方法是引入了<strong>动态分区</strong>，即Drange机制。该机制的目的是为了让业务的写入压力，在LTC即计算层的memtable上进行区间划分，每个range都有自己的memtable，通过区间划分，从而<strong>实现多个range之间进行并行compaction</strong>。以L0层为例，我们可以把L0层变成没有互相重叠的状态，这时我们就可以对L0层进行并行的compaction，可以加快L0层的文件的消化，从而减轻对memtable flush到磁盘上的过程的影响。</li>
</ul>
<p>第二个核心问题是：在这种方式下需要划分很多不同的Drange，每个range都会增加一定的memtable数量，memtable数量的增加会影响scan和get的性能。假设有一个主请求，在原来所有数据都写在一个memtable里的情况下，在读取时，索引只需要面向这个memtable，再根据跳表进行get，如果get到则可以马上返回。现在划分成不同的Drange，memtable数量增加，因此需要查找的memtable以及L0层的SST也会变多。解决办法是：实现了一个索引，可以查询到一个key在memtable或L0 SST中的最新值（若存在）。</p>
<p><strong>Nova-LSM 中的重要设计</strong></p>
<p>LTC和StoCs之间的写数据流程：</p>
<p>第一个比较重要的设计是LTC和StoCs之间的写数据流程。该流程展示的是：当在客户端发起写请求时，计算节点和存储节点是以怎样的方式将数据写进去的过程。</p>
<p>首先是计算节点的客户端发起一个新的写请求操作。存储节点在接收到该请求后，基于RDMA交互，它会在buffer区域分配一个内存区域，并且为这块内存和偏移量（当前哪块内存可以写）分配一个id，告知客户端。客户端接到响应后就会开始写数据，完成后会通知存储节点。存储节点接收到信号后，将数据持久化并且再告知客户端。</p>
<p>上述流程是写一个数据文件即SSTable。写完后，我们要以同样的流程将元数据文件更新。因为底层是分布式架构，需要知道哪些文件写在哪里以及每个SST的范围、版本号。</p>
<p><img src="/../images/nova_lsm_key_design.png" alt="img"></p>
<p>动态区间划分：</p>
<p>第二个比较重要的设计是动态区间划分。假设业务的请求范围为0-1万，当前有10个计算节点，将这10个计算节点的区间划分为10等份，比如第一个key的空间范围为0-1000。在负责0-1000的计算节点里，它会再进行划分，这一层划分业务无感知。这就叫动态区间划分，简称Drange。其作用主要有以下几点：</p>
<p>首先，每个range都是一棵LSM-Tree，按照数据区间，不同的Drange都有自己的memtables。比如0-1000区间又可以划分为10个Drange，10个Drange之间的memtable相互独立。这样做的好处是这些Drange之间的key互不重叠，例如0-100、100-200、200-300。</p>
<p>其次，在Dranges下还有一层Tranges。如果发现Drange里的部分range比如890-895存在热点现象，而旁边的range并非热点，则可以用Tranges进行细粒度的复杂重均衡，实现动态均衡负载。</p>
<p>最后，在此基础上，因为Drange的key范围互不相交，当memtable变成immutable，不可再写后，它们需要独立地flush到磁盘上。这时，在L0层的SSTable来自不同的Drange，它们之间的key完全不相交，我们就可以进行并行的compaction。</p>
<p><img src="/../images/nova_lsm_key_design_2.png" alt="img"></p>
<p>文章还将没有Drange划分和有Drange划分两种情况进行了对比：</p>
<ul>
<li>在没有Drange划分的情况下，L0的compaction无法很好并行。在这种情况下，如果遇到最坏的情况，L0层的某一个SST有可能覆盖了整个key空间，假设key范围为0-600，L0层的SST文件的范围是0-1000，当发生compaction时，它必须要跟其他4个SST做归并，这时不但要把L0层的其他SST全部读取比较一遍，还要把L1层所有的SST都读一遍再做归并排序。这时写放大会较为严重，意味着L0层到L1层的compaction会变慢，flush也会变慢，甚至flush不了时，前端就会出现缓写、停写现象。</li>
<li>有Drange划分后，相当于compaction可以分开区间，如下方的示意图所示。在0-100区间，L0到L1可以独立去compaction，100-200区间也可以独立去compaction，可以较好地实现并行compaction。而在原生的RocksDB里，只有从L1开始compaction，才能进行并行compaction操作。</li>
</ul>
<p>索引查找以及Scan操作：</p>
<p>因为划分了很多不同的动态区间，memtable的数量也会增加，意味着查询操作的耗时也会增加。所以要如何在原来的基础上维护好读性能？这篇文章提出了以下解决思路：</p>
<p>每个LTC维护了一个lookup index。如果这些数据存在于memtable和L0层的SST上，通过lookup index我们就可以快速查找到想要的数据。当某一个L0层SST被compaction到L1层时，索引上就会移除掉对应的key。</p>
<p>LTC同时还维护了一个范围索引即range index。因为知道每个Drange的范围，所以当一个scan请求所涉及到的key都可以在memtable和L0层SST中找到时，该范围索引就能快速响应scan操作。</p>
<p><img src="/../images/nova_lsm_key_design_3.png" alt="img"></p>
<p>SSTable的分布：</p>
<p>最后一个比较重要的设计涉及到存储层。当某个SST文件要写到存储节点时，分布式系统首先要保证负载均衡，要保证数据避免单点故障不可恢复的场景。</p>
<p>该文章提出根据一定策略，将数据文件即SST打散写入到多个存储节点里。考虑到存储成本，每个SSTable采用纠删码（Erasure Coding）的方式进行编码然后分布式存放。默认情况下对每个 SSTable 采用 “3+1”的 EC 配置，将一个SSTable切分为3个数据块，根据一定算法，在这3个数据块里去计算出一个校验块，变成了“3+1”的形式。这种方式比传统的多副本可以<strong>节省更多空间</strong>。假设一个SSTable是3M，这种“3+1”的方式最终所占空间为4M，并且<strong>能容忍一个节点的丢失</strong>，与占用6M空间的双副本方案拥有同样的故障容忍等级。而元数据文件因为体积比较小，所以直接采用多副本存储的方式，比如1个元数据文件可以写3个副本。</p>
<p>Challenges and Solutions:</p>
<ol>
<li><p>Write Stalls, the solutions are:</p>
<ol>
<li><p>Vertical scaling: use large memory.</p>
</li>
<li><p>Horizontal scaling: use the bandwidth of many StoCs.</p>
</li>
</ol>
</li>
<li><p>Scans are slowed down, the solutions are:</p>
<ol>
<li><p>Construct Dranges at runtime based on workload. Drange faciliates parallel compaction.</p>
</li>
<li><p>Construct range index dynamically.</p>
</li>
</ol>
</li>
<li><p>Gets are slowed down, the solution is: Use lookup index.</p>
</li>
<li><p>Temporary Bottlenecks, the solution is:</p>
<ol>
<li><p>Scatter blocks of a SSTable across multiple StoCs.</p>
</li>
<li><p>Power-of-d: power-of-d is applied in Nova-LSM to help with load balancing during SSTable placement. When writing data to storage components (StoCs), Nova-LSM doesn’t randomly select just one StoC. Instead, it chooses d StoCs at random and writes to the one with the shortest queue. This method helps avoid bottlenecks and improves throughput, ensuring that data is distributed evenly across storage nodes without overwhelming any individual node.</p>
</li>
</ol>
</li>
<li><p>Logging, the solution is: Replicating Log records in the memory of StoCs to provide high availability.</p>
</li>
<li><p>Skewed Access Pattern, the solution is: Dranges enable LTC to write 65% less data to StoCs with skewed data access.</p>
</li>
</ol>
<p>Questions:</p>
<p><strong>Why do modern database systems disaggregate compute from storage?</strong></p>
<p>Modern database systems disaggregate compute from storage to improve scalability, resource utilization, and fault isolation. By separating compute (processing) and storage, the system can independently scale each based on demand. Compute nodes handle processing, while storage nodes handle data access, optimizing resources and ensuring that failures in one component don’t impact the other. This separation also benefits cloud environments, where elastic scaling of resources is crucial.</p>
<p><strong>How does Nova-LSM provide superior performance than monolithic data stores?</strong> </p>
<p>Nova-LSM improves performance by using a component-based architecture that disaggregates processing (LTC) and storage (StoC). It allows components to scale independently and uses RDMA for fast communication. Nova-LSM also introduces dynamic range partitioning (Dranges), allowing parallel compaction and reducing write stalls, which significantly enhances throughput. This architecture minimizes bottlenecks seen in monolithic stores like LevelDB and RocksDB, especially under skewed workloads.</p>
<p><strong>Why does the standard cost-based optimizer produce sub-optimal query plans? How does Kepler improve both the query planning time and query execution time?</strong></p>
<p>The standard cost-based optimizer can produce sub-optimal plans because it relies on simplified and static cost models that don’t always capture real execution costs, especially in dynamic environments. It also may lack up-to-date statistics, leading to inaccurate decisions. Kepler, on the other hand, uses machine learning to learn from past executions and adapts to current data distributions, improving query plan selection. By pruning the search space efficiently and using real-time data, it reduces both planning time and execution time while optimizing performance.</p>
<p>References: </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2002523?areaSource=102001.3&traceId=XIO8WvF-vqiMAsiAKu2Lv">https://cloud.tencent.com/developer/article/2002523?areaSource=102001.3&amp;traceId=XIO8WvF-vqiMAsiAKu2Lv</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3448016.3457297">https://dl.acm.org/doi/pdf/10.1145/3448016.3457297</a></p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/06/Gamma/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/06/Gamma/" class="post-title-link" itemprop="url">Gamma</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-06 00:00:00" itemprop="dateCreated datePublished" datetime="2024-09-06T00:00:00-07:00">2024-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-29 19:45:44" itemprop="dateModified" datetime="2025-05-29T19:45:44-07:00">2025-05-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Gamma"><a href="#Gamma" class="headerlink" title="Gamma"></a>Gamma</h1><p>主要特点：</p>
<ol>
<li><strong>并行处理</strong>： Gamma 利用了分布式架构，通过将数据和计算任务分散到多个节点上并行处理，极大提高了查询性能和吞吐量。不同的节点可以同时处理不同的任务，从而加速整个系统的响应时间。</li>
<li><ul>
<li><strong>并行查询处理</strong>： Gamma 支持并行执行查询计划中的操作（如选择、投影、连接等）。系统采用流水线并行（pipelined parallelism）和分块并行（partitioned parallelism）技术来最大化资源利用率。</li>
<li><strong>流式处理（Pipelining）</strong>： Gamma 支持流式处理，即在一个操作产生部分结果时，直接将这些结果传递给下一个操作，而不是等待整个操作完成。这样可以减少内存占用，并加快查询处理速度。</li>
</ul>
</li>
<li><strong>数据分片（Declustering）</strong>： Gamma 系统通过数据分片将数据表水平拆分成多个片段，并将这些片段分布到不同的处理节点上。这种方式不仅均衡了负载，还支持并行的查询处理，避免单点瓶颈。</li>
<li><strong>动态负载均衡</strong>： Gamma 能够根据查询的工作负载，动态分配任务到不同的节点，确保整个系统的负载均衡，避免某些节点过载导致性能下降。通过监控每个节点的工作情况，Gamma 能够优化数据和任务分布。</li>
<li><strong>故障容错（Fault Tolerance）</strong>： Gamma 具有一定的故障容错能力，当某个节点出现故障时，系统可以通过冗余机制和数据复制，重新分配任务或从其他节点获取数据，避免系统中断。</li>
<li><strong>扩展性（Scalability）</strong>： Gamma 系统的设计能够随着节点的增加而线性扩展。通过增加处理节点，Gamma 可以处理更大规模的数据和更多的并发查询，保持高性能。</li>
</ol>
<p>Gamma is based on the concept of a shared-nothing architecture in which processors do not share disk drives or random access memory and can only communicate with one another by sending messages through an interconnection network. Mass storage in such an architecture is generally distributed among the processors by connecting one or more disk drives to each processor.</p>
<p><img src="/../images/Gamma_Arch.png" alt="Gamma 架构"></p>
<p>Reasons why the shared-nothing approach has become the architecture of choice.</p>
<ul>
<li>There is nothing to prevent the architecture from <strong>scaling</strong> to 1000s of processors unlike shared-memory machines for which scaling beyond 30-40 processors may be impossible.</li>
<li>By associating a small number of disks with each processor and distributing the tuples of each relation across the disk drives, it is possible to achieve very high aggregate I&#x2F;O bandwidths without using custom disk controllers</li>
</ul>
<p>When Gamma’s system is figuring out the best way to run a query, it uses information about how the data is divided up (partitioned). This partitioning information helps the system decide how many processors (computers) need to be involved in running the query.</p>
<ul>
<li><strong>For hash partitioning</strong>: If a table (say, “X”) is divided based on a hash function applied to a certain column (like “y”), and the query asks for records where “X.y &#x3D; some value,” <strong>the system can directly go to the specific processor that holds the data matching that value</strong>.</li>
<li><strong>For range partitioning</strong>: If the table is divided based on ranges of values for a column, <strong>the system can limit the query to only the processors that have data within the relevant range</strong>. For example, if “X” is partitioned such that one processor handles values from 1 to 100, and another handles values from 101 to 200, then a query asking for “X.y between 50 and 150” will involve only the processors that have data in those ranges.</li>
</ul>
<p>Different processes in the Gamma system work together. Here’s a simplified explanation of the main types of processes and their roles:</p>
<ol>
<li><p><strong>Catalog Manager</strong>: Acts like a “database encyclopedia,” <strong>storing all the information about data tables and structures</strong>. It ensures that data remains consistent when multiple users access the database.</p>
</li>
<li><p><strong>Query Manager</strong>: Each user gets a query manager that handles query requests. It is responsible for <strong>parsing the query, optimizing it, and generating the execution plan</strong>.</p>
</li>
<li><p><strong>Scheduler Processes</strong>: When a query is executed, the scheduler <strong>coordinates the execution steps</strong>. It activates the necessary operator processes (such as scan, selection, etc.) and ensures that all steps are performed in the correct order.</p>
</li>
<li><p><strong>Operator Processes</strong>: These processes <strong>carry out specific database operations</strong>, like filtering data or joining tables. To reduce the startup delay during query execution, some operator processes are pre-initialized when the system starts.</p>
</li>
<li><p><strong>Other Processes</strong>:</p>
<ul>
<li><p><strong>Deadlock Detection Process</strong>: Detects situations where two or more processes are stuck waiting for each other to release resources.</p>
</li>
<li><p><strong>Recovery Process</strong>: Manages data recovery after a system failure.</p>
</li>
</ul>
</li>
</ol>
<p>How the Gamma system executes database queries?</p>
<ol>
<li><strong>Query Parsing and Optimization</strong>: When a user submits a query, Gamma first parses it to understand what the query is asking for. Then, the system optimizes the query to find the most efficient way to execute it.</li>
<li><strong>Query Compilation</strong>: After optimization, the query is compiled into an “<strong>operator tree</strong>“ made up of different operations (such as scan, selection, join, etc.). This tree outlines the steps and the order in which the query will be executed.</li>
<li><strong>Single-Site vs. Multi-Site Queries</strong>: If the query only involves data on a single node (e.g., querying a small table), the system executes it directly on that node. However, if the query involves data distributed across multiple nodes (e.g., joining large tables), the system uses a “scheduler process” to coordinate the execution.</li>
<li><strong>Scheduler Coordination</strong>: The scheduler process is responsible for activating various operator processes across the nodes, such as instructing one node to scan data while another filters it. The scheduler also manages the flow of data between these operations, ensuring they happen in the correct order.</li>
<li><strong>Returning the Results</strong>: Once all operations are completed, the query results are collected and returned to the user. For queries embedded in a program, the results are passed back to the program that initiated the query.</li>
</ol>
<p>Different operations (like scanning data, filtering, joining tables, etc.) are carried out in a parallel manner. Here’s a simplified explanation:</p>
<ol>
<li><strong>Operator Processes</strong>: In Gamma, each operation in a query is handled by something called an “operator process.” For example, if the query needs to scan data from a table, filter some rows, and then join with another table, there would be separate operator processes for scanning, filtering, and joining.</li>
<li><strong>Data Flow</strong>: The data flows from one operator process to the next. For instance, the scan operator reads data from the disk and sends it to the filter operator, which then passes the filtered results to the join operator. This creates a kind of “data pipeline.”</li>
<li><strong>Split Table</strong>: Gamma uses a “split table” to decide where the data should go next. Think of it like a routing table that directs the flow of data. For example, if the data needs to be sent to multiple nodes for parallel processing, the split table helps determine which node each piece of data should go to.</li>
<li><strong>End of Processing</strong>: Once an operator finishes processing all its data, it closes its output streams and sends a signal to the scheduler process (which coordinates the whole query) to let it know that this part of the work is done.</li>
</ol>
<p>In simple terms, the operator and process structure in Gamma is like an assembly line where data moves from one step (operator) to the next, with each operator performing a specific task, and the split table guiding the data flow. This setup allows the system to process data in parallel across multiple nodes, making it much faster.</p>
<p><strong>Selection Operator</strong></p>
<p>Data Spread Across Multiple Disks: In Gamma, data tables are split up and stored across multiple disks (this is called “declustering”). Because of this, when you want to search (select) for specific data, the system can perform the search in parallel across multiple disks.</p>
<p>Parallel Selection Process:</p>
<ul>
<li>If the search condition (predicate) matches the way the data is divided (partitioned), the system can narrow down the search to just the relevant nodes (computers with disks) that have the data. For example:</li>
<li><ul>
<li>If the data is divided using a <strong>hash or range</strong> partitioning method based on a certain attribute (like “employee ID”), and the search is also based on that attribute (e.g., “employee ID &#x3D; 123”), then the search can be directed only to the node that holds data matching that condition.</li>
<li>If the data is divided using a <strong>round-robin</strong> method (spreading data evenly across all disks) or if the search condition <strong>doesn’t match the partitioning attribute</strong>, then the system has to search on all nodes.</li>
</ul>
</li>
</ul>
<p>Performance Optimization:</p>
<ul>
<li>To make the search faster, Gamma uses a “<strong>read-ahead</strong>“ technique. This means that when it reads one page of data, it starts loading the next page at the same time, so that the processing of data can keep going without waiting for the next page to load.</li>
</ul>
<p><strong>Join Operator</strong></p>
<p>Using Hash Partitioning: The join algorithms in Gamma are based on a concept called “buckets.” This means splitting the two tables to be joined into separate groups (buckets) that don’t overlap. The groups are created by applying a hash function to the join attribute (e.g., Employee ID), so that data with the same hash value ends up in the same bucket.</p>
<p>By partitioning the data into different buckets, each bucket contains unique data subsets, allowing parallel processing of these buckets, which speeds up the join operation. Additionally, <strong>all data with the same join attribute value is in the same bucket</strong>, making it easier to perform the join.</p>
<p>Gamma implements four different parallel join algorithms:</p>
<ul>
<li><strong>Sort-Merge Join</strong>: Joins data by sorting and merging.</li>
<li><strong>Grace Join</strong>: A distributed hash-based join algorithm.</li>
<li><strong>Simple Hash Join</strong>: A straightforward hash-based partitioning join.</li>
<li><strong>Hybrid Hash Join</strong>: A combination of different join techniques.</li>
</ul>
<p><strong>Default to Hybrid Hash Join:</strong> Research showed that the Hybrid Hash Join almost always performs the best, so Gamma uses this algorithm by default.</p>
<p>Limitations: These hash-based join algorithms can <strong>only handle equi-joins</strong> (joins with equality conditions, like “Employee ID &#x3D; Department ID”). They currently don’t support non-equi-joins (conditions like “Salary &gt; Department Budget * 2”). To address this, Gamma is working on designing a new parallel non-equi-join algorithm.</p>
<p><strong>Hybrid Hash-Join</strong></p>
<ul>
<li>In the first phase, the algorithm uses a hash function to partition the inner (smaller) relation, R, into N buckets. The tuples of the first bucket are used to build an in-memory hash table while the remaining N-1 buckets are stored in temporary files. A good hash function produces just enough buckets to ensure that each bucket of tuples will be small enough to fit entirely in main memory.</li>
<li>During the second phase, relation S is partitioned using the hash function from step 1. Again, the last N-1 buckets are stored in temporary files while the tuples in the first bucket are used to immediately probe the in-memory hash table built during the first phase.</li>
<li>During the third phase, the algorithm joins the remaining N-1 buckets from relation R with their respective buckets from relation S.</li>
</ul>
<p>The join is thus broken up into a series of smaller joins; each of which hopefully can be computed without experiencing join overflow. The size of the smaller relation determines the number of buckets; this calculation is independent of the size of the larger relation.</p>
<p><strong>Parallel version of Hybrid Hash-Join</strong></p>
<p>Partitioning into Buckets: The data from the two tables being joined is first divided into N buckets (small groups). The number of buckets is chosen so that each bucket can fit in the combined memory of the processors that are handling the join.</p>
<p>Storage of Buckets: Out of the N buckets, N-1 buckets are stored temporarily on disk across different disk sites, while one bucket is kept in memory for immediate processing.</p>
<p>Parallel Processing: A joining split table is used to decide which processor should handle each bucket, helping to divide the work across multiple processors. This means that <strong>different processors can work on different parts of the join at the same time</strong>, speeding up the process.</p>
<p>Overlapping Phases for Efficiency:</p>
<ul>
<li>When partitioning the <strong>first table (R)</strong> into buckets, Gamma simultaneously builds a hash table for the first bucket in memory at each processor.</li>
<li>When partitioning the <strong>second table (S)</strong>, Gamma simultaneously performs the join for the first bucket from S with the first bucket from R. This way, partitioning and joining overlap, making the process more efficient.</li>
</ul>
<p>Adjusting the Split Table for Parallel Joining: The joining split table is updated to make sure that the data from the first bucket of both tables is sent to the right processors that will perform the join. When the remaining N-1 buckets are processed, only the routing for joining is needed.</p>
<p><strong>Aggregate Operator</strong></p>
<p>Parallel Calculation of Partial Results: Each processor in the Gamma system calculates the aggregate result for its own portion of the data simultaneously. For example, if the goal is to calculate a sum, each processor will first compute the sum for the data it is responsible for.</p>
<p>Combining Partial Results: After calculating their partial results, the processors send these results to a central process. This central process is responsible for combining all the partial results to produce the final answer.</p>
<p>Two-Step Computation:</p>
<ul>
<li><strong>Step 1</strong>: Each processor calculates the aggregate value (e.g., sum, count) for its data partition, resulting in partial results.</li>
<li><strong>Step 2</strong>: The processors then redistribute these partial results based on the “group by” attribute. This means that the partial results for each group are collected at a single processor, where the final aggregation for that group is completed.</li>
</ul>
<p><strong>Update Operator</strong></p>
<p>For the most part, the update operators (replace, delete, and append) are implemented using standard techniques. The only exception occurs when a replace operator modifies the partitioning attribute of a tuple. In this case, rather than writing the modified tuple back into the local fragment of the relation, the modified tuple is passed through a split table to determine which site should contain the tuple.</p>
<p><strong>Concurrency Control</strong></p>
<p>Gamma uses a two-phase locking strategy to manage concurrency. This means that before accessing data, a process must first acquire locks (first phase), and then release the locks after completing its operations (second phase). This ensures that multiple operations do not modify the same data at the same time, preventing conflicts.</p>
<p>Gamma supports two levels of lock granularity: file-level and page-level (smaller scope). There are also five lock modes:</p>
<ul>
<li><strong>S (Shared) Lock</strong>: Allows multiple operations to read the data simultaneously.</li>
<li><strong>X (Exclusive) Lock</strong>: Only one operation can modify the data, while others must wait.</li>
<li><strong>IS, IX, and SIX Locks</strong>: Used to manage locking at larger scopes, such as entire files, allowing different combinations of read and write permissions.</li>
</ul>
<p>Each node in Gamma has its own lock manager and deadlock detector to handle local data locking. The lock manager maintains a lock table and a transaction wait-for-graph, which tracks which operations are waiting for which locks.</p>
<p>The cost of setting a lock depends on whether there is a conflict:</p>
<ul>
<li><strong>No Conflict</strong>: Takes about 100 instructions.</li>
<li><strong>With Conflict</strong>: Takes about 250 instructions because the system needs to check the wait-for-graph for deadlocks and suspend the requesting transaction using a semaphore mechanism.</li>
</ul>
<p>Gamma uses a centralized deadlock detection algorithm to handle deadlocks across nodes:</p>
<ul>
<li>Periodically (initially every second), the centralized deadlock detector requests each node’s local wait-for-graph.</li>
<li>If no deadlock is found, the detection interval is doubled (up to 60 seconds). If a deadlock is found, the interval is halved (down to 1 second).</li>
<li>The collected graphs are combined into a global wait-for-graph. If a cycle is detected in this global graph, it indicates a deadlock.</li>
</ul>
<p>When a deadlock is detected, the system will abort the transaction holding the fewest locks to free up resources quickly and allow other operations to proceed.</p>
<p><strong>Recovery and Log</strong></p>
<p>Logging Changes:</p>
<p>When a record in the database is updated, Gamma creates a log record that notes the change. Each log record has a unique identifier called a Log Sequence Number (LSN), which includes a node number (determined when the system is set up) and a local sequence number (which keeps increasing). These log records are used for recovery if something goes wrong.</p>
<p>Log Management:</p>
<ul>
<li>The system sends log records from query processors to <strong>Log Managers</strong>, which are separate processors that organize the logs into a single stream.</li>
<li>If there are multiple Log Managers (M of them), a query processor sends its logs to one of them based on a simple formula: <strong>processor number mod M</strong>. This way, each query processor always sends its logs to the same Log Manager, making it easy to find logs later for recovery.</li>
</ul>
<p>Writing Logs to Disk:</p>
<ul>
<li>Once a “page” of log records is filled, it is saved to disk.</li>
<li>The Log Manager keeps a <strong>Flushed Log Table</strong>, which tracks the last log record written to disk for each node. This helps know which logs are safely stored.</li>
</ul>
<p>Writing Data to Disk (WAL Protocol):</p>
<ul>
<li>Before writing any changed data (a <strong>dirty page</strong>) to disk, the system checks if the corresponding log records have already been saved.</li>
<li>If the logs are saved, the data can be safely written to disk. If not, the system must first ensure the logs are written to disk before proceeding.</li>
<li>To avoid waiting too long for log confirmations, the system always tries to keep a certain number of <strong>clean buffer pages</strong> (unused pages) available.</li>
</ul>
<p>Commit and Abort Handling:</p>
<ul>
<li><strong>Commit</strong>: If a transaction completes successfully, the system sends a commit message to all the relevant Log Managers.</li>
<li><strong>Abort</strong>: If a transaction fails, an <strong>abort message</strong> is sent to all processors involved, and each processor retrieves its log records to undo the changes using the <strong>ARIES algorithm</strong>, which rolls back changes in the reverse order they occurred.</li>
</ul>
<p>Recovery Process:</p>
<ul>
<li>The system uses the <strong>ARIES algorithms</strong> for undoing changes, checkpointing, and restarting after a crash.</li>
<li><strong>Checkpointing</strong> helps the system know the most recent stable state, reducing the amount of work needed during recovery.</li>
</ul>
<p><strong>Dataflow scheduling technologies</strong></p>
<ol>
<li><strong>Data-Driven Execution Instead of Operator Control</strong>: Gamma’s dataflow scheduling lets data automatically move between operators, forming a pipeline. Each operator acts like a step on an assembly line: when data reaches the operator, it processes the data and then passes the processed results to the next operator.</li>
<li><strong>Reducing Coordination Overhead</strong>: Because of this dataflow design, the system does not need to frequently coordinate or synchronize the execution of each operator. This approach reduces the complexity and overhead of scheduling, especially when multiple operators are running in parallel, and avoids performance bottlenecks caused by waiting or synchronization.</li>
<li><strong>Inherent Support for Parallelism</strong>: Dataflow scheduling is well-suited for parallel processing because data can flow between multiple operators at the same time. For example, a query can simultaneously perform scanning, joining, and aggregation across different processors. Each operator can independently process the data it receives without waiting for other operators to finish, allowing the system to efficiently utilize the computational power of multiple processors.</li>
<li><strong>Adaptability to Dynamic Environments</strong>: During query execution, dataflow scheduling can be adjusted based on the actual system load and data characteristics. This flexibility allows the system to dynamically optimize the performance of query execution, especially for large and complex queries, by better adapting to changing query demands and system conditions.</li>
</ol>
<p>Gamma’s unique dataflow scheduling techniques allow data to flow naturally between operators, reducing the need for direct control over operations. This significantly lowers coordination overhead in multi-processor environments, enhances the system’s parallel processing capabilities, and improves the efficiency of executing complex queries.</p>
<p>In Gamma’s dataflow scheduling techniques, parallelism is extensively used to improve query execution efficiency. Here’s where and how parallelism is applied:</p>
<ol>
<li><p><strong>Parallel Execution of Operators</strong>: Queries often involve multiple operators (e.g., scan, filter, join, aggregation). With dataflow scheduling, these operators can run in parallel:</p>
<ul>
<li><p><strong>Scan and Filter in Parallel</strong>: While one processor scans a data block, another processor can be filtering the data from previous blocks.</p>
</li>
<li><p><strong>Parallel Joins</strong>: If a join operation involves large datasets distributed across different nodes, Gamma can perform the join operation on these different parts of the data simultaneously. The result of the join is computed in parallel across multiple processors.</p>
</li>
</ul>
</li>
<li><p><strong>Data Partitioning for Parallelism</strong>: The relations (data tables) are often partitioned across multiple processors in Gamma. This means that different processors can work on different partitions of the data at the same time. For example:</p>
<ul>
<li><p><strong>Partitioned Hash Joins</strong>: Data can be split into “buckets” based on a hash function, and different processors can handle the join for different buckets simultaneously.</p>
</li>
<li><p><strong>Parallel Aggregation</strong>: When computing aggregate functions (e.g., sum or average), each processor calculates a partial result for its own partition of the data, and these partial results are later combined.</p>
</li>
</ul>
</li>
</ol>
<p>In summary, parallelism in Gamma is achieved through:</p>
<ul>
<li>Distributing query operators across multiple processors.</li>
<li>Partitioning data so different processors work on different sections simultaneously.</li>
<li>Enabling multiple stages of query execution (e.g., scanning, filtering, joining) to happen concurrently.</li>
</ul>
<p>Questions:</p>
<p><strong>What is a fragment or a shard in Gamma?</strong> </p>
<p>A fragment or shard refers to a portion of a database relation that is horizontally partitioned across multiple disk drives.</p>
<p><strong>How does a Gamma operator know where to send its stream of records?</strong> </p>
<p>There is a structure called split table to determine where each tuple should be sent, based on the values of tuples.</p>
<p><strong>With interleaved declusttering, why not use a cluster size that includes all nodes in the system?</strong></p>
<p>If an interleaved declustteing system includes all nodes, it will become more vulnerable to failures. The failure of any two nodes could make the data inaccessible. A smaller cluster will limits the risk of complete data unavailability and balance the load.</p>
<p><strong>Hash-join is appropriate for processing equi-join predicates (Emp.dno &#x3D; Dept.dno). How can Gamma process nonequi-join predicates (Emp.Sal &gt; Dept.dno*1000) in a pipelined manner?</strong></p>
<p><strong>Range partitioning</strong>: Pre-partition the data based on ranges of values to reduce the search space.</p>
<p><strong>Broadcast join</strong>: When the smaller relation is broadcasted to all nodes, and then each node evaluates the nonequi-join predicate in parallel.</p>
<p><strong>Nested-loop join</strong>: Use a nested-loop join strategy where each tuple from one relation is compared against all tuples from the other relation.</p>
<p><strong>What is the difference between Gamma, Google MapReduce, Microsoft Dryad and Apache Flink?</strong></p>
<table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th>Gamma</th>
<th>MapReduce</th>
<th>Dryad</th>
<th>Flink</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Primary Use</strong></td>
<td>Parallel database queries</td>
<td>Batch processing</td>
<td>Graph-based parallel computation</td>
<td>Stream and batch processing</td>
</tr>
<tr>
<td><strong>Architecture</strong></td>
<td>Shared-nothing, partitioned data</td>
<td>Cluster-based, distributed</td>
<td>DAG of tasks</td>
<td>Distributed, supports DAG</td>
</tr>
<tr>
<td><strong>Data Model</strong></td>
<td>Relational operations (SQL-like)</td>
<td>Key-value pairs</td>
<td>Data flow in DAG</td>
<td>Stream processing with state</td>
</tr>
<tr>
<td><strong>Partitioning</strong></td>
<td>Horizontal partitioning</td>
<td>Data split into chunks</td>
<td>Data partitioned across graph</td>
<td>Data partitioned into streams</td>
</tr>
<tr>
<td><strong>Fault Tolerance</strong></td>
<td>Limited</td>
<td>Checkpointing</td>
<td>Task-level recovery</td>
<td>State snapshots, exactly-once</td>
</tr>
<tr>
<td><strong>Programming</strong></td>
<td>Relational (SQL-style)</td>
<td>Functional (Map&#x2F;Reduce)</td>
<td>Sequential tasks in DAG</td>
<td>Functional, stream APIs</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Hundreds of processors</td>
<td>Horizontally across many nodes</td>
<td>Scales with more nodes</td>
<td>Highly scalable, stream and batch</td>
</tr>
<tr>
<td><strong>Use Case</strong></td>
<td>Database query processing</td>
<td>Log processing, data aggregation</td>
<td>Scientific computing</td>
<td>Real-time analytics, event processing</td>
</tr>
</tbody></table>
<p><strong>Will a version of Gamma using FLOW be more modular than its current design?</strong></p>
<p>Yes. FLOW enables more fine-grained control over the data flow and process interactions, which could simplify the addition of new operators and functionalities. It would also make the system easier to maintain and extend, as each component could be developed and optimized independently.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://pages.cs.wisc.edu/~dewitt/includes/paralleldb/ieee90.pdf">https://pages.cs.wisc.edu/~dewitt/includes/paralleldb/ieee90.pdf</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yihang Wei</p>
  <div class="site-description" itemprop="description">聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yihang Wei</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>

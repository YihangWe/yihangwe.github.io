<!DOCTYPE html>
<html lang="zh-CN,en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yihangwe.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
<meta property="og:type" content="website">
<meta property="og:title" content="EthanWeee">
<meta property="og:url" content="https://yihangwe.github.io/en/page/2/index.html">
<meta property="og:site_name" content="EthanWeee">
<meta property="og:description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yihang Wei">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yihangwe.github.io/en/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>EthanWeee</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start --><script>(function() {function calculateHeight(element) {let height = 0;const items = element.children;for (let i = 0; i < items.length; i++) {height += items[i].offsetHeight || 25;const child = items[i].querySelector(".nav-child");if (child && child.style.display !== "none") {height += calculateHeight(child);}}return height;}function generateToc(lang, container) {const content = document.getElementById(lang + "-content");if (!content) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));if (headers.length === 0) return;const ol = document.createElement("ol");ol.className = "nav";let currentLevel = 1;let currentOl = ol;let stack = [ol];let counters = [0, 0, 0, 0, 0, 0];headers.forEach((header, index) => {const level = parseInt(header.tagName[1]);counters[level - 1]++;for (let i = level; i < 6; i++) counters[i] = 0;const li = document.createElement("li");li.className = "nav-item nav-level-" + level;if (level === 1 && index === 0) {li.classList.add("active", "active-current");}const link = document.createElement("a");link.className = "nav-link";if (level === 1 && index === 0) link.classList.add("active");link.href = "#" + header.id;const numSpan = document.createElement("span");numSpan.className = "nav-number";numSpan.textContent = counters.slice(0, level).filter(n => n > 0).join(".");const textSpan = document.createElement("span");textSpan.className = "nav-text";textSpan.textContent = header.textContent;link.appendChild(numSpan);link.appendChild(document.createTextNode(" "));link.appendChild(textSpan);li.appendChild(link);if (level > currentLevel) {const newOl = document.createElement("ol");newOl.className = "nav-child";if (currentLevel === 1) {newOl.style.display = "block";stack[currentLevel - 1].lastElementChild.classList.add("active");}stack[currentLevel - 1].lastElementChild.appendChild(newOl);stack[level - 1] = newOl;currentOl = newOl;} else if (level < currentLevel) {currentOl = stack[level - 1];}currentOl.appendChild(li);currentLevel = level;});container.appendChild(ol);setTimeout(() => {const navHeight = calculateHeight(ol);ol.style.setProperty("--height", navHeight + "px");const navChilds = ol.getElementsByClassName("nav-child");Array.from(navChilds).forEach(child => {if (child.style.display === "block") {const childHeight = calculateHeight(child);child.style.setProperty("--height", childHeight + "px");}});}, 0);return ol;}function updateActiveHeading() {const activeLang = document.getElementById("en-content").style.display === "block" ? "en" : "zh";const content = document.getElementById(activeLang + "-content");const toc = document.getElementById(activeLang + "-toc");if (!content || !toc) return;const headers = Array.from(content.querySelectorAll("h1, h2, h3, h4, h5, h6"));const scrollPos = window.scrollY + window.innerHeight / 3;let activeHeader = null;for (let i = headers.length - 1; i >= 0; i--) {const header = headers[i];const headerTop = header.getBoundingClientRect().top + window.scrollY;if (headerTop <= scrollPos) {activeHeader = header;break;}}const links = toc.getElementsByClassName("nav-link");Array.from(links).forEach(link => {link.classList.remove("active");const parentLi = link.parentElement;if (parentLi) {parentLi.classList.remove("active", "active-current");}});if (activeHeader) {const activeLink = toc.querySelector(`a[href="#${activeHeader.id}"]`);if (activeLink) {activeLink.classList.add("active");let parent = activeLink.parentElement;while (parent && parent.classList) {if (parent.classList.contains("nav-item")) {parent.classList.add("active");if (parent.classList.contains("nav-level-1")) {parent.classList.add("active-current");}}parent = parent.parentElement;}}}}function initTippy() {document.querySelectorAll(".refplus-num").forEach((ref) => {if (ref._tippy) {ref._tippy.destroy();}let refid = ref.firstChild.href.replace(location.origin+location.pathname,"");let refel = document.querySelector(refid);if (!refel) return;let refnum = refel.dataset.num;let ref_content = refel.innerText.replace(`[${refnum}]`,"");tippy(ref, {content: ref_content,});});}function initToc() {const originalToc = document.querySelector(".post-toc-wrap");if (!originalToc || !document.getElementById("langToggle")) return;const tocContainer = document.createElement("div");tocContainer.className = "post-toc-wrap sidebar-panel sidebar-panel-active";const enToc = document.createElement("div");enToc.id = "en-toc";enToc.className = "post-toc motion-element";enToc.style.display = "block";const zhToc = document.createElement("div");zhToc.id = "zh-toc";zhToc.className = "post-toc motion-element";zhToc.style.display = "none";generateToc("en", enToc);generateToc("zh", zhToc);tocContainer.appendChild(enToc);tocContainer.appendChild(zhToc);originalToc.parentNode.replaceChild(tocContainer, originalToc);window.addEventListener("scroll", updateActiveHeading);setTimeout(updateActiveHeading, 0);}window.toggleLanguage = function() {const enContent = document.getElementById("en-content");const zhContent = document.getElementById("zh-content");const enToc = document.getElementById("en-toc");const zhToc = document.getElementById("zh-toc");const button = document.getElementById("langToggle");if (!enContent || !zhContent || !enToc || !zhToc || !button) return;const isEnglish = enContent.style.display === "block";enContent.style.display = isEnglish ? "none" : "block";zhContent.style.display = isEnglish ? "block" : "none";enToc.style.display = isEnglish ? "none" : "block";zhToc.style.display = isEnglish ? "block" : "none";button.querySelector(".button-text").textContent = isEnglish ? "Switch to English" : "切换中文";setTimeout(updateActiveHeading, 0);setTimeout(initTippy, 100);};document.addEventListener("DOMContentLoaded", function() {initToc();setTimeout(initTippy, 3000);});})();</script><style>.post-toc { transition: all 0.2s ease-in-out; }.post-toc .nav { padding-left: 0; }.post-toc .nav-child { padding-left: 1em; }.post-toc .nav-item { line-height: 1.8; }.post-toc .nav-link { color: #555; }.post-toc .nav-link:hover { color: #222; }.post-toc .nav-link.active { color: #fc6423; }.post-toc .active > .nav-link { color: #fc6423; }.post-toc .active-current > .nav-link { color: #fc6423; }</style><!-- hexo injector head_end end --><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EthanWeee</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/25/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20AnalyticDB%20Real-time%20OLAP%20Database%20System%20at%20Alibaba%20Cloud/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/25/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20AnalyticDB%20Real-time%20OLAP%20Database%20System%20at%20Alibaba%20Cloud/" class="post-title-link" itemprop="url">AnalyticDB - Real-time OLAP Database System at Alibaba Cloud</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-25 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-25T00:00:00-08:00">2025-01-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-31 23:05:39" itemprop="dateModified" datetime="2025-05-31T23:05:39-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">高级数据存储</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>The paper addresses 3 problems:</p>
<ol>
<li>How to process more complicated and diverse queries of users with low latency;</li>
<li>How to design a friendly and unified data layout that is compatible with column-stores and row-stores and is able to process the complex data type, with low latency;</li>
<li>How to process massive requests per second with low latency.</li>
</ol>
<p>The paper provides 5 novel ideas and elaborates the proposed approach for the implementation of each idea:</p>
<ol>
<li>Read&#x2F;write decoupling, a worker node processes either read or write operation, guarantees that read operations do not interfere with read operations. In the real-time read mode, version verification is introduced to ensure that each query retrieves the latest data, i.e., the $\text{latest version} &#x3D; max⁡(V_1,V_2)$. Furthermore, after each write operation is completed, the write node actively pulls the latest version to the corresponding read node to avoid high latency in subsequent read operations.</li>
<li>The hybrid row-columnar storage for storing complex-typed data, a detail file &#x3D; $n$ row groups, a row group &#x3D; $k$ data blocks, a data block &#x3D; $p$ FBlocks, a FBlock &#x3D; $x$ values of a certain column of a partial row to many rows.</li>
<li>Efficient index management includes the building and maintenance of indexes for both baseline data and incremental data. For baseline data, AnalyticDB builds inverted indexes and introduces a filter ratio to optimize read operations. For incremental data, AnalyticDB constructs lightweight sorted indexes on read nodes to expedite read operations before the asynchronous construction of the inverted index of this type of data is finished.</li>
<li>For the optimizer, AnalyticDB introduces the STARs framework to evaluate both the capability of storage and relational algebra ability and adopts dynamic programming, in order to achieve efficient predicate push-down. This database minimizes the cost of shuffling tables for join push-down. For index-based join and aggregation, it employs the LeftDeepTree to efficiently utilize index-on-all-columns while also pushing down predicates and aggregations. Furthermore, the optimizer and execution engine perform sampling-based cardinality estimation with caching previously sampled results, optimized sampling algorithm, and improved derived cardinality.</li>
<li>The execution engine is able to operate directly on serialized binary data rather than Java objects, eliminating the expensive costs of serialization and deserialization during the process of shuffling big data.</li>
</ol>
<p>Analytical and experimental findings: AnalyticDB outperforms PrestoDB, Druid, Spark-SQL, and Greenplum in performance on the 1TB dataset, and its performance is not affected dramatically when scaled to the 10TB dataset. As the number of write nodes increases, the write throughput of AnalyticDB exhibits steady growth. In the TPC-H Benchmark, AnalyticDB completes 20 out of 22 queries in half the time required by the second-fastest database. However, for Query 2, AnalyticDB is slower than PrestoDB and Greenplum due to selecting a different join order.</p>
<h1 id="Paper-Strength"><a href="#Paper-Strength" class="headerlink" title="Paper Strength"></a>Paper Strength</h1><ul>
<li>3 challenges outlined in the paper precisely target the critical obstacles faced by OLAP systems in achieving real-time and efficient response. Addressing these challenges not only significantly enhances the database’s compatibility with diverse queries and complex data types, but also improves its responsiveness in the production environment, underscoring the high research value. The integration of read-write decoupling with version verification ensures that read and write operations remain isolated, while consistently providing read queries with the latest data. The design of the hybrid data layout and indexes incorporates support for complex data types such as JSON, full-text, and vector data, enabling a unified access interface for diverse data operations. This significantly broadens the applicability of AnalyticDB to a wider range of use cases. Furthermore, the execution engine, which combines sampling-based cardinality estimation with caching and optimized sampling algorithms, etc, achieves high estimation accuracy at low overhead and minimal latency. In conclusion, the advancements and optimizations introduced in AnalyticDB represent a major step forward in enhancing data versatility and real-time responsiveness in OLAP systems, the comprehensive solution will fully demonstrate its capabilities in high-concurrency e-commerce scenarios.</li>
<li>The paper provides a comprehensive literature review. The 2. Related Work section discusses the shortcomings of different databases. For example, expensive index updates in OLTP databases can degrade throughput and increase latency, and column-store in OLAP databases, such as TeradataDB and Greenplum, causes high random I&#x2F;O costs for point-lookup queries. Those issues above are effectively addressed in AnalyticDB. Furthermore, the paper outlines AnalyticDB’s improvements over Amazon Redshift and differences in query and aggregation compared to Google BigQuery.</li>
<li>The paper provides the detailed description of processes of functionalities in AnalyticDB, including:<ol>
<li>A thorough explanation of the read-write decoupling process from both reading and writing perspectives, accompanied by a flowchart specifically illustrating the more complex read operations.</li>
<li>Pseudocode and comments of key instructions for 3 algorithms involved in Query Execution.</li>
<li>A diagram outlining the merging process of baseline data and incremental data, broken down into 3 phases.</li>
</ol>
</li>
</ul>
<h1 id="Paper-Weakness"><a href="#Paper-Weakness" class="headerlink" title="Paper Weakness"></a>Paper Weakness</h1><ul>
<li>The description of some functionalities in the paper is not complete. In Section 3.4 Read&#x2F;Write Decoupling of the paper, only the real-time read is mentioned, while the bounded-staleness read is missed. For OLAP, reading outdated data is acceptable, and the bounded-staleness read allows read nodes to only access the latest data on write nodes after a certain delay following write operations, so it ensures fast responses in AnalyticDB to some extent. However, this type of read is not explained in the paper.</li>
<li>The paper lacks clarity in its description of certain newly introduced concepts. In Section 5.1.1, the discussion on predicate push-down provides only a brief overview of the STARs framework, without explaining how it applies relational algebra, how cost calculations are performed, and how the optimizer uses dynamic programming to encapsulate relational algebra operators. These omissions may leave readers confused.</li>
<li>The performance evaluation experiments in the paper are relatively simplistic. First, only 3 SQL statements were tested, focusing on a narrow range of query types, specific table partitioning strategies, specific tables and fields, and specific data types. The tests did not address complex data types such as JSON. Second, the experiments only tested datasets of 1TB and 10 TB. However, in production environments, daily new data can reach tens or even hundreds of petabytes. On peak promotional days like Double 11 or 618, the daily data processing amount can reach hundreds or even thousands of petabytes. For example, during the 2022 Double 11, Taobao and Tmall experienced a peak order payment rate of 583000 transactions per second. Thus, the datasets used in the experiments fall far short of reflecting the scale of real-world scenarios. Third, the tests based on the TPC-H Benchmark evaluated only 22 queries, which is insufficient to comprehensively reflect the true performance of these 5 databases. Therefore, it is recommended to include all data and queries involved in the production environment, such as MySQL binlogs, ElasticSearch index logs and Kafka logs, etc, and a stability test lasting one week or longer should be conducted on these 5 databases to provide a more realistic evaluation.</li>
</ul>
<p>Reference: <a target="_blank" rel="noopener" href="https://www.vldb.org/pvldb/vol12/p2059-zhan.pdf">https://www.vldb.org/pvldb/vol12/p2059-zhan.pdf</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/20/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/OLAP%20%E7%B4%A2%E5%BC%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/20/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/OLAP%20%E7%B4%A2%E5%BC%95/" class="post-title-link" itemprop="url">OLAP 索引</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-20 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-20T00:00:00-08:00">2025-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-06 21:47:43" itemprop="dateModified" datetime="2025-06-06T21:47:43-07:00">2025-06-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">高级数据存储</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Data-Skipping"><a href="#Data-Skipping" class="headerlink" title="Data Skipping"></a>Data Skipping</h1><p>我们可以利用辅助数据结构标识表中哪些部分可以跳过扫描，避免对每一行（tuple）都做判断。</p>
<p><strong>OLAP 场景下为什么要用 Data Skipping？</strong></p>
<p>在 OLAP 场景中，查询往往是扫描大量历史数据，筛选满足某种复杂条件的子集，典型例子如数据仓库的多维分析、报表查询等。此时：</p>
<ol>
<li><strong>表非常大</strong>：可能有数十亿、上百亿行，逐行检查每个行是否满足条件，代价非常高。</li>
<li><strong>查询通常按若干列做过滤</strong>：比如 <code>WHERE 年份 = 2024 AND 地区 = &#39;北京&#39;</code>，而且常常只要少量数据就够了（高选择性场景）。</li>
<li><strong>批量扫描 vs. 随机读取</strong>：读整个表，会触发大量 I&#x2F;O；如果能一眼就知道哪些数据页根本不用读，就能省下大量 I&#x2F;O 时间。</li>
</ol>
<p>因此，Data Skipping 的核心思路，就是通过在数据块或分区级别维护一些<strong>辅助信息</strong>，让查询在执行过滤时能够判断“某个数据块（比如一个文件区块、一个数据页、一个段&#x2F;partition）肯定不包含满足过滤条件的行，就直接跳过，不去读取和解析该块中的每一行”，从而大幅减少 I&#x2F;O，以及后续的行级计算。</p>
<p>常见的 Data Skipping 技术主要有以下几类：</p>
<ol>
<li><strong>Zone Map &#x2F; Min-Max 索引</strong><ul>
<li>每个数据块（block 或 segment）存一对最小值和最大值，例如按时间分块后，记录该块中时间列的 [min_timestamp, max_timestamp]。</li>
<li>查询时，如果查询条件为 <code>timestamp BETWEEN &#39;2024-01-01&#39; AND &#39;2024-03-01&#39;</code>，当某个块的最大时间都早于 ‘2024-01-01’，或者最小时间都晚于 ‘2024-03-01’，就可以完全跳过该块。</li>
<li>优点：实现简单、维护开销低；适用于列值分布相对平缓的场景。</li>
<li>缺点：如果一个块里 min–max 范围很宽（分布倾斜或块太大），就容易出现“区间重叠”，跳不过去，效率下降。</li>
</ul>
</li>
<li><strong>位图索引（Bitmap Index）</strong><ul>
<li>针对低基数的列（如性别、地区、状态等），为每个可能的离散取值维护一个位图（bitset）。</li>
<li>每个数据块也可以维护该位图的摘要（比如每 N 行分一个小位图），查询时快速根据位图判断该数据段中是否存在某个值，若不存在就跳过。</li>
<li>优点：在基数较低、数据倾斜度不高时，效果很好；位图操作位运算开销低。</li>
<li>缺点：对于高基数列，位图会很大，且维护成本高。</li>
</ul>
</li>
<li><strong>布隆过滤器（Bloom Filter）</strong><ul>
<li>保存每个数据页中的列值集合的 Bloom Filter，当查询时先检查 Bloom Filter 是否可能包含该值，若 Bloom Filter 结果为不可能存在，则跳过整个页。</li>
<li>优点：Bloom Filter 占用空间小，查询时布尔判断开销低。</li>
<li>缺点：存在误报率（false-positive），即有时候 Bloom Filter 误判可能存在，需要再回退到读页；但绝不会误判真正存在而跳过，保证正确性。</li>
</ul>
</li>
<li><strong>Skip List／Skip Table 结构</strong><ul>
<li>某些列可以对数据块按值范围做多级跳跃索引，比如最外层根据范围将整个表划分为若干区域，里层再分页索引。</li>
<li>OLAP 引擎（如 Apache Parquet、ORC）通常在文件格式层面（列存格式）会同时维护多层次的自底向上的统计信息（Statistics）。</li>
<li>Query Planner 就会根据统计信息决定哪些 Row Group（行组）或 Stride（步长）可以跳过。</li>
</ul>
</li>
<li><strong>分区表（Partitioning）</strong><ul>
<li>将大表水平拆分成多个分区（按时间、地区、业务线等），在查询时如果过滤条件里包含分区键（如 date），可以直接分区剪裁，跳过不相关分区。</li>
<li>其实分区本质也是一种粗粒度的 Data Skipping，跳过整个分区文件。</li>
</ul>
</li>
</ol>
<p><strong>设计 Data Skipping 时的关键考虑点</strong></p>
<p><strong>1. 谓词选择性</strong></p>
<ul>
<li><p><strong>低选择性场景</strong></p>
<p>比如查询 <code>WHERE gender = &#39;M&#39;</code>，如果全表大约一半行都是 <code>gender=&#39;M&#39;</code>，则满足谓词的行很多，跳过就算跳掉了另一半行，也意味着还要扫描一半行，节省有限。</p>
</li>
<li><p><strong>高选择性场景</strong></p>
<p>比如查询 <code>WHERE customer_id = 123456789</code>，或 <code>WHERE event_date = &#39;2025-06-01&#39; AND region = &#39;北京&#39;</code>，只会有极少量行符合条件。此时如果辅助结构能识别掉绝大多数数据块，就能极大节省 I&#x2F;O。</p>
</li>
</ul>
<p><strong>2. 列值分布</strong></p>
<ul>
<li>如果某列的值高度偏斜，例如某个列绝大多数行都是同一个值，其余值极少，Zone Map 或基于统计的跳过就不太准。<ul>
<li>举例：某列几乎全是 ‘A’，只有很少行是 ‘B’，若文件切分并不按照该列聚簇，那么即便大多数块里都是 ‘A’，但每块的最小值&#x2F;最大值同样会显示为 ‘A’，所有块看起来都可能包含 ‘B’，无法跳过。</li>
</ul>
</li>
<li>因此，一般会结合<strong>物理排序（clustering）或分桶（bucketing）</strong>：<ul>
<li>在生成文件时先按该列做排序&#x2F;分桶，这样相同值会被写到同一数据块里；</li>
<li>有了同一数据块里只有一个值的特性，块级统计就变得非常有效。</li>
</ul>
</li>
</ul>
<h1 id="Bitmap-Indexes"><a href="#Bitmap-Indexes" class="headerlink" title="Bitmap Indexes"></a>Bitmap Indexes</h1><p><strong>位图索引</strong>为某个属性（列）的每个不同取值，都存储一份单独的位图，位图中的第 <em>i</em> 位对应了表中第 <em>i</em> 条记录。</p>
<p>对于下表的原始数据：</p>
<table>
<thead>
<tr>
<th>order_id</th>
<th>paid</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>Y</td>
</tr>
<tr>
<td>2</td>
<td>Y</td>
</tr>
<tr>
<td>3</td>
<td>Y</td>
</tr>
<tr>
<td>4</td>
<td>N</td>
</tr>
</tbody></table>
<p>压缩后的数据如下：</p>
<table>
  <thead>
    <tr>
      <th>order_id</th>
      <th colspan="2">paid</th>
    </tr>
    <tr>
      <td></td>
      <td>Y</td>
      <td>N</td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>对于多列多值的组合过滤，Bitmap 索引支持高效的按位与（AND）、或（OR）、非（NOT）等位运算。</p>
<p>如果把整个表的行数记为 <em>N</em>，那么一个属性值对应的位图长度就是 <em>N</em> 位。当表非常大（数千万、上亿行），单张位图也会变得很大。为避免一次性分配庞大、连续的内存块，常见做法是分段（segment&#x2F;chunk）：把总行数 <em>N</em> 划分成若干个固定大小的块（例如每块 1 百万行），那么位图也相应地分成若干个子段，每个子段只需要维护该区间内的位。查询时只需要扫描与目标区间重叠的位段。</p>
<p>思考：</p>
<p>为什么使用 Bitmap？</p>
<ol>
<li><p><strong>空间利用率较高</strong></p>
<p>位图本质上是一个位数组，对于低基数或中等基数的列，位图能精确记录行与属性值的映射，且只需 1 位&#x2F;行。</p>
</li>
<li><p><strong>位运算速度快，适合多条件混合过滤</strong></p>
<p>多列过滤条件时，只要获取每个条件对应的位图，对它们做位与&#x2F;或&#x2F;非等操作，就能快速算出满足所有条件的行集合。</p>
</li>
</ol>
<p>为什么位图索引适合压缩？</p>
<ul>
<li>如果某个取值非常稀疏地分布在整个表中，那么位图中绝大多数位都是 0，只有少数位置是 1。或者，若有些连续行都满足该取值，又会出现连续的 1 序列。利用这种长串 0 或长串 1 可用<strong>压缩算法</strong>（如 Run-Length Encoding、WAH、Roaring Bitmap 等）进一步缩小存储空间。</li>
<li>现代 OLAP 引擎（例如 Apache Druid、ClickHouse、Presto 等）在底层都会选择一些成熟的压缩位图实现，这些实现不仅压缩率高，而且能够在压缩态下做位运算，避免了每次查询都需要先把整个位图解压到内存。</li>
</ul>
<h1 id="常见的四种编码技术"><a href="#常见的四种编码技术" class="headerlink" title="常见的四种编码技术"></a>常见的四种编码技术</h1><p>位图索引常见的四种编码技术：等值编码（Equality Encoding）、区间编码（Range Encoding）、分层编码（Hierarchical Encoding）和位切片编码（Bit-sliced Encoding）。</p>
<ol>
<li><p><strong>等值编码</strong></p>
<p>为某一列（属性）中的每个<strong>唯一取值</strong>都维护一份独立的位图。也就是说，如果该列有 m 个不同取值，则总共有 m 个位图，每份位图长度为表的总行数 N。这种编码形式上类似 One-Hot 独热编码。具体示例可参考之前的两张表。</p>
<p>优点：直观简单并且等值查询高效。</p>
<p>缺点：占用大量空间。</p>
</li>
<li><p><strong>区间编码</strong></p>
<p>与等值编码不同，不为每个<strong>单一取值</strong>都建一份位图，而是按照若干有意义的区间（interval）来划分，每个区间对应一份位图。</p>
<p>举例：以一个数值列 age 为例，表中 age 范围在 0–100 岁之间，可以把区间划分为 [0–17]、[18–30]、[31–50]、[51–70]、[71–100]。</p>
<p>为每个区间建一份位图：</p>
<ul>
<li>Bitmap_0_17：标记哪些行的 age 落在 0–17 之间；</li>
<li>Bitmap_18_30：标记哪些行的 age 落在 18–30 之间；</li>
<li>以此类推。</li>
</ul>
<p>优点：适合范围查询并且比等值编码更节省空间。</p>
<p>缺点：区间的粒度需要权衡。划分的区间越细，能更精准地匹配查询，但要维护的位图份数越多；划分区间越粗，则位图份数越少，但位图对应的行集合范围更大，查询时跳过的效率会下降。</p>
</li>
<li><p><strong>分层编码</strong></p>
<p>通过构建<strong>树形层次结构</strong>（hierarchy），在不同层级对值或值范围做<strong>分段统计&#x2F;位图标识</strong>，用于快速识别那些整个子树&#x2F;子范围内没有数据的情况。</p>
<p>通常从较粗的范围到较细的范围构建多层位图：</p>
<ol>
<li>第一层（根层）：记录整个列的全部可能值范围，并标注哪些子范围是非空（或者空）。</li>
<li>第二层：把根层范围继续细分为若干更小区间，分别维护各自的标识。</li>
<li>依次向下，直到最底层是单个取值或某个可接受的最小分区。</li>
</ol>
<p>以一列 zipcode（邮政编码）为例，取值范围是 00000–99999，总计 100,000 个可能值。如果直接对每个具体的 5 位数字都建一份位图，太耗空间。改用分层编码：</p>
<ol>
<li><strong>第一层</strong>：只看邮编的第一位，共 10 种可能（0、1、2、…、9），<ul>
<li>构建 10 个顶层位图，分别标记表中哪些行的 zipcode 第一位是 0、哪些行的第一位是 1，以此类推。</li>
</ul>
</li>
<li><strong>第二层</strong>：假设在第一层为前缀 &#x3D; 3（假设查询潜在目标值的前缀为 3）的情况下，第二层把 zipcode 的前两位 30 ～ 39 这 10 个前缀，各自再建位图；</li>
<li><strong>第三层</strong>：再细分为 300 ～ 309 等前缀，递归向下，直到精确到具体 5 位。</li>
</ol>
<p>优点：快速剪裁空范围且适用于高基数&#x2F;稀疏分布的列。</p>
<p>缺点：存储与维护复杂度更高。需要为每层维护对应的位图，并保持各层之间的一致性。</p>
</li>
<li><p><strong>位切片编码</strong></p>
<p>不是将每个取值或每个区间映射到一份位图，而是针对某个<strong>数值列的二进制表示</strong>，维护<strong>按位的位置拆分的一组位图</strong>。</p>
<p>假设有一个 8 位的整数列，如下：</p>
<table>
<thead>
<tr>
<th><strong>行号</strong></th>
<th>value（原始值）</th>
<th><strong>value（二进制）</strong></th>
<th><strong>Bit_0</strong></th>
<th><strong>Bit_1</strong></th>
<th><strong>Bit_2</strong></th>
<th><strong>Bit_3</strong></th>
<th><strong>Bit_4</strong></th>
<th><strong>Bit_5</strong></th>
<th><strong>Bit_6</strong></th>
<th><strong>Bit_7</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>5</td>
<td>00000101</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>18</td>
<td>00010010</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>7</td>
<td>00000111</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>25</td>
<td>00011001</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody></table>
<p>上表中，Bit_0 代表的是最低位，Bit_7 代表最高位。</p>
<p>位切片编码中，如果要基于上表中的数据进行如下条件过滤：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl <span class="keyword">WHERE</span> <span class="keyword">value</span> <span class="operator">&gt;</span> <span class="number">17</span>;</span><br></pre></td></tr></table></figure>

<p>我们先将 17 转换成二进制：10001，并在位切片编码过的位图索引中搜索并过滤：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">10100000</span><br><span class="line">01001000</span><br><span class="line">11100000</span><br><span class="line">10011000</span><br><span class="line">--------</span><br><span class="line">10001000 # 17</span><br></pre></td></tr></table></figure>

<p>我们先找出 17 的最高位是 Bit_4，并得出表中 4 行的这一位以及更高位中只有第 2 和第 4 行是和 17 至少一样大，因此可以快速过滤掉第 1 和第 3 行。</p>
<p>优点：</p>
<ol>
<li><strong>支持任意数值比较</strong>：当需要执行 WHERE value &gt; K 或 WHERE value BETWEEN L AND R 之类的数值比较时，只需根据 K、L、R 的二进制表现，结合多份 Bit_j 位图做<strong>布尔代数运算</strong>（AND、OR、NOT），就能快速构造出满足条件的行号位图。</li>
<li><strong>对高基数数值列尤为合适</strong>：与等值编码相比，高基数字段如果直接为每个取值建位图会产生海量位图；而位切片只需按二进制位宽度建 B 份位图，无论具体取值基数多大，都只需 B 份（例如 32 或 64）位图。</li>
</ol>
<p>缺点：</p>
<ol>
<li><strong>查询时位运算代价较大</strong>：与简单的等值位图（只要读取并返回一份位图）、区间位图（只要读取一份或几份位图）相比，位切片编码在做数值比较或范围查询时，需要对多份位图做组合运算（AND&#x2F;OR&#x2F;NOT），逻辑比较稍复杂，CPU 开销更高。但<strong>总体仍比全表扫描或传统 B+ 树索引的多次随机 I&#x2F;O 要高效得多</strong>，尤其在 OLAP 大规模并行计算场景下，位运算的吞吐量很大。</li>
</ol>
</li>
</ol>
<h1 id="BitWeaving"><a href="#BitWeaving" class="headerlink" title="BitWeaving"></a>BitWeaving</h1><p>BitWeaving 项目由威斯康星大学 Quickstep 团队提出和实现，旨在在主内存分析型 DBMS 中以“裸机”速度运行全表扫描操作  。</p>
<p>它利用处理器的位级并行性，将来自多个元组和列的位在单个时钟周期内同时处理，从而将每列的扫描周期降低到低于 1 个周期的级别 。</p>
<p>为支持复杂谓词评估，BitWeaving 提出了算术框架，将列值编码转换为适合位操作的格式，并生成结果位向量，用于后续布尔组合运算 。</p>
<p><strong>与传统列式存储的比较</strong></p>
<ul>
<li><strong>传统列式存储</strong>：将压缩后或原始列值按值连续存储，为了适配 SIMD 指令集，通常需要将每个值填充到固定边界（如 32 位、64 位），这会造成大量空间浪费，并增加对齐开销 。</li>
<li><strong>SIMD 扫描</strong>：将多个字典编码值打包到 SIMD 寄存器槽中，利用向量指令并行比较，但依然存在填充浪费和对齐处理两个瓶颈 。</li>
<li><strong>BitWeaving&#x2F;V</strong>：将每个列值的同一比特位聚集存储，实现完整利用字宽，且通过“早期剪枝”可在满足或不满足条件时跳过后续比特的处理，极大减少处理位数和内存带宽 。</li>
<li><strong>BitWeaving&#x2F;H</strong>：将编码后的整值按交错方式存储，并在每个值中预留一位结果标记，既可支持快速位级扫描，也可在需要完整重构列值时一次性提取，兼顾扫描和输出需求 。</li>
</ul>
<p>BitWeaving 包括两种主要存储布局：</p>
<ul>
<li><strong>BitWeaving&#x2F;Vertical</strong>：按位平面（bit-plane）垂直存储，每个比特位平面连续排列，天然支持按位剪枝；</li>
<li><strong>BitWeaving&#x2F;Horizontal</strong>：按元组水平交错存储，附加一位用于记录谓词结果，便于整值提取和多阶段谓词级联</li>
</ul>
<p>以下详细说明 Horizontal 和 Vertical Storage。</p>
<h2 id="Horizontal-storage"><a href="#Horizontal-storage" class="headerlink" title="Horizontal storage"></a>Horizontal storage</h2><p>把每条记录的全部 k 位 code 串成一块，按记录依次存放。</p>
<p>在内存中，一个 SIMD 寄存器会同时装入多条记录的全部位，对这些记录做并行比较。</p>
<p>我们以某个整数类型的字段为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Segment#1:</span><br><span class="line">t0: bits=[0, 0, 1] -&gt; 1</span><br><span class="line">t1: bits=[1, 0, 1] -&gt; 5</span><br><span class="line">t2: bits=[1, 1, 0] -&gt; 6</span><br><span class="line">t3: bits=[0, 0, 1] -&gt; 1</span><br><span class="line">t4: bits=[1, 1, 0] -&gt; 6</span><br><span class="line">t5: bits=[1, 0, 0] -&gt; 4</span><br><span class="line">t6: bits=[0, 0, 0] -&gt; 0</span><br><span class="line">t7: bits=[1, 1, 1] -&gt; 7</span><br><span class="line"></span><br><span class="line">Segment#2:</span><br><span class="line">t8: bits=[1, 0, 0] -&gt; 4</span><br><span class="line">t9: bits=[0, 1, 1] -&gt; 3</span><br></pre></td></tr></table></figure>

<p>以上内容中，t0 到 t9 为行号，bits 为该字段的二进制表示，箭头右边的是原始数据。全部的数据被分成了多个段。</p>
<p>假设一个处理器 word 只能容纳 8 个 bit 用于并行处理，那么以上代码块中的内容会被表示为如下结构：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_bitweaving_h.drawio.png" alt="img"></p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>Word</strong> 是处理器一次能够自然处理的固定长度数据单元，其长度称为<strong>字长</strong>（word length 或 word size），通常是处理器数据总线宽度的整数倍或分数。字长决定了CPU在单次操作中能读取、写入或传输的数据量，同时影响指令长度、寄存器宽度、寻址能力等关键特性。</p>
</blockquote>
<p>如果要处理如下 SQL 指令：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl <span class="keyword">WHERE</span> val <span class="operator">&lt;</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure>

<p>那么会进行如下 SIMD 计算（以 t0 和 t4 为例）：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_bitweaving_h_cal.drawio.png" alt="img"></p>
<p>最终，分割符为 1 的就是符合条件的数据。</p>
<p>但是之后我们需要如何汇总所有的过滤结果来确定要获取哪些数据呢？</p>
<p>我们可以通过 shift 操作输出所有的过滤结果到一个 bitmap 中，这个 bitmap 的每一位对应表中数据的行号。我们以 Segment#1 为例给出汇总过程：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_bitweaving_h_bitmap.drawio.png" alt="img"></p>
<p>上图中的位图也被称为 selection vector。</p>
<p>现在我们汇总得到了哪些数据符合条件以及哪些不符合，那要如何转成实际的下标或者是偏移量呢？</p>
<p>有两种常见的方法：</p>
<ol>
<li><p>迭代扫描</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Integer&gt; sel = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">  <span class="keyword">if</span> (selectionVector[i] == <span class="number">1</span>) &#123;</span><br><span class="line">    sel.add(i);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>优点</strong>：实现简单，不需要预处理。</p>
<p><strong>缺点</strong>：每次都要检查 N 位，分支预测失效会有开销；对于高选择率或低选择率都要遍历全阵列。</p>
</li>
<li><p>预计算位置表</p>
<p>把 selection vector（这里是 8 位）转换成十进制数据作为表的 key，而下标数组作为 payload。</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_bitweaving_h_pre_tbl.drawio.png" alt="img"></p>
<p><strong>优点</strong>：避免了每个位的分支判断，大大提高吞吐；字节级批量处理，利用了查表的连续读缓存。</p>
<p><strong>缺点</strong>：需要额外的 256 条查表开销，及存储这些小列表。</p>
</li>
</ol>
<h2 id="Vertical-storage"><a href="#Vertical-storage" class="headerlink" title="Vertical storage"></a>Vertical storage</h2><p>把所有记录在同一位位置 i 上的 bit 聚到一起，形成一条长长的位向量，然后依次存放各个位向量（从最低位到最高位）。</p>
<p>在内存中，一个 SIMD 寄存器会拿到同一位向量的一大段数据，一次性对所有记录的该位进行逻辑运算。</p>
<p>还是用之前的例子，数据如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Segment#1:</span><br><span class="line">t0: bits=[0, 0, 1] -&gt; 1</span><br><span class="line">t1: bits=[1, 0, 1] -&gt; 5</span><br><span class="line">t2: bits=[1, 1, 0] -&gt; 6</span><br><span class="line">t3: bits=[0, 0, 1] -&gt; 1</span><br><span class="line">t4: bits=[1, 1, 0] -&gt; 6</span><br><span class="line">t5: bits=[1, 0, 0] -&gt; 4</span><br><span class="line">t6: bits=[0, 0, 0] -&gt; 0</span><br><span class="line">t7: bits=[1, 1, 1] -&gt; 7</span><br><span class="line"></span><br><span class="line">Segment#2:</span><br><span class="line">t8: bits=[1, 0, 0] -&gt; 4</span><br><span class="line">t9: bits=[0, 1, 1] -&gt; 3</span><br></pre></td></tr></table></figure>

<p>在 Vertical Storage 中，以上数据会被表示为以下结构：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_bitweaving_v.drawio.png" alt="img"></p>
<p>也就是说，同一 segment 的 bits 数组中的同一下标的数据会被放到一个 word 中处理。</p>
<p>如果我们要处理如下命令：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl <span class="keyword">WHERE</span> val <span class="operator">=</span> <span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<p>会进行如下计算：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_bitweaving_v_cal.drawio.png" alt="img"></p>
<p>这里在处理完 col1 之后，我们观察到结果已经全是 0（即没有任何记录同时满足前两位匹配），就可以 <strong>跳过</strong> 第三轮比较（早剪枝），直接得出最终结果全 0。</p>
<p>优点：</p>
<ul>
<li>极高的查询性能：在 OLAP 场景下，Bit Weaving 能在压缩数据上直接进行位运算（如 AND、OR、XOR 等），极大提升聚合、过滤、排序等操作的速度。</li>
<li>空间效率高：采用位存储和压缩技术，极大节省磁盘和内存空间，同时也减少了 I&#x2F;O 成本。</li>
<li>良好的向量化和 SIMD 支持：在现代 CPU（如 AVX、SSE）下可充分利用 SIMD 指令集，实现高并发、高吞吐量的位级操作。</li>
</ul>
<p>缺点：</p>
<ul>
<li>对点查询不友好：相较于 B+ 树等结构，Bit Weaving 在单点查找或小范围查找时效率较低，主要优势在大范围数据扫描和聚合分析中。</li>
</ul>
<h1 id="近似位图索引"><a href="#近似位图索引" class="headerlink" title="近似位图索引"></a>近似位图索引</h1><p>传统的位图索引对每一个值都精确地维护一个 bitmap，而近似位图索引只保留粗略信息，牺牲部分准确性（即可能出现 false positives，假阳性），以便在常见查询场景下更快速地过滤大部分不匹配的元组。</p>
<p>当位图判断某条记录可能匹配时，还需要回到原始数据进行最后一次精确检查，从而消除 false positives。</p>
<p><strong>两种主流技术</strong></p>
<ul>
<li><p><strong>Column Imprints（列印迹）</strong></p>
<p>将列值按块（如每 64 或 128 个值）划分，对每个块维护一个小位图，位图的每一位对应一个值范围（bucket）。</p>
<p>查询时只需查块级位图，快速过滤掉整个块都不可能包含目标值的部分；再对剩余块逐条扫描。</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_approx_bitmap_col.drawio.png" alt="img"></p>
<p>或者更通用的方式是：</p>
<p>在 Column Imprint 中，每个 0 或 1 表示的是缓存行中的信息。具体来说，每一位表示某个直方图区间是否在对应的缓存行中有数据值：</p>
<ul>
<li><p>如果缓存行中的某些值落入某个区间，则该区间对应的位被设置为 1；</p>
</li>
<li><p>如果缓存行中的数据都不在该区间中，则该区间对应的位为 0。</p>
</li>
</ul>
<p>一个缓存行可以包含多个数据值（例如 64 字节的数据）。Column Imprint 并不记录每个数据值的精确位置，而是粗粒度地表示这些值的分布范围。</p>
<p>同样用之前的例子，假设缓存行中有三个值：1，8，4。</p>
<p>这些值会被映射到直方图的不同区间，例如：值 1 属于区间 [1, 2)；值 8 属于区间 [8, 9)；值 4 属于区间 [4, 5)。</p>
<p>最终的位向量可能是 10010001，其中：第 1 位表示缓存行中有值落入区间 [1, 2)；第 4 位表示缓存行中有值落入区间 [4, 5)；第 8 位表示缓存行中有值落入区间 [8, 9)。</p>
<p>如果之后要查询如 <code>val BETWEEN 4 AND 8</code> → 对应要检查的 bins 为 B₃…B₇ → mask &#x3D; 00011111：</p>
<ul>
<li>位向量 <code>AND mask</code> ≠ 0 → 说明该缓存行<strong>可能</strong>包含符合条件的值 → 需要回表做精确过滤；</li>
<li>位向量 <code>AND mask</code> &#x3D; 0 → 直接跳过整个缓存行。</li>
</ul>
</li>
<li><p><strong>Column Sketches（列摘要&#x2F;草图）</strong></p>
<p>用固定长度码（通常只有几位）替代完整位图。每个记录只存一个小码，表示它属于哪几个预定义区间。</p>
<p>整体流程如下：</p>
<p>首先根据列的值分布，在直方图中将整个值域划分成若干区间。之后为每个区间分配一个固定长度的二进制码，每条记录的 sketch，就是它所属区间的那个短码。</p>
<p>假设原数据为 [13, 191, 56, 92, 81, 140, 231, 172]，那么分布直方图和用于映射区间和 compression map 如下：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_approx_bitmap_skch_hist.drawio.png" alt="img"></p>
<p>map 中的对应关系代表：小于 60 的数据映射到 00 代码，小于 132 大于 60 的数据映射到 01 代码，以此类推。</p>
<p>因此原数据对应的 sketched column 为：[‘00’, ‘11’, ‘00’, ‘01’, ‘01’, ‘10’, ‘11’, ‘11’]。</p>
<p>此时，如果我们要执行如下命令：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> tbl <span class="keyword">WHERE</span> val <span class="operator">&lt;</span> <span class="number">90</span>;</span><br></pre></td></tr></table></figure>

<p>首先会确定要在区间 [0, 60] 和 (60, 132] 中找数据，接着对于代码为 00 的数据可直接确定为符合条件，对于代码为 01 的数据则要回表查看实际数据是否小于 90。</p>
<p>也就是，对于 [‘00’, ‘11’, ‘00’, ‘01’, ‘01’, ‘10’, ‘11’, ‘11’] 中的 01，我们要查看原数据，分别是 92 和 81，92 &gt; 90，因此排除掉 92，保留 81。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/13/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%B8%83%E5%B1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/13/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%B8%83%E5%B1%80/" class="post-title-link" itemprop="url">数据存储布局</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-01-13 00:00:00" itemprop="dateCreated datePublished" datetime="2025-01-13T00:00:00-08:00">2025-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-02 21:23:35" itemprop="dateModified" datetime="2025-06-02T21:23:35-07:00">2025-06-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">高级数据存储</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>数据存储格式决定了数据如何在物理介质上组织与编码，这直接影响了系统的读写性能与资源使用效率。在大数据环境下，不同的格式会带来显著的 I&#x2F;O 差异，从而影响查询响应时间和吞吐量  。此外，恰当的存储格式有助于提高压缩比，实现更高的数据密度，降低存储成本，并减少网络传输开销 。对于需要长期保留的数据，选择稳定且可持续的格式至关重要，否则会面临文件格式过时与不可读的风险。最后，不同应用场景（如 OLTP 与 OLAP）对读写模式有不同要求，合适的存储格式既能满足高并发事务访问，也能兼顾批量分析查询，实现系统的整体优化。</p>
<p>当前常用的数据存储格式有以下三种：</p>
<ol>
<li><strong>行式存储（Row-oriented Layout）</strong></li>
<li><strong>列式存储（Column-oriented Layout）</strong></li>
<li><strong>混合式存储（Hybrid Layout）</strong></li>
</ol>
<h1 id="行式存储"><a href="#行式存储" class="headerlink" title="行式存储"></a>行式存储</h1><p>行式存储将同一行的所有字段值连续存储在物理介质上，因而非常适合事务型（OLTP）操作，能够快速插入、更新和检索整行数据，同时在点查询时性能较优，因为一次查询往往需要访问同一行数据的多个字段值。</p>
<p>行式存储下，数据库通常需要小页面。因为：</p>
<ol>
<li>磁盘是按照页来读取数据的，无论实际需要一页中的多少数据（哪怕只需要一行数据），都会加载整个页面。这种情况下如果页面较大，数据库就会反复加载无关数据，从而浪费磁盘和内存带宽。</li>
<li>数据库在执行事务的时候会锁住索引中被操作的叶子节点（数据页）来保持一致性，因此如果页面越大，那么被锁住的无关数据也就会越多，从而加剧了锁竞争。</li>
</ol>
<p>比如，我们有一个表 <code>user</code>，内容如下：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Age</th>
<th>Address</th>
</tr>
</thead>
<tbody><tr>
<td>Sam</td>
<td>18</td>
<td>Los Angeles</td>
</tr>
<tr>
<td>John</td>
<td>16</td>
<td>London</td>
</tr>
<tr>
<td>Alice</td>
<td>16</td>
<td>New York</td>
</tr>
</tbody></table>
<p>那么行式存储格式如下：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_row_layout.drawio.png" alt="img"></p>
<p>因此在上图中，我们可以通过页号 + slot 号来确定一个页的位置。</p>
<p>思考：</p>
<ol>
<li><p>上图中为什么 Slots Array 和具体的数据要放在一个页的两端并且从两端向中间增加数据，而不是放在一起呢？</p>
<p>因为系统在插入时并不知道一页能容纳多少条可变长度的记录。把真实数据区从页面尾部往中间分配，就能在新增槽条目（在页面头部扩展）和写数据（在页面尾部收缩）之间形成一个松紧自适应的自由空间。如果数据紧跟在槽数组后面，那么插入时槽数组增长会挤占数据区，就必须把数据整体往后搬移，造成大量的内存（或磁盘页）移动开销。将数据放到页尾之后，每次插入都只在两端各做一小步，不会触及中间已存数据，从而极大降低了插入时的成本。</p>
<p>此外，系统只需要比较槽数组末尾指针和数据区末尾指针是否交错，即可判断该页是否还有足够余量插入新行。</p>
</li>
<li><p>频繁的指针访问（Pointer Access）在这里会引发什么问题？</p>
<ol>
<li><p>Cache Misses（缓存未命中）</p>
<p>指针场景下的实际数据往往在内存中彼此不连续放置，CPU 需要不断跳转到新的地址才能访问下一个数据，这种非顺序的内存访问会导致缓存行频繁未命中。</p>
</li>
<li><p>Memory Indirection（内存间接访问）</p>
<p>当程序想读取一个字段值时，通常先要读取槽数组里保存的偏移量（offset），然后去跳转到真正的物理地址。对于可变长度属性，元组头里还可能保存了再一层次的 pointer（比如 varchars 可能存放在页外溢出页），导致额外一跳。这会触发二次甚至多次的内存访问。</p>
<p>如果该偏移地址对应的缓存行当前不在 L1&#x2F;L2&#x2F;L3 缓存中，就必须从主存加载数据，导致高昂的内存访问延迟（几十到上百纳秒），远高于高速缓存访问延迟（几纳秒）。</p>
</li>
<li><p>Branch Prediction and Speculation Issues（分支预测与推测执行）</p>
<p>如果访问逻辑里要做大量的指针非空检查或可变长度判断，会引入很多条件分支。当 CPU 的分支预测器频繁猜错，就会导致流水线冲刷和重新预测，进一步影响性能。</p>
</li>
<li><p>TLB Misses（TLB 未命中）</p>
<p>大页、小页切换、可变长度数据放在页外时，程序要根据虚拟地址到物理地址多次查表。如果 TLB 不命中，CPU 就要走更慢的页表遍历，也会严重拖慢访问速度。</p>
</li>
</ol>
</li>
</ol>
<p><strong>优点</strong>：</p>
<ul>
<li><p>适合需要访问整条记录（整个元组）的查询。</p>
</li>
<li><p>适合插入、更新和删除操作（OLTP 工作负载）。</p>
</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li><p>不适合需要访问整列数据（整个列）的查询。</p>
<p>例如，在行式布局下，如果执行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT <span class="title function_">SUM</span><span class="params">(colA)</span>, AVG(colC)</span><br><span class="line">  FROM xxx</span><br><span class="line">  WHERE colA &gt; <span class="number">1000</span>;</span><br></pre></td></tr></table></figure>

<p>每一行会被遍历两遍，也就是说要为每条记录分别读取 colA 和 colC，重复读取同一条数据，无法做到顺序访问。</p>
</li>
<li><p>不适合大规模扫描和读取（OLAP 工作负载）。</p>
<p>因为数据是散落在每行里的，无法连续读取（非 sequential access），会产生大量指针跳转，开销很高。</p>
</li>
<li><p>不利于压缩节省。</p>
<p>不同列的数据往往混杂在一起，数据类型不一致，压缩效率会下降。</p>
</li>
</ul>
<h1 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h1><p>列式存储则将同一列的所有数值放在一起，便于对单列或少量列进行聚合及分析查询，尤其在数据仓库和在线分析处理（OLAP）场景下能够显著减少 I&#x2F;O 开销和提升查询效率。</p>
<p>该布局使得聚合操作变成了顺序读，因此 CPU 会进行预取操作，降低了缓存未命中的情况。并且列式存储布局更适合大页面，因为 OLAP 查询通常会一次性处理整个列的数据，大页面可以将更多的列数据存储在一个连续区域中，从而减少磁盘 I&#x2F;O 次数。同时，列式存储中的数据类型通常是同质化的（比如 Name 列都是 Char 类型的），这使得压缩算法在列式存储中的效果更好。</p>
<p><code>user</code> 表在列式存储下的结构为：</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_col_layout.drawio.png" alt="img"></p>
<p>在列式布局中，不再需要页号 + 槽号来唯一标识一条记录。相应地，我们只要给每列分配一个简单的偏移索引（即行号，从 0 开始到 N−1），就能唯一定位到该列对应行的值。</p>
<p>实际业务中往往会有可变长度字段（如 VARCHAR、TEXT、BLOB）。这时常见的做法是：往往会把这一列中的每一行真正的值存放在一个连续的数据区，而在该列文件中同时保留一个偏移量数组。偏移量数组里的第 i 项记录了第 i 行的数据在数据区里的起始位置和长度。</p>
<p>上图中，紧接在 Header 后面，会放一个 Null Bitmap，并且假设当前列有 M 行（行号从 0 到 M−1），则 Bitmap 通常是 M 位（二进制位），第 i 位为 1 表示第 i 行对应列值为 NULL，为 0 表示不为 NULL。</p>
<p>思考：</p>
<p>这里采用 Null Bitmap 的好处是什么？</p>
<ol>
<li>空值稀疏时，可以节省存储空间，不必给每个空值都分配实际存储字节。</li>
<li>在做向量化扫描时，可以直接跳过 NULL 行，提升 CPU 缓存命中率。</li>
</ol>
<blockquote>
<p>[!NOTE]</p>
<p>对于变长数据，我们还能怎么处理？</p>
<ol>
<li>在实际内容后面追加特殊字符，让每条记录占用的存储空间都达到一个统一的固定大小。但是当全表有成千上万行，其中大部分都比最大长度要短得多时，累积起来的无用填充就会非常庞大，导致磁盘空间和内存的浪费，还会降低 I&#x2F;O 和缓存利用率。</li>
<li>把本来可变字段都映射到某个定长标识上，那么存储时就无需再让每条记录都占用不同的字节数。比如有一个国家名称列，实际内容只有中国&#x2F;美国&#x2F;英国&#x2F;法国&#x2F;德国……这几十个明确的枚举值，那么我们可以先构建一个字典，给每个国家分配一个固定长度的编码（比如 2 字节、4 字节的整数）。在真实数据页里，只存整数编码（定长字段），而把对应的国家名称及其编码关系放在一个独立的字典里（通常在内存或元数据结构中）。如此一来，表里的国家列就变成了一个定长的整数列，检索时再通过字典查回实际名称。</li>
</ol>
</blockquote>
<p><strong>优点</strong>：</p>
<ul>
<li>适合需要访问整列数据的查询（例如聚合查询）。</li>
<li>适合大规模扫描与读取（OLAP 工作负载）。</li>
<li>能实现更紧凑的存储：与各种数据压缩技术天然契合，可显著减少磁盘与内存占用。</li>
<li>更好的局部性与缓存重用：单列数据连续存放，CPU 预取与缓存命中率更高，加快查询处理速度。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>不适合需要访问整条记录的查询：若一次要读回多列，就必须在多个列文件之间来回跳转并重组整个元组，开销较大。</li>
<li>不适合插入、更新和删除操作（OLTP 工作负载）：单条写入会涉及多个列文件的维护与重组，随机 I&#x2F;O 开销较高。</li>
</ul>
<h1 id="混合式存储"><a href="#混合式存储" class="headerlink" title="混合式存储"></a>混合式存储</h1><blockquote>
<p>理论上，纯列式布局对“只扫描一两列”非常高效；但实际 OLAP 查询往往不仅仅要过滤某列，还需要把过滤后的结果组装为“完整行”，甚至会用到其他列或多列联合过滤。</p>
<p>如果直接把所有列彻底拆开，各列之间又没有任何物理地域上的联系，就会让行重建的开销变得十分巨大。</p>
<p><strong>因此，需要一种折中布局：既要保证列连续以获得压缩和大规模顺序扫描的优势，又要让同一行的各列值彼此在磁盘&#x2F;内存中相对接近，好在需要时快速组装回完整元组。</strong></p>
</blockquote>
<p>混合式存储（PAX）融合了行式与列式两者的优点，通过在写入时同时维护行与列两种视图来兼顾高并发点查和批量分析，但会带来额外的存储开销与维护成本，需要根据业务场景权衡选择。</p>
<p>混合存储的核心思想是：水平分区 + 垂直分区。</p>
<p>首先，将整个表的所有行（Row #0、Row #1、…、Row #5）按照一定的行数分成多个行组（Row Group）。在每个行组内部，再进一步将行组里的各列分开存放。也就是说，组内的所有行先把同一列的值放一起，然后再放下一列的值。</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_hybrid_layout.drawio.png" alt="img"></p>
<p>因此混合存储通过这种方式，就在一个页或一个文件片段（segment）内部，既保留了局部行按顺序聚集的信息，也保留了同一列值连续存放的好处。</p>
<h1 id="水平分区"><a href="#水平分区" class="headerlink" title="水平分区"></a>水平分区</h1><p>将原本集中存储在单一服务器或单个存储介质上的数据，按照某种策略拆分成多份，分别放到不同的物理节点或不同的磁盘上。</p>
<p><strong>为什么要这样做？</strong></p>
<ol>
<li><strong>扩展性能</strong>：当数据量或并发请求量超过单台机器的承载能力时，把数据拆分到多台机器能够并行处理，提升吞吐量。</li>
<li><strong>扩展存储容量</strong>：单个磁盘或服务器空间有限，把数据分散到多台机器才能存下更多数据。</li>
<li><strong>可用性&#x2F;容错</strong>：如果某台机器或某个磁盘出现故障，只会影响部分分片的数据，整体系统仍可继续对其它分片提供服务（可结合副本机制进一步提高容灾）。</li>
</ol>
<p>两种分区模式：</p>
<ol>
<li><p>逻辑分区（Logically, Shared Storage）</p>
<p>多个分区虽然在逻辑上被看作是分散的，但底层共用同一个存储介质。换句话说，数据切分为多个逻辑分区，但这些分区的数据仍然落在同一套磁盘或存储系统上。</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_shared_storage.drawio.png" alt="img"></p>
<p>优点：部署相对简单，无需管理多台物理机器；数据仍然集中在一起，备份和维护方便。</p>
<p>缺点：底层物理存储是共享的 I&#x2F;O 总线，如果并发量很大，仍然会遇到单个存储后端的带宽瓶颈；并不能真正摆脱单点故障。</p>
</li>
<li><p>物理分区（Physically, Shared Nothing）</p>
<p>每个分区都完全独占自己的计算与存储资源，真正做到各自为政、不共享存储，也就是典型的 Shared‐Nothing 架构。</p>
<p><img src="/../../images/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/adv_db_shared_nothing.drawio.png" alt="img"></p>
<p>优点：可线性扩展，新增机器即可增加吞吐和存储；不同分区之间互不干扰，故障隔离更好，一个节点挂掉只影响该分区，其他节点仍可正常提供服务。</p>
<p>缺点：架构更复杂，需要维护多台机器，多副本同步、路由与协调也更困难；跨分区的事务和 JOIN 查询会额外复杂且性能成本更高。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/MapReduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/MapReduce/" class="post-title-link" itemprop="url">MapReduce</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-11-22 00:00:00" itemprop="dateCreated datePublished" datetime="2024-11-22T00:00:00-08:00">2024-11-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-18 00:31:28" itemprop="dateModified" datetime="2025-06-18T00:31:28-07:00">2025-06-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <style>.lang-content {width: 100%;overflow: hidden;}.lang-content:not(:first-child) {display: none;}</style><div style="text-align: right; margin: 0 0 20px auto; max-width: 200px;"><button id="langToggle" onclick="toggleLanguage()" class="lang-switch-btn" style="width: 100%;padding: 10px 20px;border-radius: 8px;border: 2px solid #2c3e50;background-color: #fff;cursor: pointer;color: #2c3e50;font-size: 15px;transition: all 0.3s ease;display: flex;align-items: center;justify-content: center;gap: 8px;box-shadow: 0 2px 4px rgba(0,0,0,0.1);"><span class="button-text">切换中文</span></button></div>

<div id="en-content" class="lang-content" style="display: block;"><h1 id="Programming-Model"><a href="#Programming-Model" class="headerlink" title="Programming Model"></a>Programming Model</h1><p>Input and Output:</p>
<ul>
<li>Takes a <strong>set of input key&#x2F;value pairs</strong>.</li>
<li>Produces a <strong>set of output key&#x2F;value pairs</strong>.</li>
</ul>
<p>User-Defined Functions:</p>
<ul>
<li><strong>Map Function</strong>:<ul>
<li>Written by the user.</li>
<li>Processes each input key&#x2F;value pair.</li>
<li>Generates a set of <strong>intermediate key&#x2F;value pairs</strong>.</li>
</ul>
</li>
<li><strong>Reduce Function</strong>:<ul>
<li>Also written by the user.</li>
<li>Takes an intermediate key <code>I</code> and a <strong>set of values</strong> associated with it.</li>
<li>Merges these values to produce a <strong>smaller set</strong> of output values, often zero or one value.</li>
</ul>
</li>
</ul>
<p>Intermediate Data Handling:</p>
<ul>
<li>The <strong>MapReduce library</strong> groups intermediate values by their key (<code>I</code>) and sends them to the Reduce function.</li>
<li>Intermediate values are supplied to the Reduce function using an <strong>iterator</strong>, enabling efficient handling of data sets that are too large to fit into memory.</li>
</ul>
<p>Fault Tolerance and Scalability:</p>
<ul>
<li>By breaking tasks into smaller, independent computations, MapReduce ensures scalability and fault tolerance, even in large distributed environments.</li>
</ul>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/mapred_exe_overview.png" alt="img"></p>
<p>数据分割与任务分配：</p>
<ul>
<li>输入数据划分：MapReduce 库将输入文件自动分割成 M 个片段（通常每个片段16MB到64MB，可由用户控制）。</li>
<li>启动程序实例：在集群中启动多个程序副本。</li>
<li>角色分配：其中一个程序实例被指定为主节点（master），其余的作为工作节点（workers）。</li>
</ul>
<p>任务调度：</p>
<ul>
<li>主节点的职责：主节点负责管理 M 个 map 任务和 R 个 reduce 任务。</li>
<li>任务分配：主节点将空闲的工作节点分配给 map 任务或 reduce 任务。</li>
</ul>
<p>Map 阶段：</p>
<ul>
<li>读取数据：被分配 map 任务的工作节点读取对应的输入片段。</li>
<li>处理数据：解析出键&#x2F;值对，并将其传递给用户定义的 Map 函数。</li>
<li>生成中间结果：<strong>Map 函数产生的中间键&#x2F;值对会存储在本地磁盘中。</strong></li>
</ul>
<p>中间数据处理：</p>
<ul>
<li>写入本地磁盘：**缓存的中间结果会定期写入本地磁盘，**<strong>并根据分区函数划分为 R 个区域。</strong></li>
<li>通知主节点：工作节点将这些中间数据的位置告知主节点，主节点负责将这些信息传递给 reduce 工作节点。</li>
</ul>
<p>Reduce 阶段准备：</p>
<ul>
<li>读取中间数据：reduce 工作节点收到主节点的通知后，通过远程过程调用（RPC）从 map 工作节点的本地磁盘读取中间数据。</li>
<li>排序数据：reduce 工作节点将所有中间数据按键排序，以确保相同的键聚集在一起。如果数据量过大，无法全部加载到内存，会采用外部排序。</li>
</ul>
<p>Reduce 阶段：</p>
<ul>
<li>执行 Reduce 函数：reduce 工作节点遍历排序后的中间数据，对于每个唯一的中间键，将键和对应的值列表传递给用户定义的 Reduce 函数。</li>
<li>生成最终输出：Reduce 函数的输出被追加到该 reduce 分区的最终输出文件中。</li>
</ul>
<p>任务完成与结果返回：</p>
<ul>
<li>任务监控：当所有的 map 和 reduce 任务都完成后，主节点会唤醒用户程序。</li>
<li>返回结果：此时，用户程序中的 MapReduce 调用返回，用户可以获取 R 个输出文件，每个 reduce 任务对应一个输出文件。</li>
</ul>
<p>额外说明：</p>
<ul>
<li>数据处理链：通常用户不需要将这 R 个输出文件合并成一个文件，因为这些文件可以直接作为下一个 MapReduce 调用的输入，或者被能够处理多文件输入的分布式应用程序使用。</li>
<li>流程图参考：上图👆用于展示 MapReduce 操作的整体流程，对应上面的步骤1到7。</li>
</ul>
<h2 id="Master-Data-Structure"><a href="#Master-Data-Structure" class="headerlink" title="Master Data Structure"></a>Master Data Structure</h2><p>Task State Tracking:</p>
<ul>
<li>For each <strong>map</strong> and <strong>reduce task</strong>, the master stores:<ul>
<li><strong>State</strong>:<ul>
<li><code>idle</code>: Task is yet to be assigned.</li>
<li><code>in-progress</code>: Task is currently being executed.</li>
<li><code>completed</code>: Task has finished execution.</li>
</ul>
</li>
<li><strong>Worker Identity</strong>: The worker machine handling the task (for non-idle tasks).</li>
</ul>
</li>
</ul>
<p>Intermediate Data Management:</p>
<ul>
<li>The master acts as the <strong>conduit</strong> for transferring intermediate data from map tasks to reduce tasks.</li>
<li>For each completed map task:<ul>
<li>It records the <strong>locations</strong> and <strong>sizes</strong> of the <code>R</code> intermediate file regions generated.</li>
<li>This data is crucial for reduce tasks to fetch intermediate results from the corresponding map workers.</li>
</ul>
</li>
</ul>
<p>Dynamic Updates:</p>
<ul>
<li>As map tasks finish, the master continuously updates its records of intermediate file locations and sizes.</li>
<li>These updates are incrementally pushed to reduce workers that are currently in-progress.</li>
</ul>
<h2 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h2><p>Worker Failure</p>
<ul>
<li><strong>Failure Detection</strong>:<ul>
<li>The <strong>master node</strong> periodically <strong>pings every worker</strong>.</li>
<li>If a worker does not respond within a certain timeframe, the master marks the worker as <strong>failed</strong>.</li>
</ul>
</li>
<li><strong>Task Rescheduling</strong>:<ul>
<li><strong>Map Tasks</strong>:<ul>
<li><strong>Completed Map Tasks</strong>:<ul>
<li>If the failed worker had completed a map task, its output becomes inaccessible (stored on the failed machine’s local disk).</li>
<li>These tasks are reset to their <strong>idle state</strong> and re-executed on other workers.</li>
</ul>
</li>
<li><strong>In-Progress Map Tasks</strong>:<ul>
<li>Similarly, in-progress tasks are marked as idle and reassigned to available workers.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Reduce Tasks</strong>:<ul>
<li><strong>Completed Reduce Tasks</strong>:<ul>
<li>These do not require re-execution, as their output is stored in a <strong>global file system</strong>, which remains accessible despite the failure.</li>
</ul>
</li>
<li><strong>In-Progress Reduce Tasks</strong>:<ul>
<li>如果某些 reduce 节点尚未读取中间数据，它们会从新的执行结果中读取数据。</li>
</ul>
</li>
</ul>
</li>
<li><strong>Data Coordination</strong>:<ul>
<li>When a map task is re-executed on a new worker:<ul>
<li><strong>Notification</strong>: All reduce workers are informed of the re-execution.</li>
<li><strong>Data Redirection</strong>: Reduce workers that have not yet fetched the intermediate data from the failed worker will instead fetch it from the new worker.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Master Failure</p>
<p>It is easy to make the master write periodic checkpoints of the master data structures described above. If the master task dies, a new copy can be started from the last checkpointed state. However, given that there is only a single master, its failure is unlikely; <strong>therefore our current implementation aborts the MapReduce computation if the master fails.</strong> Clients can check for this condition and retry the MapReduce operation if they desire.</p>
<h2 id="Locality-数据本地化优化"><a href="#Locality-数据本地化优化" class="headerlink" title="Locality 数据本地化优化"></a>Locality 数据本地化优化</h2><p>存储设计： 数据存储在 Google 文件系统 (GFS) 中。 GFS 将每个文件分割为 64 MB 的块，并在不同的机器上保存多个副本（通常是 3 个）。</p>
<p>任务调度优先级：</p>
<ul>
<li>优先本地化调度：The master node prioritizes assigning map tasks to workers that are on the same machine containing the replica of the data block.</li>
<li>次优调度：If local scheduling is not possible (e.g., the worker with the data block is busy), the master assigns the task to a machine near the replica, such as within the same rack or data center.</li>
</ul>
<p>实际效果： 在运行大型 MapReduce 操作时，大部分输入数据会从本地磁盘读取。 因为数据本地化，减少了跨网络传输的数据量，从而节省网络带宽。</p>
<h2 id="Task-Granularity"><a href="#Task-Granularity" class="headerlink" title="Task Granularity"></a>Task Granularity</h2><ol>
<li>Map 和 Reduce 阶段的划分<ol>
<li>任务数量 (M 和 R)： Map 阶段被划分为 M 个任务。 Reduce 阶段被划分为 R 个任务。</li>
<li>划分原则：理想情况下，M 和 R 的数量应该远大于工作节点的数量（即机器的数量）。</li>
</ol>
</li>
<li>多任务划分的好处<ol>
<li>动态负载均衡： 每个工作节点可执行多个任务，这样可以动态调整任务分配，避免某些节点过载或闲置。</li>
<li>故障恢复加速： 如果某个工作节点失败，已完成的多个任务可以分散到其他节点重新执行，恢复速度更快。</li>
</ol>
</li>
<li>任务划分的实际限制<ol>
<li>调度开销： 主节点需要进行 O(M + R) 次调度决策，且需要存储 O(M × R) 的状态信息。 虽然每对 map&#x2F;reduce 任务对仅占用约 1 字节内存，但过多任务会增加内存需求和调度复杂性。</li>
<li>输出文件限制： R 的大小往往受到用户需求限制，因为每个 reduce 任务会生成一个独立的输出文件。 输出文件过多会导致文件管理复杂。</li>
</ol>
</li>
<li>实际任务大小选择</li>
<li><ol>
<li>Map 阶段： 每个 map 任务通常处理 16 MB 到 64 MB 的输入数据。 这样的任务大小可以充分利用数据本地化优化（即尽量从本地磁盘读取数据）。</li>
<li>Reduce 阶段： R 通常是工作节点数量的几倍，以充分利用并行能力。 在一个典型的大规模 MapReduce 计算中： M &#x3D; 200,000（Map 阶段任务数）。 R &#x3D; 5,000（Reduce 阶段任务数）。 工作节点 &#x3D; 2,000（机器数量）。</li>
</ol>
</li>
</ol>
<h2 id="Backup-Tasks"><a href="#Backup-Tasks" class="headerlink" title="Backup Tasks"></a>Backup Tasks</h2><ol>
<li><p>什么是拖后腿的任务（Straggler Tasks）？</p>
<ul>
<li>定义：A straggler task refers to a task (map or reduce) in a MapReduce job that runs much slower than other tasks, delaying the overall completion of the job.</li>
</ul>
</li>
<li><p>解决方法：</p>
<ul>
<li><p>备份任务机制：当 MapReduce 计算接近完成时，主节点会为未完成的任务安排备份执行（Backup Executions）。同一任务的多个副本在不同的工作节点上同时运行。只要其中一个副本完成，任务即被标记为完成。</p>
</li>
<li><p>资源开销：调整后的机制只增加少量（通常是几个百分点）的计算资源使用。通过备份执行，能够显著缩短总执行时间。</p>
</li>
</ul>
</li>
</ol>
<h1 id="Refinement"><a href="#Refinement" class="headerlink" title="Refinement"></a>Refinement</h1><p><strong>Partitioning Function</strong></p>
<ol>
<li>Reduce 任务与分区<ol>
<li>用户通过设置 R 来指定需要的 reduce 任务数或输出文件数。</li>
<li>数据在这些 reduce 任务之间分区，分区方式取决于分区函数（partitioning function）。</li>
</ol>
</li>
<li>默认分区方式<ol>
<li>默认使用 哈希函数（hash function）：</li>
<li>分区规则：hash(key) mod R。</li>
<li>优势：通常能实现较为均衡的分区（即数据均匀分布到不同 reduce 任务中）。</li>
</ol>
</li>
<li>自定义分区方式<ol>
<li>有时默认的哈希分区不满足实际需求，需要根据特定逻辑对数据进行分区。例如：数据的键是 URL，用户希望所有来自同一主机（host）的条目存储在同一个输出文件中。</li>
<li>解决方案：用户可以定义自己的分区函数，例如：hash(Hostname(urlkey)) mod R：根据 URL 的主机名分区。这样，来自同一主机的所有条目会被分配到相同的 reduce 任务中。</li>
</ol>
</li>
</ol>
<p><strong>Ordering Guarantees</strong></p>
<ol>
<li>排序保证<ol>
<li>在 MapReduce 的 每个分区内，中间的键&#x2F;值对（key&#x2F;value pairs）会按照键的递增顺序进行处理。</li>
<li>目标：确保每个分区的输出文件是有序的。</li>
</ol>
</li>
<li>排序的作用<ol>
<li>生成有序输出文件：每个 reduce 任务生成的输出文件是按键排序的，直接支持有序数据的存储。</li>
<li>支持高效随机访问：有序数据便于快速查找，比如通过键值实现高效的随机访问。</li>
<li>用户便利：用户使用这些输出文件时，不需要额外排序，使用起来更方便。</li>
</ol>
</li>
</ol>
<p><strong>Combiner Function</strong></p>
<ol>
<li>问题背景<ol>
<li>在某些情况下，中间键重复率较高，每个 map 任务可能会生成大量重复的中间键记录。示例：在单词计数任务中（例如 〈the, 1〉），常见单词（如 “the”）会频繁出现。</li>
<li>结果：这些重复记录需要通过网络传输到同一个 reduce 任务，增加了网络负载。</li>
</ol>
</li>
<li>Combiner 函数的解决方案<ol>
<li>定义：Combiner 是一个可选的、局部的聚合函数，<strong>用于在 map 任务所在机器上对中间数据进行部分合并。</strong></li>
<li>工作原理：</li>
<li><ol>
<li>执行位置：Combiner 在 map 任务的机器上运行。</li>
<li>功能：对重复键的中间结果进行局部汇总，减少需要传输的数据量。</li>
<li>例如：将 〈the, 1〉、〈the, 1〉、〈the, 1〉 合并为 〈the, 3〉。</li>
</ol>
</li>
</ol>
</li>
<li>Combiner 和 Reduce 的区别<ol>
<li>相同点：通常，Combiner 的代码与 Reduce 函数的代码相同。都用于对数据进行聚合处理。</li>
<li>不同点：<ol>
<li>Combiner：输出的是中间结果，数据会继续传递给 Reduce 任务。</li>
<li>Reduce：输出的是最终结果，数据写入最终的输出文件。</li>
</ol>
</li>
</ol>
</li>
<li>优化效果<ol>
<li>减少网络传输量：通过提前合并数据，Combiner 减少了从 map 任务到 reduce 任务的数据量。例如，不传输 1000 条 〈the, 1〉，而是只传输 1 条 〈the, 1000〉。</li>
<li>提升性能：对于重复率高的任务，Combiner 能显著加快 MapReduce 操作的速度。</li>
</ol>
</li>
</ol>
<p><strong>Input and Output Types</strong></p>
<ol>
<li>输入数据格式的支持<ol>
<li>预定义格式：<ol>
<li>文本模式（text mode）： 每行数据被视为一个键&#x2F;值对。<ol>
<li>键：文件中该行的偏移量（offset）。</li>
<li>值：该行的内容。 排序键&#x2F;值对模式： 存储的键&#x2F;值对按键排序，便于按范围处理。</li>
</ol>
</li>
<li>自动分割范围： 每种输入格式都有分割机制，可将输入数据划分为适合 map 任务处理的范围。 例如，文本模式会确保分割发生在行边界，而不是行中间，保证数据的完整性。</li>
</ol>
</li>
<li>用户自定义格式： 用户可以通过实现简单的读取接口（reader interface），支持新的输入类型。<ol>
<li>非文件输入：数据可以来自其他来源，如数据库或内存中的数据结构，而不一定是文件。</li>
</ol>
</li>
</ol>
</li>
<li>输出数据格式的支持<ol>
<li>类似输入格式，MapReduce 也支持多种输出格式：<ol>
<li>预定义格式：提供了一些常用的输出格式。</li>
<li>自定义格式：用户可以通过实现新的接口定义输出数据格式。</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Skipping Bad Records</strong></p>
<ol>
<li>问题背景<ol>
<li>用户代码缺陷：Map 或 Reduce 函数中可能存在错误（如某些记录引发崩溃）。</li>
<li>确定性崩溃：对特定记录，每次处理都会发生崩溃。</li>
<li>问题影响：这类错误可能阻止整个 MapReduce 操作完成。</li>
<li>无法修复的情况：错误可能在第三方库中，用户无法访问源代码。</li>
</ol>
</li>
<li>MapReduce 提供的解决方案<ol>
<li>跳过问题记录：MapReduce 允许系统检测引发崩溃的记录，并跳过这些记录以继续操作。</li>
<li>实现机制：<ol>
<li>信号处理：每个工作节点安装信号处理器（signal handler），捕获段错误（segmentation violations）和总线错误（bus errors）。</li>
<li>记录错误序号：在调用用户的 Map 或 Reduce 函数之前，系统将参数的序列号存储在全局变量中。</li>
<li>发送错误报告：如果用户代码触发错误，信号处理器会发送一个“最后的喘息”UDP 数据包，包含引发错误的记录序号，通知主节点。</li>
<li>主节点决策：If a record causes failures many times, the master node instructs the record to be skipped the next time the task is retried.</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Local Execution</strong></p>
<ol>
<li>分布式调试的挑战<ol>
<li>复杂性：Map 和 Reduce 函数的实际计算是在分布式系统上完成，涉及数千台机器。主节点动态分配任务，调试难以直接定位问题。</li>
<li>常见问题：分布式环境下的日志、任务状态和数据流使得问题排查更加困难。</li>
</ol>
</li>
<li>本地执行模式的设计<ol>
<li>功能：MapReduce 提供了一种 本地执行的替代实现，在单台机器上顺序执行整个 MapReduce 操作。</li>
<li>特点：所有任务按顺序运行，无需分布式调度。用户可以限制计算范围，仅调试特定的 map 任务。</li>
</ol>
</li>
</ol>
<p><strong>Counter</strong></p>
<ul>
<li>Counters are used to track occurrences of specific events during a MapReduce operation, such as:<ul>
<li><strong>Custom events</strong> defined by the user (e.g., counting words, detecting specific patterns).</li>
<li><strong>System-defined metrics</strong>, like the number of input&#x2F;output key-value pairs processed.</li>
</ul>
</li>
</ul>
<p><strong>How Counters Work</strong></p>
<ul>
<li><strong>Propagation to the Master</strong>:<ul>
<li>Counter values from individual workers are sent to the <strong>master node</strong> via <strong>ping responses</strong>.</li>
</ul>
</li>
<li><strong>Aggregation</strong>:<ul>
<li>The master aggregates counter values from all completed tasks.</li>
<li>It ensures <strong>no double counting</strong> by ignoring duplicate task executions (e.g., due to re-executions or backup tasks).</li>
</ul>
</li>
</ul>
<p><strong>Monitoring and Reporting</strong></p>
<ul>
<li><strong>Real-Time Monitoring</strong>:<ul>
<li>The current counter values are displayed on the <strong>master status page</strong>, allowing users to observe the progress of the computation.</li>
</ul>
</li>
<li><strong>Final Reporting</strong>:<ul>
<li>When the MapReduce job finishes, the aggregated counter values are returned to the user program.</li>
</ul>
</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Assuming M&#x3D;10 and R&#x3D;20, What is the total number of files produced by the mappers?</strong></p>
<p>Total Files &#x3D; M×R &#x3D; 10×20 &#x3D; 200</p>
<p><strong>Why does MapReduce store the output of Reduce in the Google File System?</strong> </p>
<ul>
<li><p><strong>High Availability</strong>: GFS provides fault tolerance by replicating data across multiple machines. This ensures the output is not lost if a machine fails.</p>
</li>
<li><p><strong>Scalability</strong>: GFS is designed to handle large-scale data storage, making it suitable for the massive outputs of MapReduce jobs.</p>
</li>
</ul>
<p><strong>What is the purpose of a straggler?</strong> </p>
<ul>
<li><strong>“Straggler” refers to</strong> <strong>slow-running</strong> <strong>tasks</strong>, often map or reduce tasks, that significantly delay the completion of a MapReduce job.</li>
<li>Solution:</li>
<li><ul>
<li><strong>Backup Execution</strong>: The master node schedules backup executions of straggler tasks on other available workers.</li>
</ul>
</li>
</ul>
<p><strong>True or False: One may use SQL++ with a CSV data file and no schema.</strong></p>
<p>True: SQL++ can operate on semi-structured data, including CSV files, without requiring a predefined schema.</p>
<p><strong>With SQL++, what is the difference between pivot and unpivot?</strong></p>
<p>Pivot:</p>
<ul>
<li><p><strong>Purpose</strong>: Transforms rows into attributes (columns).</p>
</li>
<li><p><strong>Example</strong>:</p>
<ul>
<li><p>Input: <code>[ { &quot;symbol&quot;: &quot;amzn&quot;, &quot;price&quot;: 1900 }, { &quot;symbol&quot;: &quot;goog&quot;, &quot;price&quot;: 1120 }, { &quot;symbol&quot;: &quot;fb&quot;, &quot;price&quot;: 180 } ]</code></p>
</li>
<li><p>Query: <code>PIVOT sp.price AT sp.symbol FROM today_stock_prices sp;</code></p>
</li>
<li><p>Output: <code>{ &quot;amzn&quot;: 1900, &quot;goog&quot;: 1120, &quot;fb&quot;: 180 }</code></p>
</li>
</ul>
</li>
</ul>
<p>Unpivot:</p>
<ul>
<li><p><strong>Purpose</strong>: Transforms attributes (columns) into rows.</p>
</li>
<li><p><strong>Example</strong>:</p>
<ul>
<li><p>Input: <code>{ &quot;date&quot;: &quot;4/1/2019&quot;, &quot;amzn&quot;: 1900, &quot;goog&quot;: 1120, &quot;fb&quot;: 180 }</code></p>
</li>
<li><p>Query: <code>UNPIVOT c AS price AT sym FROM closing_prices c WHERE sym != &#39;date&#39;;</code></p>
</li>
<li><p>Output: <code>[ { &quot;date&quot;: &quot;4/1/2019&quot;, &quot;symbol&quot;: &quot;amzn&quot;, &quot;price&quot;: 1900 }, { &quot;date&quot;: &quot;4/1/2019&quot;, &quot;symbol&quot;: &quot;goog&quot;, &quot;price&quot;: 1120 }, { &quot;date&quot;: &quot;4/1/2019&quot;, &quot;symbol&quot;: &quot;fb&quot;, &quot;price&quot;: 180 } ]</code></p>
</li>
</ul>
</li>
</ul>
<p><strong>Using BG, one may summarize the performance of a data store using its SoAR. What is the input to BG to compute the SoAR of a data store?</strong></p>
<h3 id="1-SLA-Specifications"><a href="#1-SLA-Specifications" class="headerlink" title="1. SLA Specifications"></a><strong>1.</strong> <strong>SLA Specifications</strong></h3><p>The Service Level Agreement (SLA) defines the conditions under which SoAR is computed. The SLA includes:</p>
<ul>
<li><strong>α:</strong> Percentage of requests that must observe a response time less than or equal to β (e.g., 95%).</li>
<li><strong>β:</strong> Maximum acceptable response time (e.g., 100 ms).</li>
<li><strong>τ:</strong> Maximum allowable percentage of requests that observe unpredictable (stale or inconsistent) data (e.g., 0.01%).</li>
<li><strong>Δ:</strong> Duration for which the SLA must be satisfied (e.g., 10 minutes).</li>
</ul>
<h3 id="2-Database-Configuration"><a href="#2-Database-Configuration" class="headerlink" title="2. Database Configuration"></a><strong>2. Database Configuration</strong></h3><p>Details about the data store being tested:</p>
<ul>
<li><strong>Logical Schema:</strong> The data model used by the data store (e.g., relational schema, JSON-like schema for NoSQL).</li>
<li><strong>Physical Setup:</strong> Hardware configuration, including:<ul>
<li>Number of nodes.</li>
<li>Storage and memory resources.</li>
<li>Networking capabilities.</li>
</ul>
</li>
<li><strong>Population Size:</strong><ul>
<li><strong>M:</strong> Number of members in the database.</li>
<li><strong>ϕ:</strong> Number of friends per member.</li>
<li><strong>ρ:</strong> Number of resources per member.</li>
</ul>
</li>
</ul>
<h3 id="3-Workload-Parameters"><a href="#3-Workload-Parameters" class="headerlink" title="3. Workload Parameters"></a><strong>3. Workload Parameters</strong></h3><p>The workload specifies the nature and intensity of the actions BG will simulate:</p>
<ul>
<li><strong>Mix of</strong> <strong>Actions</strong><strong>:</strong><ul>
<li>Types of social networking actions (e.g., View Profile, List Friends, View Friend Requests).</li>
<li>Percentage of each action type (read-heavy, write-heavy, or mixed workloads).</li>
</ul>
</li>
<li><strong>Think Time (ϵ):</strong><ul>
<li>The delay between consecutive actions performed by a single thread.</li>
</ul>
</li>
<li><strong>Inter-Arrival Time (ψ):</strong><ul>
<li>The delay between new user sessions.</li>
</ul>
</li>
</ul>
<h3 id="4-Environmental-Parameters"><a href="#4-Environmental-Parameters" class="headerlink" title="4. Environmental Parameters"></a><strong>4. Environmental Parameters</strong></h3><p>Details about how BG generates and manages workloads:</p>
<ul>
<li><strong>Number of BGClients (N):</strong> Instances responsible for generating requests.</li>
<li><strong>Number of Threads (T):</strong> Concurrency level (threads per BGClient).</li>
<li><strong>D-Zipfian Distribution Parameters (θ):</strong><ul>
<li>Defines the access pattern (e.g., frequency of popular vs. less popular data).</li>
</ul>
</li>
</ul>
<p><strong>Consider the following binary representation of the priority of a key-value pair, 00101001. What is its CAMP rounding with precision 4?</strong></p>
<p>00101000</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/bg_bm_rounding.png" alt="img"></p>
<p><strong>What is a thundering herd and how does the IQ framework prevent it from causing the persistent data store to become the bottleneck?</strong></p>
<p>Thundering Herd Problem:</p>
<ul>
<li>When a key-value pair is not found in the KVS (a <strong>KVS miss</strong>), multiple read sessions might simultaneously query the RDBMS to fetch the value.</li>
<li>This can overload the RDBMS and degrade performance under high concurrency.</li>
</ul>
<p>Solution:</p>
<ul>
<li>When the first read session encounters a KVS miss, it requests an I lease for the key.</li>
<li>Once the I lease is granted, the KVS prevents other read sessions from querying the RDBMS for the same key.</li>
<li>All other read sessions must “back off” and wait for the value to be updated in the KVS by the session holding the I lease.</li>
</ul>
<blockquote>
<p>A thundering herd happens when a specific key undergoes heavy read and write activity.</p>
<ul>
<li>Writes invalidate the cache repeatedly  </li>
<li>All reads are forced to query the database</li>
</ul>
<p>I lease solves this problem</p>
<ul>
<li>The first read for the specific key is granted the I lease  </li>
<li>All other reads observe a miss and back-off  </li>
<li>The read with the I lease queries the RDBMS, computes the missing value, and populates the cache  </li>
<li>All other reads observe a cache hit</li>
</ul>
</blockquote>
<p>Reference: <a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf</a></p>
</div>

<div id="zh-content" class="lang-content" style="display: none;"><p>在海量数据环境下，高效、可靠地进行批处理分析是工程与科研常见的需求。MapReduce 以极简的 <strong>Map&#x2F;Reduce</strong> 双函数模型，把复杂的数据流水线拆解成可并行、可恢复的小任务，并通过数据本地化、任务复制等机制，在上千台机器上实现了高吞吐、强容错的离线计算。</p>
<h1 id="核心编程模型"><a href="#核心编程模型" class="headerlink" title="核心编程模型"></a>核心编程模型</h1><p>MapReduce 的核心只包含两类用户函数：</p>
<ul>
<li><strong>Map</strong> 接收单条输入键&#x2F;值对，输出若干 <em>中间</em> 键&#x2F;值对，可重复键值以支持分组。</li>
<li><strong>Reduce</strong> 按中间键聚集来自所有 Map 任务的值列表，并将其归并为更小的结果集，通常是一条或零条记录。</li>
</ul>
<p>框架负责把同键数据路由给同一个 Reduce，并通过迭代器流式传输，避免内存瓶颈。拆分-归并的模式天然支持可伸缩性：更多机器意味着更多同时执行的 Map&#x2F;Reduce 实例，吞吐随之提升。</p>
<h1 id="执行流程详解"><a href="#执行流程详解" class="headerlink" title="执行流程详解"></a>执行流程详解</h1><h3 id="1-输入切分与任务启动"><a href="#1-输入切分与任务启动" class="headerlink" title="1. 输入切分与任务启动"></a>1. 输入切分与任务启动</h3><p>MapReduce 库读取位于 GFS 上的原始文件，将其切分成 16 MB~64 MB 的片段；分片大小与 GFS 块一致，以便把计算尽量调度到本地持有副本的节点。随后，用户程序的多份副本在集群中启动，其中一份充当 <strong>Master</strong>，其余为 <strong>Worker</strong>。</p>
<h3 id="2-Map-阶段"><a href="#2-Map-阶段" class="headerlink" title="2. Map 阶段"></a>2. Map 阶段</h3><p>每个 Map Worker 读取自己的输入片段，执行用户 Map 逻辑，把结果暂存本地磁盘，并按 <strong>partition(key) &#x3D; hash(key) mod R</strong> 分成 R 个桶。分桶元数据同步给 Master，供后续 Reduce 拉取。</p>
<h3 id="3-Shuffle-与-Sort"><a href="#3-Shuffle-与-Sort" class="headerlink" title="3. Shuffle 与 Sort"></a>3. Shuffle 与 Sort</h3><p>Reduce Worker 通过 RPC 从各 Map 节点抓取属于自身分区的数据，先在内存、后在磁盘进行归并排序，保证同键记录连续。</p>
<h3 id="4-Reduce-阶段与输出"><a href="#4-Reduce-阶段与输出" class="headerlink" title="4. Reduce 阶段与输出"></a>4. Reduce 阶段与输出</h3><p>Reduce 依次遍历排序后的键分组，将每组〈key, value list〉喂给用户 Reduce 函数；输出最终写入 R 个结果文件，直接存放在 GFS 上以获得副本级高可用。</p>
<p>下图形象展示了上述个步骤：</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/mapred_exe_overview.png" alt="img"></p>
<h1 id="Master-与容错设计"><a href="#Master-与容错设计" class="headerlink" title="Master 与容错设计"></a>Master 与容错设计</h1><p>Master 跟踪每个 Map&#x2F;Reduce 任务的状态（idle, in-progress, completed）与所属 Worker ID。它定期 ping Worker；若超时则认为节点失效：</p>
<ul>
<li><strong>Map</strong> 已完成或进行中任务都被重置为 idle 并重新调度，因为中间文件仅存本地磁盘。</li>
<li><strong>Reduce</strong> 已完成任务不需重跑，输出已写 GFS；进行中任务可在新数据就绪后继续。</li>
<li>重新执行的 Map 会把新文件位置广播给所有 Reduce，确保尚未抓取的节点读取正确副本。</li>
</ul>
<p>尽管可以通过日志检查点重启 Master，Google 论文的实现简化为“Master 挂了就整体重跑”，实际部署中（Hadoop、Colossus）已演进到多 Master 设计。</p>
<h1 id="性能优化要诀"><a href="#性能优化要诀" class="headerlink" title="性能优化要诀"></a>性能优化要诀</h1><h3 id="数据本地化"><a href="#数据本地化" class="headerlink" title="数据本地化"></a>数据本地化</h3><p>集群带宽宝贵，优先把 Map 调度到存有数据块副本的机器；若不可用，则选同机架节点，显著减少网络流量。</p>
<h3 id="任务粒度"><a href="#任务粒度" class="headerlink" title="任务粒度"></a>任务粒度</h3><p>把 M、R 设成远大于机器数，可提升负载均衡与失效恢复速度；Google 大规模作业常用 M≈200 K、R≈5 K 对 2 K 台机器。</p>
<h3 id="Straggler-与备份任务"><a href="#Straggler-与备份任务" class="headerlink" title="Straggler 与备份任务"></a>Straggler 与备份任务</h3><p>极少数拖慢整体完成时间的 Straggler 会在作业末期触发 <strong>backup execution</strong>：Master 为未完成任务再起一到两个副本，谁先结束便认定成功，仅耗费几个百分点资源，即可大幅压缩尾部延迟。</p>
<h3 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h3><p>当中间键重度重复（如 WordCount）时，可在 Mapper 侧引入与 Reduce 同逻辑的 <strong>Combiner</strong>，先把〈the, 1〉… 合并为〈the, N〉，网络数据量骤减。</p>
<h3 id="计数器与监控"><a href="#计数器与监控" class="headerlink" title="计数器与监控"></a>计数器与监控</h3><p>框架自带 <strong>Counter</strong> 机制：Worker 在 ping 响应中上报局部计数，Master 聚合并去重后展示在 Web 状态页，便于实时掌握任务进度与异常。</p>
<h3 id="跳过坏记录"><a href="#跳过坏记录" class="headerlink" title="跳过坏记录"></a>跳过坏记录</h3><p>若某条输入反复触发第三方库 segfault，可启用 “skip bad records”：Worker 在信号处理器内记录崩溃序号；出现多次后 Master 指示下次直接跳过，保障作业整体完成。</p>
<h1 id="高级机制"><a href="#高级机制" class="headerlink" title="高级机制"></a>高级机制</h1><ul>
<li><strong>分区函数定制</strong>：默认哈希足以平衡负载，但用户可按 URL 主机等语义自定义，以确保相关数据汇聚同一 Reduce 逻辑。</li>
<li><strong>输入&#x2F;输出格式</strong>：Hadoop 的 TextInputFormat 保证切分发生在行边界，用户亦可自实现 Reader 以处理数据库等非文件来源。</li>
<li><strong>本地调试模式</strong>：为降低分布式排障成本，可在单机顺序跑完整作业，或仅重放指定 Map 分片，调试体验与普通程序无异。</li>
<li><strong>I-lease 与 Thundering Herd</strong>：在高并发缓存场景中，可给首个 KVS 未命中请求颁发 <strong>I-lease</strong>，其他请求退避，避免对后端数据库造成“惊群效应”。</li>
</ul>
<h1 id="典型问答回顾"><a href="#典型问答回顾" class="headerlink" title="典型问答回顾"></a>典型问答回顾</h1><table>
<thead>
<tr>
<th>问题</th>
<th>解答</th>
</tr>
</thead>
<tbody><tr>
<td><strong>M &#x3D; 10，R &#x3D; 20 时 Mapper 产生文件数？</strong></td>
<td>10 × 20 &#x3D; 200 个中间文件（每个 Map 对每个 Reduce 一份）。</td>
</tr>
<tr>
<td><strong>为何 Reduce 输出落盘到 GFS？</strong></td>
<td>GFS 通过三副本提供故障恢复且可横向扩容，适合作业级输出。</td>
</tr>
<tr>
<td><strong>Straggler 的含义与解决？</strong></td>
<td>极慢任务拖延作业尾期；Master 为其启动备份实例，先完成者胜。</td>
</tr>
<tr>
<td><strong>SQL++ 是否能直接查询无模式 CSV？</strong></td>
<td>可以；SQL++ 面向半结构化数据，天然支持 CSV 等自描述文件。</td>
</tr>
<tr>
<td><strong>Pivot &#x2F; Unpivot 差异？</strong></td>
<td>Pivot 把行转列；Unpivot 把列转行，常用于灵活透视表分析。</td>
</tr>
<tr>
<td><strong>BG 计算 SoAR 的必要输入？</strong></td>
<td>SLA 参数、数据库配置、工作负载与生成器并发度等四大类信息。</td>
</tr>
<tr>
<td><strong>CAMP 精度 4 取整示例 00101001→？</strong></td>
<td>00101000。<br><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/bg_bm_rounding.png" alt="img"></td>
</tr>
<tr>
<td><strong>Thundering Herd 如何被 I-lease 避免？</strong></td>
<td>首个 miss 获得 I-lease 访问 DB 并回填缓存，其余请求退避等待缓存命中。</td>
</tr>
</tbody></table>
<p>参考文献：<a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf</a></p>
</div>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN,en">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/15/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQL++/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/15/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQL++/" class="post-title-link" itemprop="url">SQL++</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-11-15 00:00:00" itemprop="dateCreated datePublished" datetime="2024-11-15T00:00:00-08:00">2024-11-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-31 21:01:48" itemprop="dateModified" datetime="2025-05-31T21:01:48-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Data-Model"><a href="#Data-Model" class="headerlink" title="Data Model"></a>Data Model</h1><p><strong>SQL++ has a more flexible data model:</strong></p>
<ul>
<li>It relaxes traditional SQL’s strict rules to handle modern, semi-structured data like JSON or CBOR.</li>
<li>SQL++ databases can store self-describing data, meaning you don’t need a predefined schema (data structure).</li>
</ul>
<p><strong>Supports diverse data types:</strong></p>
<ul>
<li>Data can be single values (scalars), tuples (a set of key-value pairs), collections (like arrays or multisets), or combinations of these.</li>
<li>Unlike traditional SQL, tuples in SQL++ are <strong>unordered</strong>, which means the order of attributes doesn’t matter.</li>
</ul>
<p><strong>Allows duplicate attribute names but discourages them:</strong></p>
<ul>
<li>This is to accommodate non-strict formats like JSON.</li>
<li>However, duplicate names can lead to unpredictable query results, so they’re not recommended.</li>
</ul>
<p><strong>Two kinds of missing values:</strong> <strong><code>NULL</code> and <code>MISSING</code></strong>:</p>
<ul>
<li><strong><code>NULL</code></strong>: Means an attribute exists but has no value.</li>
<li><strong><code>MISSING</code></strong>: Means the attribute doesn’t exist at all.</li>
<li>This distinction is useful for clearer query results and error handling.</li>
</ul>
<p><strong>Importance of</strong> <strong><code>MISSING</code></strong>:</p>
<ul>
<li>SQL++ doesn’t stop processing if some data is missing; instead, it marks those cases as <code>MISSING</code> and continues.</li>
<li>This makes queries more robust and tolerant of data inconsistencies.</li>
</ul>
<h1 id="Accessing-Nested-Data"><a href="#Accessing-Nested-Data" class="headerlink" title="Accessing Nested Data"></a>Accessing Nested Data</h1><p><strong>SQL-92 vs. Modern Data:</strong></p>
<ul>
<li>SQL-92 only supports tables with rows (tuples) containing simple values (scalars).</li>
<li>Modern data formats often include <strong>nested structures</strong>, where attributes can hold complex data types like arrays, tables, or even arrays of arrays.</li>
</ul>
<p><strong>Nested Data Example:</strong></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code1.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code2.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code3.png" alt="img"></p>
<ul>
<li>In the example, the <code>projects</code> attribute of an employee is an <strong>array of tuples</strong>, representing multiple projects each employee is involved in.</li>
</ul>
<p><strong>Querying Nested Data in SQL++:</strong></p>
<ul>
<li>SQL++ can handle such nested data without adding new syntax to SQL.</li>
<li>For example, a query can find employees working on projects with “security” in their names and output both the employee’s name and the project’s name.</li>
</ul>
<p><strong>How It Works:</strong></p>
<ul>
<li>SQL++ uses <strong>left-correlation</strong>, allowing expressions in the <code>FROM</code> clause to refer to variables declared earlier in the same clause.</li>
<li>For instance, <code>e.projects</code> accesses the projects of an employee <code>e</code>.</li>
<li>This relaxes SQL’s restrictions and effectively enables a join between an employee and their projects.</li>
</ul>
<p><strong>Using Variables in Queries:</strong></p>
<ul>
<li>SQL++ requires <strong>explicit</strong> use of variables (e.g., <code>e.name</code> instead of just <code>name</code>) because schema is optional and cannot guarantee automatic disambiguation.</li>
<li>If a schema exists, SQL++ can still optimize by rewriting the query for clarity and execution.</li>
</ul>
<p><strong>Flexibility with Nested Collections:</strong></p>
<ul>
<li>Variables in SQL++ can represent any type of data—whether it’s a table, array, or scalar.</li>
<li>These variables can be used seamlessly in <code>FROM</code>, <code>WHERE</code>, and <code>SELECT</code> clauses.</li>
</ul>
<p><strong>Aliases Can Bind to Any Data Type:</strong></p>
<ul>
<li>In SQL++, variables (aliases) don’t have to refer only to tuples.</li>
<li>They can bind to <strong>arrays of scalars</strong>, <strong>arrays of arrays</strong>, or any combination of scalars, tuples, and arrays.</li>
</ul>
<p><strong>Flexibility in Querying Nested Data:</strong></p>
<ul>
<li>Users don’t need to learn new query syntax for different data structures.</li>
<li>The same <strong>unnesting feature</strong> is used regardless of whether the data is an array of tuples or an array of scalars.</li>
</ul>
<p>Example:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code4.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code5.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code6.png" alt="img"></p>
<ul>
<li>If the <code>projects</code> attribute is an <strong>array of strings</strong> (instead of tuples), SQL++ queries can still process it.</li>
<li>The query would range over <code>e.projects</code> and bind <code>p</code> to each project name (a string).</li>
</ul>
<p><strong>Relaxed Semantics Compared to SQL:</strong></p>
<ul>
<li>In traditional SQL, the <code>FROM</code> clause binds variables strictly to tuples.</li>
<li>SQL++ generalizes this by treating the <code>FROM</code> clause as a function that can bind variables to <strong>any type of data</strong>—not just tuples.</li>
</ul>
<p><strong>Practical Outcome:</strong></p>
<ul>
<li>In the example, the <code>FROM</code> clause produced variable bindings like <code>&#123;e: employee_data, p: project_name&#125;</code>.</li>
<li>This allows the query to handle data structures that SQL would not support without extensions.</li>
</ul>
<h1 id="ABSENCE-OF-SCHEMA-AND-SEMI-STRUCTURED-DATA"><a href="#ABSENCE-OF-SCHEMA-AND-SEMI-STRUCTURED-DATA" class="headerlink" title="ABSENCE OF SCHEMA AND SEMI-STRUCTURED DATA"></a>ABSENCE OF SCHEMA AND SEMI-STRUCTURED DATA</h1><p>Schemaless Data:</p>
<ul>
<li>Many modern data formats (e.g., JSON) don’t require a predefined schema to describe their structure.</li>
<li>This allows for <strong>flexible and diverse data</strong>, but it also introduces <strong>heterogeneity</strong>.</li>
</ul>
<p>Types of Heterogeneity:</p>
<ul>
<li><strong>Attribute presence</strong>: Some tuples may have a specific attribute (e.g., <code>x</code>), while others may not.</li>
<li><strong>Attribute type</strong>: The same attribute can have different types across tuples. For example:<ul>
<li>In one tuple, <code>x</code> might be a string.</li>
<li>In another tuple, <code>x</code> might be an array.</li>
</ul>
</li>
<li><strong>Element types in collections</strong>: A collection (e.g., an array or a bag) can have elements of different types. For example:<ul>
<li>The first element could be a string, the second an integer, and the third an array.</li>
</ul>
</li>
<li><strong>Legacy or data evolution</strong>: These heterogeneities often result from evolving requirements or data conversions (e.g., converting XML to JSON).</li>
</ul>
<p>Heterogeneity Is Not Limited to Schemaless Data:</p>
<ul>
<li>Even structured databases can have heterogeneity. For example:<ul>
<li>Hive’s <strong>union type</strong> allows an attribute to hold multiple types, like a string or an array of strings.</li>
</ul>
</li>
</ul>
<p>How SQL++ Handles It:</p>
<ul>
<li>SQL++ is designed to work seamlessly with <strong>heterogeneous data</strong>, whether the data comes from a schemaless format or a schema-based system.</li>
<li>It offers features and mechanisms to process such data flexibly, without enforcing rigid structure requirements.</li>
</ul>
<h2 id="Missing-Attributes"><a href="#Missing-Attributes" class="headerlink" title="Missing Attributes"></a>Missing Attributes</h2><ol>
<li><p><strong>Representation of Missing Information</strong>:</p>
<ul>
<li><p>In SQL, a missing value is typically represented as <code>NULL</code> (e.g., Bob Smith’s title in the first example).</p>
</li>
<li><p>In SQL++, there’s an additional option: simply omitting the attribute altogether (as seen in the second example for Bob Smith).</p>
</li>
</ul>
</li>
<li><p><code>NULL</code> <strong>vs.</strong> <code>MISSING</code>:</p>
<ul>
<li><p><code>NULL</code>: Indicates the attribute exists but has no value.</p>
</li>
<li><p><code>MISSING</code>: Indicates the attribute is entirely absent.</p>
</li>
<li><p>SQL++ supports distinguishing between these two cases, unlike traditional SQL.</p>
</li>
</ul>
</li>
<li><p><strong>Why This Matters</strong>:</p>
<ul>
<li><p>Some data systems or formats (e.g., JSON) naturally omit <strong>missing</strong> attributes rather than assigning a <code>NULL</code> value.</p>
</li>
<li><p>SQL++ makes it easy to work with both approaches by allowing queries to handle <code>NULL</code> and <code>MISSING</code> values distinctly.</p>
</li>
</ul>
</li>
<li><p><strong>Query Behavior</strong>:</p>
<ul>
<li><p>Queries in SQL++ can propagate <code>NULL</code> and <code>MISSING</code> values as they are.</p>
</li>
<li><p>The system introduces the special value <code>MISSING</code> to represent absent attributes, allowing clear differentiation from <code>NULL</code>.</p>
</li>
</ul>
</li>
</ol>
<h2 id="MISSING-as-a-Value"><a href="#MISSING-as-a-Value" class="headerlink" title="MISSING as a Value"></a>MISSING as a Value</h2><p>What Happens When Data is Missing:</p>
<ul>
<li>If a query references an attribute that doesn’t exist in a tuple (e.g., <code>e.title</code> for Bob Smith), SQL++ assigns the value <code>MISSING</code>.</li>
<li>This avoids query failures and ensures processing can continue.</li>
</ul>
<p><strong>Three Cases Where</strong> <code>MISSING</code> <strong>is Produced</strong>:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code7.png" alt="img"></p>
<ul>
<li><strong>Case 1</strong>: Accessing a missing attribute. For example, <code>&#123;id: 3, name: &#39;Bob Smith&#39;&#125;.title</code> results in <code>MISSING</code>.</li>
<li><strong>Case 2</strong>: Using invalid input types for functions or operators (e.g., <code>2 * &#39;some string&#39;</code>).</li>
<li><strong>Case 3</strong>: When <code>MISSING</code> is an input to a function or operator, it propagates as <code>MISSING</code> in the output.</li>
</ul>
<p>SQL Compatibility Mode:</p>
<ul>
<li>In SQL compatibility mode, <code>MISSING</code> behaves like <code>NULL</code> for compatibility. For instance, <code>COALESCE(MISSING, 2)</code> will return <code>2</code>, just as <code>COALESCE(NULL, 2)</code> does in SQL.</li>
</ul>
<p><strong>Propagation of</strong> <code>MISSING</code> <strong>in Queries</strong>:</p>
<ul>
<li>In queries, <code>MISSING</code> values flow naturally through transformations, enabling consistent handling of absent data.</li>
<li>For example, in a <code>CASE</code> statement, if <code>e.title</code> evaluates to <code>MISSING</code>, the result of the entire <code>CASE</code> expression will also be <code>MISSING</code>.</li>
</ul>
<p><strong>Results with</strong> <code>MISSING</code>:</p>
<ul>
<li>If a query result includes <code>MISSING</code>, SQL++ will omit the attribute from the result tuple.</li>
<li>In communication with external systems like JDBC&#x2F;ODBC, <code>MISSING</code> is transmitted as <code>NULL</code> to ensure compatibility.</li>
</ul>
<h1 id="RESULT-CONSTRUCTION-NESTING-AND-GROUPING"><a href="#RESULT-CONSTRUCTION-NESTING-AND-GROUPING" class="headerlink" title="RESULT CONSTRUCTION,NESTING, AND GROUPING"></a>RESULT CONSTRUCTION,NESTING, AND GROUPING</h1><h2 id="Creating-Collections-of-Any-Value"><a href="#Creating-Collections-of-Any-Value" class="headerlink" title="Creating Collections of Any Value"></a>Creating Collections of Any Value</h2><p><strong>Power of</strong> <code>SELECT VALUE</code>:</p>
<ul>
<li>The <code>SELECT VALUE</code> clause in SQL++ allows constructing collections of any type of data, not just tuples.</li>
<li>It enables creating outputs that match the structure of nested data without flattening it unnecessarily.</li>
</ul>
<p>Example Query:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code8.png" alt="img"></p>
<ul>
<li>The query in Listing 10 demonstrates how to use <code>SELECT VALUE</code> to extract only the “security” projects of employees, resulting in a nested structure.</li>
<li>Each employee’s tuple includes their ID, name, title, and a collection of their security-related projects.</li>
</ul>
<p>Result:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code9.png" alt="img"></p>
<ul>
<li>Listing 11 shows the result where each employee has a field <code>security_proj</code> containing a nested collection of projects that match the condition (e.g., projects with “Security” in the name).</li>
</ul>
<p>Key Difference from Standard SQL:</p>
<ul>
<li>SQL’s <code>SELECT</code> clause can be viewed as shorthand for <code>SELECT VALUE</code>, but with differences:<ul>
<li>SQL automatically coerces subquery results into scalar values, collections of scalars, or tuples based on context.</li>
<li>In contrast, <code>SELECT VALUE</code> in SQL++ consistently produces a collection and does not apply implicit coercion.</li>
</ul>
</li>
</ul>
<p>Flexibility:</p>
<ul>
<li>SQL++ avoids implicit “magic” by explicitly treating <code>SELECT</code> as shorthand for <code>SELECT VALUE</code>.</li>
<li>This approach aligns more closely with functional programming principles, making it easier to handle and compose nested data results.</li>
</ul>
<h2 id="GROUP-BY-and-GROUP-AS"><a href="#GROUP-BY-and-GROUP-AS" class="headerlink" title="GROUP BY and GROUP AS"></a>GROUP BY and GROUP AS</h2><p><strong>Introduction to</strong> <code>GROUP BY ... GROUP AS</code>:</p>
<ul>
<li>This feature extends SQL’s <code>GROUP BY</code> functionality, allowing groups (and their contents) to be directly accessible in the <code>SELECT</code> and <code>HAVING</code> clauses.</li>
<li>It is more efficient and intuitive for creating nested results compared to traditional SQL, especially when the output nesting doesn’t directly align with the input data structure.</li>
</ul>
<p>How It Works:</p>
<ul>
<li><strong>Generalization</strong>: Unlike SQL, which limits access to grouped data in <code>GROUP BY</code>, SQL++ allows accessing the full group details as part of the query.</li>
<li><strong>Pipeline Model</strong>: SQL++ processes queries in a step-by-step fashion, starting with <code>FROM</code>, followed by optional clauses like <code>WHERE</code>, <code>GROUP BY</code>, <code>HAVING</code>, and ending with <code>SELECT</code>.</li>
</ul>
<p>Example:</p>
<ul>
<li>In the query from <strong>Listing 12</strong>, employees are grouped by their project names (converted to lowercase), and a nested list of employees for each project is created.</li>
<li>The <code>GROUP BY LOWER(p) AS p GROUP AS g</code> clause groups data and stores each group in <code>g</code>.</li>
<li>The <code>SELECT</code> clause then extracts project names and employees.</li>
</ul>
<p>Result:</p>
<ul>
<li>The output (shown in <strong>Listing 13</strong>) contains nested objects:</li>
<li><ul>
<li>Each object has a <code>proj_name</code> (e.g., <code>&#39;OLTP Security&#39;</code>) and an <code>employees</code> field listing the names of employees associated with that project.</li>
</ul>
</li>
</ul>
<p><strong>Details of</strong> <code>GROUP BY ... GROUP AS</code>:</p>
<ul>
<li>The clause produces bindings like the ones in <strong>Listing 14</strong>, where each group (<code>g</code>) includes all the data for its corresponding key (<code>p</code>).</li>
<li>The result allows users to flexibly access and format the grouped data.</li>
</ul>
<p>SQL++ Flexibility:</p>
<ul>
<li>SQL++ allows placing the <code>SELECT</code> clause either at the start or the end of a query block, enhancing readability and flexibility.</li>
<li>This approach is more consistent with functional programming and reduces constraints found in traditional SQL.</li>
</ul>
<p>Advanced Features:</p>
<ul>
<li>SQL++ supports additional analytical tools like <code>CUBE</code>, <code>ROLLUP</code>, and <code>GROUPING SETS</code>, making it highly compatible with SQL but better suited for nested and semi-structured data.</li>
</ul>
<h2 id="Aggregate-Functions"><a href="#Aggregate-Functions" class="headerlink" title="Aggregate Functions"></a>Aggregate Functions</h2><p>Limitations of Traditional SQL Aggregate Functions:</p>
<ul>
<li>Aggregate functions like <code>AVG</code> and <code>MAX</code> in traditional SQL lack <strong>composability</strong>.</li>
<li>They work directly on table columns but don’t easily integrate with more complex expressions or subqueries.</li>
</ul>
<p>SQL++ Solution:</p>
<ul>
<li>SQL++ introduces <strong>composable aggregate functions</strong>, such as <code>COLL_AVG</code> (for calculating the average of a collection) and <code>COLL_MAX</code>.</li>
<li>These functions take a <strong>collection</strong> as input and return the aggregated value.</li>
</ul>
<p>Importance of Composability:</p>
<ul>
<li>In SQL++, data is conceptually <strong>materialized</strong> into a collection first, then passed to the composable aggregate function.</li>
<li>While this materialization is conceptual, SQL++ engines optimize the execution (e.g., using pipelined aggregation).</li>
</ul>
<p>Example 1: Calculating the Average Salary of Engineers:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code10.png" alt="img"></p>
<ul>
<li><strong>SQL Query</strong> (Listing 15): Uses <code>AVG(e.salary)</code> directly.</li>
<li><strong>SQL++ Core Query</strong> (Listing 16): Converts <code>e.salary</code> into a collection and applies the <code>COLL_AVG</code> function.</li>
<li>SQL++ clearly defines the flow of data, making it more intuitive and flexible.</li>
</ul>
<p>Example 2: Calculating the Average Salary of Engineers by Department:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code11.png" alt="img"></p>
<ul>
<li><strong>SQL Query</strong> (Listing 17): Uses <code>GROUP BY</code> and <code>AVG</code>.</li>
<li><strong>SQL++ Core Query</strong> (Listing 18):<ul>
<li>Uses <code>GROUP BY ... GROUP AS</code> to form groups.</li>
<li>Feeds each group into <code>COLL_AVG</code> to calculate the average salary.</li>
<li>Constructs the result using the <code>SELECT VALUE</code> clause, explicitly specifying the output format.</li>
</ul>
</li>
</ul>
<p>Flexibility of SQL++ Style:</p>
<ul>
<li>SQL++ allows the <code>SELECT</code> clause to be written at the end of a query block, consistent with functional programming styles.</li>
<li>This enhances readability and composability while maintaining compatibility with SQL.</li>
</ul>
<h1 id="Pivoting-and-Unpivoting"><a href="#Pivoting-and-Unpivoting" class="headerlink" title="Pivoting and Unpivoting"></a>Pivoting and Unpivoting</h1><h2 id="UNPIVOT-Transforming-Attributes-into-Rows"><a href="#UNPIVOT-Transforming-Attributes-into-Rows" class="headerlink" title="UNPIVOT: Transforming Attributes into Rows"></a>UNPIVOT: Transforming Attributes into Rows</h2><ol>
<li><p><strong>What is Unpivoting?</strong></p>
<ul>
<li><p>Unpivoting is the process of converting attribute names (used as keys) into data rows.</p>
</li>
<li><p>This is useful for cases where key-value pairs in the data need to be analyzed as individual rows.</p>
</li>
</ul>
</li>
<li><p><strong>Example (Listing 19-21)</strong>:</p>
</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code12.png" alt="img"></p>
<ul>
<li><p>Input: A <code>closing_prices</code> collection where stock symbols (<code>amzn</code>, <code>goog</code>, <code>fb</code>) are attributes with prices as values.</p>
</li>
<li><p>Query (Listing 20): The <code>UNPIVOT</code> clause transforms these attributes into rows with fields for <code>symbol</code> and <code>price</code>.</p>
</li>
<li><p>Output (Listing 21): A flattened structure where each row contains the date, stock symbol, and price.</p>
</li>
</ul>
<h2 id="Pivoting"><a href="#Pivoting" class="headerlink" title="Pivoting"></a>Pivoting</h2><ol>
<li><strong>Purpose of Pivoting</strong>:<ul>
<li>Pivoting transforms rows into attributes (columns).</li>
</ul>
</li>
<li><strong>Example from Listings 23-25</strong>:</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code13.png" alt="img"></p>
<ul>
<li><strong>Input (Listing 23)</strong>: Rows of <code>today_stock_prices</code> where each stock symbol and its price are separate rows.</li>
<li><strong>Query (Listing 24)</strong>: The <code>PIVOT</code> operation turns these rows into a single object, using <code>sp.symbol</code> as attribute names and <code>sp.price</code> as their values.</li>
<li><strong>Output (Listing 25)</strong>: A tuple where each stock symbol (<code>amzn</code>, <code>goog</code>, <code>fb</code>) is an attribute, and their corresponding prices are the values.</li>
</ul>
<p><strong>Combining Grouping and Pivoting</strong></p>
<ol>
<li><strong>Using Pivot with Grouping</strong>:</li>
<li><ul>
<li>Combining <code>GROUP BY</code> and <code>PIVOT</code> enables aggregation of grouped rows into a more structured output.</li>
<li>This is particularly useful when working with time-series data or hierarchical datasets.</li>
</ul>
</li>
<li><strong>Example Query (Listing 26)</strong>:</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code14.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code15.png" alt="img"></p>
<ul>
<li>Input: Data from <code>stock_prices</code> (Listing 27), which includes stock prices for multiple dates as individual rows.</li>
<li>Query:<ul>
<li>Groups the data by <code>date</code> using <code>GROUP BY sp.date</code>.</li>
<li>Pivots the grouped rows to produce a nested structure where each date contains all its stock prices as attributes.</li>
</ul>
</li>
<li>Output (Listing 28): For each date, an object with a <code>prices</code> field lists the stock symbols as attributes and their respective prices as values.</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>SQL++ identifies aggregate functions as an SQL violation of functional composability. Give an example of an aggregate function and describe how it violates SQL’s functional composability.</strong></p>
<ul>
<li><p><strong>Aggregate Function</strong>:<code>COLL_AVG()</code></p>
</li>
<li><p><strong>Violation Explanation</strong>:</p>
<ul>
<li><p>In traditional SQL, aggregate functions like <code>AVG</code> processes the column and returns a single value.</p>
</li>
<li><p>In SQL++, this issue is resolved by providing <strong>composable versions</strong> of aggregate functions, such as <code>COLL_AVG</code>, which operate on collections, allowing intermediate results to flow naturally into the aggregation.</p>
</li>
</ul>
</li>
</ul>
<p><strong>With SQL++, what is the difference between NULL and Missing?</strong></p>
<p><code>NULL</code>: Indicates that an attribute exists but has no value.</p>
<p><code>MISSING</code>: Indicates that an attribute is completely absent in the data.</p>
<p><strong>True or false: One must define a schema for data prior to using SQL++.</strong> </p>
<p>False:</p>
<ul>
<li>SQL++ supports <strong>schema-optional</strong> and <strong>schema-less</strong> data formats, such as JSON.</li>
<li>While schemas can improve query optimization and validation, SQL++ can process data without requiring predefined schemas, making it highly flexible for semi-structured data use cases.</li>
</ul>
<p><strong>How does the I lease prevent a thundering herd?</strong></p>
<p>The I lease (Inhibit Lease) prevents a thundering herd problem by ensuring that only one read session at a time is allowed to query the RDBMS for a missing key-value pair in the Key-Value Store (KVS). Here’s how it works:</p>
<ol>
<li><p><strong>Thundering Herd Problem</strong>:</p>
<ul>
<li><p>When a key-value pair is not found in the KVS (a <strong>KVS miss</strong>), multiple read sessions might simultaneously query the RDBMS to fetch the value.</p>
</li>
<li><p>This can overload the RDBMS and degrade performance under high concurrency.</p>
</li>
</ul>
</li>
<li><p><strong>Role of the I Lease</strong>:</p>
<ul>
<li><p>When the first read session encounters a KVS miss, it requests an I lease for the key.</p>
</li>
<li><p>Once the I lease is granted, the KVS prevents other read sessions from querying the RDBMS for the same key.</p>
</li>
<li><p>All other read sessions must “back off” and wait for the value to be updated in the KVS by the session holding the I lease.</p>
</li>
</ul>
</li>
<li><p><strong>Result</strong>:</p>
<ul>
<li><p>The session with the I lease queries the RDBMS, retrieves the value, and populates the KVS.</p>
</li>
<li><p>Subsequent read sessions observe a <strong>KVS hit</strong> and do not need to access the RDBMS.</p>
</li>
<li><p>This mechanism avoids simultaneous RDBMS queries, effectively solving the thundering herd problem.</p>
</li>
</ul>
</li>
</ol>
<p><strong>What is the difference between invalidate and refresh&#x2F;refill for maintaining the cache consistent with the database management system?</strong></p>
<ul>
<li><strong>Invalidate</strong>: Deletes stale cache entries to prevent incorrect reads, but at the cost of forcing subsequent queries to access the RDBMS.</li>
<li><strong>Refresh&#x2F;Refill</strong>: Proactively updates the cache with new data, ensuring consistent reads while reducing future load on the RDBMS at the expense of immediate computation.</li>
</ul>
<p><strong>Describe how CAMP inserts a key-value pair in the cache.</strong></p>
<p><strong>Check Cache Capacity</strong></p>
<ul>
<li>If there is <strong>enough memory</strong> to store the new key-value pair:<ul>
<li>The pair is inserted directly into the appropriate <strong>priority group</strong> based on its cost-to-size ratio.</li>
<li>L is not updated.</li>
</ul>
</li>
<li>If the cache is <strong>full</strong>:<ul>
<li>CAMP selects one or more key-value pairs to <strong>evict</strong> based on their H(p) values.</li>
<li>It removes the pair(s) with the <strong>lowest H(p)</strong> values until there is sufficient space for the new pair.</li>
</ul>
</li>
</ul>
<p><strong>Insert the New Pair</strong></p>
<ul>
<li>The new key-value pair p is added to the cache, and its H(p) value is computed and recorded.</li>
<li>The pair is placed in the appropriate priority queue based on its cost-to-size ratio.</li>
</ul>
<p><strong>How does BG compute the SoAR of a database management system?</strong> </p>
<ol>
<li>Define the SLA.</li>
<li>Run a series of experiments with increasing numbers of threads (T) to find the peak throughput while ensuring SLA compliance.</li>
</ol>
<p>Reference: <a target="_blank" rel="noopener" href="https://escholarship.org/content/qt2bj3m590/qt2bj3m590_noSplash_084218340bb4e928c05878f04d01f04d.pdf">https://escholarship.org/content/qt2bj3m590/qt2bj3m590_noSplash_084218340bb4e928c05878f04d01f04d.pdf</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/en/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/en/">1</a><span class="page-number current">2</span><a class="page-number" href="/en/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/en/page/8/">8</a><a class="extend next" rel="next" href="/en/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yihang Wei</p>
  <div class="site-description" itemprop="description">聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>English</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical="">
      
        <option value="zh-CN" data-href="/page/2/index.html" selected="">
          简体中文
        </option>
      
        <option value="en" data-href="/en/page/2/index.html" selected="">
          English
        </option>
      
    </select>
  </div>

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yihang Wei</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '50%',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: '#fff',
  backgroundColor: '#fff',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '明暗',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

</body>
</html>

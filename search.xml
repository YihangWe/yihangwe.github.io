<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>常用函数</title>
    <url>/MySQL/2024/09/15/MySQL/%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h1><ul>
<li>CONCAT(S1, S2, …) 字符串拼接；</li>
<li>LOWER(str) 小写转换；</li>
<li>UPPER(str) 大写转换；</li>
<li>LPAD(str, n, pad) 左填充，用字符串 pad 对 str 左边进行填充，直到长度为 n；</li>
<li>RPAD(str, n, pad) 右填充，用字符串 pad 对 str 右边进行填充，直到长度为 n；</li>
<li>TRIM(str) 去掉字符串头部和尾部的空格；</li>
<li>SUBSTRING(str, start, len) 返回字符串 str 从 start 位置起的 len 长度的字符串，<strong>需要注意的是字符索引是从1开始的。</strong></li>
</ul>
<h1 id="数值函数"><a href="#数值函数" class="headerlink" title="数值函数"></a>数值函数</h1><ul>
<li>CEIL(x) 向上取整；</li>
<li>FLOOR(x) 向下取整；</li>
<li>MOD(x, y) 返回 x&#x2F;y 的模；</li>
<li>RAND() 返回0～1的随机数；</li>
<li>ROUND(x, y) 求参数 x 四舍五入的值并保留 y 位小数。</li>
</ul>
<h1 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h1><ul>
<li>CURDATE() 当前日期；</li>
<li>CURTIME() 当前时间；</li>
<li>NOW() 当前日期和时间；</li>
<li>YEAR(date) 获取指定 date 的年份；</li>
<li>MONTH(date) 获取指定 date 的月份；</li>
<li>DAY(date) 获取指定 date 的日期；</li>
<li>DATE_ADD(date, INTERVAL expr type) 返回一个日期&#x2F;时间值加上一个时间间隔 expr 后的时间值；</li>
<li>DATEDIFF(date1, date2) 返回起始时间 date1 和结束时间 date2 之间的天数。</li>
</ul>
<h1 id="流程函数"><a href="#流程函数" class="headerlink" title="流程函数"></a>流程函数</h1><ul>
<li>IF(value, t, f) 如果 value 为真，返回 t，否则返回 f；</li>
<li>IFNULL(value1, value2) 如果 value1 不为空，返回 value1，否则返回 value2；</li>
<li>CASE WHEN [val1] THEN [res1] … ELSE [default] END 如果 val1 为 true，返回 res1，… 否则返回 default 默认值；</li>
<li>CASE [expr] WHEN [val1] THEN [res1] … ELSE [default] END 如果 expr 的值等于 val1，返回 res1, … 否则返回 default 默认值。</li>
</ul>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>介绍</title>
    <url>/MySQL/2024/09/12/MySQL/%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>数据库是数据存储的仓库，是文件的集合，是依照某种数据模型组织起来并存放于二级存储器中的数据集合。</p>
<p>数据库实例是程序，是位于用户和操作系统之间的一层数据管理软件，用户对数据库数据的任何操作都是在数据库实例下进行的。应用程序只有通过数据库实例才能和数据库打交道。</p>
<p>在 MySQL 中，实例和数据库的关系通常是一一对应的。但在集群下可能存在一个数据库被多个数据库实例使用的情况。</p>
<p>关系型数据库（结构数据模型，表）：建立在关系模型基础上，由多张相互连接的<strong>二维表</strong>组成的数据库。</p>
<p>数据库管理系统（DBMS）：操纵和管理数据库的应用程序。</p>
<p>SQL：操作关系型数据库的编程语言，也是一套标准。</p>
<p>客户端&#x3D;&gt;数据库管理系统&#x3D;&gt;数据库&#x3D;&gt;数据表。</p>
<h1 id="数据库三大范式"><a href="#数据库三大范式" class="headerlink" title="数据库三大范式"></a>数据库三大范式</h1><p>第一范式（1NF）：确保表的每一列都是<strong>不可分割</strong>的基本数据单元，比如说用户地址，应该拆分成省、市、区、详细信息等 4 个字段。</p>
<p>第二范式（2NF）：在 1NF 的基础上，要求数据库表中的<strong>每一列都和主键直接相关</strong>，而不能只与主键的某一部分相关（主要针对联合主键）。</p>
<p>第三范式（3NF）：在 2NF 的基础上，消除非主键列对主键的传递依赖，即<strong>非主键列只依赖于主键列</strong>，不依赖于其他非主键列。</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>MySQL 是一款开源的关系型数据库管理系统（RDBMS），最初由 MySQL AB 于 1995 年 5 月 23 日推出，现由 Oracle 维护和发布。MySQL 使用 SQL 来定义、操作和管理数据表，将数据组织为由行与列构成的表格，以实现数据之间的关联与查询。它原生支持完整的 ACID 事务特性和多版本并发控制（MVCC），在高并发环境下能够保持数据的一致性与隔离度；InnoDB 引擎通过回滚段存储旧版本数据，并结合两阶段锁定（Two-Phase Locking）和插入意向锁等机制，实现并发控制与死锁检测。MySQL 可跨 Windows、Linux、macOS、FreeBSD 等多个操作系统部署，凭借易用性、高性能与可靠性，长期被广泛应用于 Web 应用、电子商务平台和企业级系统。</p>
<p>MySQL 采用可插拔的存储引擎架构，允许用户根据不同业务需求选择最合适的引擎。默认的 InnoDB 引擎提供事务处理、行级锁、外键约束、崩溃恢复（通过 redo log）和双写缓冲等功能，以确保数据安全与快速恢复。在 InnoDB 之前，MyISAM 曾为默认引擎，其采用表级锁设计、不支持事务与外键，适用于读密集型场景但无法满足高并发写入需求。此外，MySQL 还支持 Memory 引擎（将数据保存在内存中，适合临时表或高速缓存）和 NDB Cluster 引擎（面向分布式高可用集群，支持自动分片和多主复制），以满足不同场景下对性能与可用性的多样化需求。</p>
<p>在服务器层面，MySQL 包括 SQL 解析器、查询优化器和执行器三大组件。解析器负责将客户端提交的 SQL 文本进行词法与语法分析，生成内部抽象语法树（AST）；优化器基于统计信息与索引代价估算，选择最优执行计划；执行器则通过存储引擎接口调用底层引擎完成实际的数据访问和操作，例如数据页读取、加锁、写入等。MySQL 采用磁盘导向的存储架构，InnoDB 使用页为单位将数据加载到缓冲池并通过分代 LRU 策略进行页面替换，以优化磁盘 I&#x2F;O 性能。在并发查询执行方面，MySQL 以元组级的迭代器模型处理查询，不支持内部并行化，但可借助索引和优化器策略减少 I&#x2F;O 次数，从而提升查询效率。</p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>多表查询</title>
    <url>/MySQL/2024/09/23/MySQL/%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2/</url>
    <content><![CDATA[<h1 id="多表关系"><a href="#多表关系" class="headerlink" title="多表关系"></a>多表关系</h1><ul>
<li>一对多：在多的一方建立外键，指向一的一方的主键。如：部门-员工。</li>
<li>多对多：建立第三张中间表，其中至少包含两个外键，分别关联两方主键。如：学生-课程。</li>
<li>一对一：用于单表拆分，将一张表的基础字段放在一张表中，其他详情字段放在另一张表中，以提升操作效率。在任意一方加入外键，关联另一方的主键，并且设置外键为唯一（UNIQUE） 。</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p>在多表查询时，需要消除无效的笛卡尔积。</p>
</blockquote>
<h1 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h1><h2 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h2><p>相当于查询两张表交集部分数据。</p>
<ul>
<li>隐式内连接：<code>SELECT 字段列表 FROM 表1，表2 WHERE 条件;</code></li>
<li>显式内连接 ：<code>SELECT 字段列表 FROM 表1 [INNER] JOIN 表2 ON 连接条件;</code></li>
</ul>
<h2 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h2><p>左外连接：查询左表所有数据，以及两张表交集部分数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 字段列表 <span class="keyword">FROM</span> 表<span class="number">1</span> <span class="keyword">LEFT</span> [<span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> 表<span class="number">2</span> <span class="keyword">ON</span> 条件;</span><br></pre></td></tr></table></figure>

<p>右外连接：查询右表所有数据，以及两张表交集部分数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 字段列表 <span class="keyword">FROM</span> 表<span class="number">1</span> <span class="keyword">RIGHT</span> [<span class="keyword">OUTER</span>] <span class="keyword">JOIN</span> 表<span class="number">2</span> <span class="keyword">ON</span> 条件;</span><br></pre></td></tr></table></figure>

<p>自连接：当前表与自身的连接查询，必须使用表别名。可以是内连接，也可以是外连接。</p>
<p>联合查询：把多次查询的结果合并以形成一个新的查询结果集。不使用 ALL 的时候，有去重效果。联合查询的多张表之间的列数和字段类型需要<strong>保持一致</strong>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 字段列表 <span class="keyword">FROM</span> 表<span class="number">1</span> <span class="keyword">UNION</span> [<span class="keyword">ALL</span>] <span class="keyword">SELECT</span> 字段列表 <span class="keyword">FROM</span> 表<span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<h1 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h1><p>SQL语句中嵌套 SELECT 语句，外部语句可以是 INSERT&#x2F;UPDATE&#x2F;DELETE&#x2F;SELECT 中任何一个。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> 表名 <span class="keyword">WHERE</span> col <span class="operator">=</span> (<span class="keyword">SELECT</span> col <span class="keyword">FROM</span> 表<span class="number">2</span>);</span><br></pre></td></tr></table></figure>

<h2 id="子查询种类"><a href="#子查询种类" class="headerlink" title="子查询种类"></a>子查询种类</h2><p>根据结果</p>
<ul>
<li>标量子查询：返回结果是单个值。</li>
<li>列子查询：返回结果是一列。</li>
<li>行子查询：返回结果是一行（可以是多列）。</li>
<li>表子查询：返回结果是多行多列。</li>
</ul>
<p>根据位置：WHERE 之后，FROM 之后，SELECT 之后。</p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>SQL</title>
    <url>/MySQL/2024/09/14/MySQL/SQL/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>注释：</p>
<p>单行注释：<em>–</em> 或 #</p>
<p>多行注释：<em>&#x2F;**&#x2F;</em></p>
<p>SQL分类：</p>
<p>DDL：数据定义语言，<strong>定义</strong>数据库对象（数据库，表，字段）。</p>
<p>DML：数据操作语言，对数据进行<strong>增删改</strong>。</p>
<p>DQL：数据查询语言，<strong>查询</strong>数据表的记录。</p>
<p>DCL：数据<strong>控制</strong>语言，创建数据库用户，控制数据库访问权限。</p>
<h1 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h1><p>查询所有数据库：<code>SHOW DATABASES;</code></p>
<p>查询当前数据库：<code>SELECT DATABASE();</code></p>
<p>创建数据库：<code>CREATE DATABASE [IF NOT EXISTS] 数据库名 [DEFAULT CHARSET 字符集] [COLLATE 排序规则];</code></p>
<p>删除数据库：<code>DROP DATABASE [IF EXISTS] 数据库名;</code></p>
<p>切换数据库：<code>USE 数据库名;</code></p>
<p>查询当前数据库中的所有表：<code>SHOW TABLES;</code></p>
<p>查询表结构：<code>DESC 表名;</code></p>
<p>查询指定表的建表语句：<code>SHOW CREATE TABLE 表名;</code></p>
<p>创建表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> 表名(</span><br><span class="line">  字段<span class="number">1</span> 字段<span class="number">1</span>类型 [COMMENT 字段注释],</span><br><span class="line">  字段<span class="number">2</span> 字段<span class="number">2</span>类型 [COMMENT 字段注释],</span><br><span class="line">  ...</span><br><span class="line">  字段n 字段n类型 [COMMENT 字段注释]</span><br><span class="line">) [表注释];</span><br></pre></td></tr></table></figure>

<p>修改表：</p>
<ul>
<li>添加字段：<code>ALTER TABLE 表名 ADD 字段名 类型（长度）[COMMENT 注释] [约束];</code></li>
<li>修改数据类型： <code>ALTER TABLE 表名 MODIFY 字段名 新数据类型（长度）;</code></li>
<li>修改字段名和字段类型：<code>ALTER TABLE 表名 CHANGE 旧字段名 新字段名 类型（长度） [COMMENT 注释] [约束];</code></li>
<li>删除字段：<code>ALTER TABLE 表名 DROP 字段名;</code></li>
<li>修改表名：<code>ALTER TABLE 表名 RENAME TO 新表名;</code></li>
<li>删除表：<code>DROP TABLE [IF EXISTS] 表名;</code></li>
<li>删除指定表，并重新创建该表：<code>TRUNCATE TABLE;</code></li>
</ul>
<p>示例：</p>
<p>现有一张用户信息表 user_info，其中包含多年来在平台注册过的用户信息。</p>
<p>请在用户信息表，字段 level 的后面增加一列最多可保存 15 个汉字的字段 school；并将表中 job 列名改为 profession，同时 varchar 字段长度变为 10；achievement 的默认值设置为 0。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER TABLE</span> user_info <span class="keyword">ADD</span> school <span class="type">varchar</span>(<span class="number">15</span>) AFTER `level`;</span><br><span class="line"><span class="keyword">ALTER TABLE</span> user_info CHANGE job profession <span class="type">varchar</span>(<span class="number">10</span>);</span><br><span class="line"><span class="keyword">ALTER TABLE</span> user_info CHANGE <span class="keyword">COLUMN</span> achievement achievement <span class="type">int</span> <span class="keyword">DEFAULT</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure>

<h1 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h1><p>给指定字段添加数据：<code>INSERT INTO 表名 (字段1，字段2，...) VALUES (值1，值2);</code></p>
<p>给全部字段添加数据：<code>INSERT INTO 表名 VALUES (值1，值2);</code></p>
<p>批量添加数据：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT INTO</span> 表名 (字段<span class="number">1</span>，字段<span class="number">2</span>，...) <span class="keyword">VALUES</span> (值<span class="number">1</span>，值<span class="number">2</span>, ...), (值<span class="number">1</span>，值<span class="number">2</span>, ...), (值<span class="number">1</span>，值<span class="number">2</span>, ...);</span><br><span class="line"><span class="keyword">INSERT INTO</span> 表名 <span class="keyword">VALUES</span> (值<span class="number">1</span>，值<span class="number">2</span>，...), (值<span class="number">1</span>，值<span class="number">2</span>，...), (值<span class="number">1</span>，值<span class="number">2</span>，...);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>[!NOTE]</p>
<p>将多次单行插入合并为一次批量插入（multi-row INSERT）能够显著提升性能，主要原因包括：</p>
<ul>
<li>减少网络往返次数，降低客户端与服务器的通信开销；</li>
<li>减少 SQL 解析与执行计划的开销，只需对一条语句进行解析与优化；</li>
<li>减少事务与日志写入开销，合并写入二进制日志（binlog）和 InnoDB 重做日志；</li>
<li>优化索引更新，批量更新索引比单次多次更新更高效；</li>
<li>降低锁竞争与事务开销，减少锁的申请与释放次数；</li>
<li>充分利用 InnoDB 的 Group Commit 机制，进一步减少磁盘刷新次数。</li>
</ul>
</blockquote>
<p>示例：</p>
<p>现有一张试卷作答记录表exam_record，结构如下表，其中包含多年来的用户作答试卷记录，由于数据越来越多，维护难度越来越大，需要对数据表内容做精简，历史数据做备份。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT INTO</span> exam_record_before_2021(uid, exam_id, start_time, submit_time, score)</span><br><span class="line"><span class="keyword">SELECT</span> uid, exam_id, start_time, submit_time, score</span><br><span class="line"><span class="keyword">FROM</span> exam_record</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">YEAR</span>(submit_time) <span class="operator">&lt;</span> <span class="string">&#x27;2021&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>现在有一套ID为9003的高难度SQL试卷，时长为一个半小时，请你将 2021-01-01 00:00:00 作为发布时间插入到试题信息表examination_info（其表结构如下图），不管该ID试卷是否存在，都要插入成功，请尝试插入它。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">REPLACE <span class="keyword">INTO</span> examination_info</span><br><span class="line"><span class="keyword">VALUES</span>(<span class="keyword">NULL</span>,<span class="number">9003</span>,<span class="string">&#x27;SQL&#x27;</span>,<span class="string">&#x27;hard&#x27;</span>,<span class="number">90</span>,<span class="string">&#x27;2021-01-01 00:00:00&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> examination_info</span><br><span class="line"><span class="keyword">WHERE</span> exam_id<span class="operator">=</span><span class="number">9003</span>;</span><br><span class="line"><span class="keyword">INSERT INTO</span> examination_info</span><br><span class="line"><span class="keyword">VALUES</span>(<span class="keyword">NULL</span>,<span class="number">9003</span>, <span class="string">&#x27;SQL&#x27;</span>,<span class="string">&#x27;hard&#x27;</span>, <span class="number">90</span>, <span class="string">&#x27;2021-01-01 00:00:00&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>修改数据：<code>UPDATE 表名 SET 字段1=值1， 字段2=值2，... [WHERE 条件];</code></p>
<p>删除数据：<code>DELETE FROM 表名 [WHERE 条件];</code></p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>DELETE 与 TRUNCATE 的区别</strong></p>
<p>操作类型</p>
<ul>
<li>DELETE：属于 DML（数据操作语言），需要逐行删除数据，并记录每一行的删除操作。删除后需要手动提交事务。</li>
<li>TRUNCATE：属于 DDL（数据定义语言），直接释放表的所有数据页，然后重新初始化表，速度更快。</li>
</ul>
<p>日志记录</p>
<ul>
<li>DELETE：会记录每一行的删除操作到 binlog，用于事务回滚和主从同步。</li>
<li>TRUNCATE：只记录表的重建操作，不记录逐行删除，日志量较小。</li>
</ul>
<p>重置 AUTO_INCREMENT</p>
<ul>
<li>DELETE：不会重置自增值，下一次插入时继续从当前最大值递增。</li>
<li>TRUNCATE：会重置自增值为初始值。</li>
</ul>
<p>外键约束</p>
<ul>
<li>DELETE：可以针对有外键约束的表逐行删除，受外键规则影响。</li>
<li>TRUNCATE：不能直接操作有外键约束的表，否则会报错。</li>
</ul>
<p>当一个表中有大量的 DELETE 操作时，你会采取哪些措施来优化性能或管理存储空间？</p>
<ol>
<li>如果需要清空整张表，用 TRUNCATE 或 DROP。</li>
<li>如果 DELETE 操作是高频行为，考虑使用 分区表 或 分表。</li>
<li>如果需要保留数据历史，使用 软删除。</li>
<li>定期使用 OPTIMIZE TABLE 或分批 DELETE 来回收空间。</li>
</ol>
</blockquote>
<p>示例：</p>
<p>请删除exam_record表中未完成作答或作答时间小于5分钟整的记录中，开始作答时间最早的3条记录。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> exam_record <span class="keyword">where</span> date_add(start_time, <span class="type">interval</span> <span class="number">5</span> <span class="keyword">minute</span>) <span class="operator">&gt;</span> submit_time <span class="keyword">or</span> submit_time <span class="keyword">is</span> <span class="keyword">null</span> limit <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p>请删除exam_record表中所有记录，并重置自增主键。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> exam_record;</span><br></pre></td></tr></table></figure>

<h2 id="更新语句的执行过程"><a href="#更新语句的执行过程" class="headerlink" title="更新语句的执行过程"></a>更新语句的执行过程</h2><p><img src="/../../images/MySQL/mysql_update.drawio.png" alt="img"></p>
<p>在执行 UPDATE 时，InnoDB 会先按照 WHERE 条件对匹配的记录及其前后间隙加上 next-key 锁，以防幻读；然后从缓冲池中将数据页加载到内存，先在 Undo 日志中记录修改前的值（用于回滚和 MVCC），再将更新操作以物理日志的形式写入 Redo 日志并将数据页标记为“脏页”。之后 MySQL 采用两阶段提交：第一阶段将 Redo 日志持久化并标记为 prepare 状态，第二阶段将事务的所有变更以逻辑或行事件写入 Binlog 并执行 fsync，最后将 Redo 日志标记为已提交并释放锁，从而保证 Redo 与 Binlog 的原子一致性。</p>
<p>MySQL 在执行更新语句时，在服务层执行语句的解析和执行，在引擎层执行数据的提取和存储；在服务层对 binlog 进行写入，在引擎层对 redo log 进行写入。</p>
<h2 id="事务的两阶段提交"><a href="#事务的两阶段提交" class="headerlink" title="事务的两阶段提交"></a>事务的两阶段提交</h2><p>这是 MySQL 中保证数据一致性和持久性的关键机制。</p>
<p><img src="/../../images/MySQL/mysql_2pc.drawio.png" alt="mysql_2pc.drawio"></p>
<ol>
<li><strong>prepare 阶段</strong>：记录事务的变更到 redo log，并标记为 prepare 状态。</li>
<li><strong>binlog 写入</strong>：将对应的 SQL 语句写入 binlog。</li>
<li><strong>commit 阶段</strong>：将 redo log 中对应的日志条目标记为 commit 状态，并完成整个提交过程。</li>
</ol>
<p><strong>事务不一致的处理</strong></p>
<p>**情况一：**系统在 redo log 标记为 prepare 之后崩溃。这种情况下，事务已经记录到 redo log 中，但可能还未写入 binlog。恢复时，InnoDB 会检查 redo log 中的 prepare 状态。如果找到这样的记录，会继续检查 binlog。</p>
<ol>
<li>如果 binlog 中没有找到对应的记录，说明事务未提交，InnoDB 会回滚事务，确保数据一致性。</li>
<li>如果 binlog 中找到了对应的记录，说明事务已提交，InnoDB 会完成提交过程，确保数据一致性。</li>
</ol>
<p>**情况二：**系统在 binlog 写入后但在 redo log commit 前崩溃。在这种情况下，事务已经写入了 binlog，但未完成在 redo log 中的 commit 标记。恢复时，InnoDB 会首先检查 redo log，如果发现 prepare 状态的记录且 binlog 中有对应的记录，InnoDB 会将事务标记为 commit 状态并完成提交，这确保了事务的一致性和持久性。</p>
<h1 id="DQL"><a href="#DQL" class="headerlink" title="DQL"></a>DQL</h1><p>查询多个字段：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 字段<span class="number">1</span>，字段<span class="number">2</span>，字段<span class="number">3.</span>.. <span class="keyword">FROM</span> 表名;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> 表名;</span><br></pre></td></tr></table></figure>

<p>设置别名：<code>SELECT 字段1 [AS 别名1]，字段2[AS 别名2] ... FROM 表名;</code></p>
<p>去除重复记录：<code>SELECT DISTINCT 字段列表 FROM 表名;</code></p>
<p>聚合函数：<code>SELECT 聚合函数(字段) FROM 表名;</code></p>
<p>以上 SQL 语句将一列数据作为一个整体，进行纵向计算。NULL 不参与所有聚合函数运算。</p>
<p>分组查询：<code>SELECT 字段列表 FROM 表名 [WHERE 条件] GROUP BY 分组字段名 [HAVING 分组后过滤条件];</code></p>
<blockquote>
<p>[!NOTE]</p>
<p><strong>WHERE 和 HAVING 区别</strong></p>
<ul>
<li>执行时机不同：WHERE 是分组之前进行过滤；HAVING 是分组之后进行过滤。</li>
<li>判断条件不同：WHERE 不能对聚合函数进行判断，HAVING 可以。</li>
<li>执行顺序：WHERE &gt; 聚合函数 &gt; HAVING</li>
</ul>
</blockquote>
<p>排序查询：<code>SELECT 字段列表 FROM 表名 ORDER BY 字段1 排序方式1, 字段2 排序方式2;</code></p>
<p>排序方式：ASC 升序（默认），DESC 降序。</p>
<p>如果是多字段排序，当第一个字段值相同时，才会根据第二个字段进行排序。</p>
<p>分页查询：<code>SELECT 字段列表 FROM 表名 LIMIT 起始索引，查询记录数;</code></p>
<p>起始索引从0开始，起始索引 &#x3D;（查询页码 - 1）* 每页显示记录数;</p>
<h2 id="SELECT-查询执行顺序"><a href="#SELECT-查询执行顺序" class="headerlink" title="SELECT 查询执行顺序"></a>SELECT 查询执行顺序</h2><p><img src="/../../images/MySQL/mysql_select_exe_priority.drawio.png" alt="mysql_select_exe_priority.drawio"></p>
<ol>
<li><strong>FROM</strong>：对 FROM 子句中的左表<left_table>和右表<right_table>执行<strong>笛卡儿积</strong>（Cartesianproduct），产生虚拟表 VT1。</li>
<li><strong>ON</strong>：对虚拟表 VT1 应用 ON 筛选，只有那些符合<join_condition>的行才被插入虚拟表 VT2 中。</li>
<li><strong>JOIN</strong>：如果指定了 OUTER JOIN（如 LEFT OUTER JOIN、RIGHT OUTER JOIN），那么保留表中未匹配的行作为外部行添加到虚拟表 VT2 中，产生虚拟表 VT3。<strong>如果 FROM 子句包含两个以上表，则对上一个连接生成的结果表 VT3 和下一个表重复执行步骤 1）～步骤 3），直到处理完所有的表为止。</strong></li>
<li><strong>WHERE</strong>：对虚拟表 VT3 应用 WHERE 过滤条件，只有符合<where_condition>的记录才被插入虚拟表 VT4 中。</li>
<li><strong>GROUP BY</strong>：根据 GROUP BY 子句中的列，对 VT4 中的记录进行分组操作，产生 VT5</li>
<li><strong>CUBE | ROLLUP</strong>：对表 VT5 进行 CUBE 或 ROLLUP 操作，产生表 VT6。<ol>
<li><strong>CUBE</strong>：生成所有可能组合的汇总，包括每个维度的组合。适用于多维数据分析。</li>
<li><strong>ROLLUP</strong>：生成层级汇总，从详细级别到总体总和。适用于生成部分汇总数据。</li>
</ol>
</li>
<li><strong>HAVING</strong>：对虚拟表 VT6 应用 HAVING 过滤器，只有符合<having_condition>的记录才被插入虚拟表 VT7 中。</li>
<li><strong>SELECT</strong>：第二次执行 SELECT 操作，选择指定的列，插入到虚拟表 VT8 中。</li>
<li><strong>DISTINCT</strong>：去除重复数据，产生虚拟表 VT9。</li>
<li><strong>ORDER BY</strong>：将虚拟表 VT9 中的记录按照<order_by_list>进行排序操作，产生虚拟表 VT10。</li>
<li><strong>LIMIT</strong>：取出指定行的记录，产生虚拟表 VT11，并返回给查询用户。</li>
</ol>
<h2 id="DQL-语句的执行过程"><a href="#DQL-语句的执行过程" class="headerlink" title="DQL 语句的执行过程"></a>DQL 语句的执行过程</h2><p><img src="/../../images/MySQL/mysql_dql_exe.drawio.png" alt="img"></p>
<ol>
<li>客户端通过 TCP 连接发送 DQL 到 MySQL 服务器。</li>
<li>连接器开始处理该请求，包括建立连接、权限验证等。</li>
<li>解析器进行词法分词和语法分析，生成抽象语法树 AST 或解析树，同时检查 SQL 语法合法性和基本语法错误。在生成 AST 后，解析器将数据库名、表名和列名等标识符与内部数据字典中的对象进行映射，并对引用的对象执行权限检查，只有在用户拥有相应权限时，才允许继续执行。</li>
<li>优化器基于成本模型，对解析树进行查询重写（如谓词下推、视图展开）和逻辑优化，然后评估多种访问路径：全表扫描 vs 索引扫描、Nested Loop Join vs Hash Join 等，计算各方案的成本并选择最优执行计划，该计划以具体的物理操作算子（索引扫描、排序、聚合）为粒度进行组合。</li>
<li>执行器根据优化器生成的执行计划，调用相应的存储引擎接口，逐步执行算子操作（TableScan、IndexScan、Join、Sort），并在内存中构建最终的结果集。</li>
<li>对于 InnoDB 引擎，普通 SELECT 语句采用多版本并发控制（MVCC），从缓冲池内查找 Undo 日志中保存的历史版本来重建查询时刻的数据快照，若未命中则从磁盘读取并加载到缓冲池，同时维护 LRU 链表。</li>
<li>执行器完成结果集的生成后，通过 Protocol 层将数据逐行或一次性打包返回给客户端。</li>
</ol>
<h1 id="DCL"><a href="#DCL" class="headerlink" title="DCL"></a>DCL</h1><p>查询用户：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">USE mysql;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>

<p>创建用户：<code>CREATE USER &#39;用户名&#39;@&#39;主机名&#39; IDENTIFIED BY &#39;密码&#39;;</code></p>
<p>修改用户密码：<code>ALTER USER &#39;用户名&#39;@&#39;主机名&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;;</code></p>
<p>删除用户：<code>DROP USER &#39;用户名&#39;@&#39;主机名&#39;;</code></p>
<p>查询权限：<code>SHOW GRANTS FOR &#39;用户名&#39;@&#39;主机名&#39;;</code></p>
<p>授予权限：<code>GRANT 授权列表 ON 数据库名.表名 TO &#39;用户名&#39;@&#39;主机名&#39;;</code></p>
<p>撤销权限：<code>REVOKE 权限列表 ON 数据库名.表名 FROM &#39;用户名&#39;@&#39;主机名&#39;;</code></p>
<blockquote>
<p>[!NOTE]</p>
<p>主机名可使用 % 通配。</p>
<p>多个权限之间使用逗号分隔。</p>
<p>数据库名和表名可使用 <code>*</code> 进行通配，代表所有。</p>
</blockquote>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>DynamoDB</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/DynamoDB/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>Dynamo has the ability of incremental scalability and predictable high performance, but it carries the operational complexity of self-managed large database systems.</p>
<p>SimpleDB is easy to administrate a cloud service, consistency, and a table-based data model, but it has limitations that tables have a small capacity in terms of storage and of request throughput, and that a unpredictable query and write latency.</p>
<p>DynamoDB &#x3D; Dynamo + SImpleDB</p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/dynamodb_arch.png" alt="img"></p>
<ol>
<li><strong>DynamoDB Tables and Primary Keys</strong>: A DynamoDB table is a collection of items, each uniquely identified by a primary key. The primary key can be a partition key or a composite key (partition key + sort key).</li>
<li><strong>Secondary Indexes</strong>: DynamoDB supports secondary indexes, allowing queries using alternate keys in addition to the primary key, enhancing query capabilities.</li>
<li><strong>Partitions and Replication Groups</strong>: DynamoDB tables are divided into multiple partitions, with each partition managing a contiguous key range. Each partition has multiple replicas across different Availability Zones for high availability and durability. A <strong>Multi-Paxos</strong> consensus is used for leader election within the replication group, and the leader handles writes and strongly consistent reads.</li>
<li><strong>Write-Ahead Logs and Consistency</strong>: The leader replica generates a write-ahead log for write requests. A write is acknowledged once a quorum of replicas persists the log. DynamoDB supports both strongly consistent and eventually consistent reads.</li>
<li><strong>Failure Detection and Leader Election</strong>: If the current leader is detected as unhealthy, other replicas initiate a new election. The new leader can only start serving writes or consistent reads after the old leader’s lease expires.</li>
<li><strong>Autoadmin Service</strong>: The autoadmin service monitors the health of the fleet and partitions, scaling tables and replacing unhealthy replicas or hardware to maintain system stability. It automatically detects and resolves issues, ensuring a stable and healthy infrastructure.</li>
</ol>
<h1 id="Journey-from-provisioned-to-on-demand"><a href="#Journey-from-provisioned-to-on-demand" class="headerlink" title="Journey from provisioned to on-demand"></a>Journey from provisioned to on-demand</h1><ul>
<li><p><strong>Bursting:</strong> To address the issue of uneven workload distribution across partitions, DynamoDB introduced the concept of <strong>bursting</strong>. Bursting allows an application to utilize unused capacity at the partition level when its provisioned throughput is exhausted, helping to <strong>handle short-term spikes</strong> in workload. DynamoDB retains unused capacity in a partition for <strong>up to 300 seconds</strong>, which can be tapped into when the consumed capacity exceeds the provisioned capacity. This reserved capacity is referred to as <strong>burst capacity</strong>.</p>
<p><strong>How It Works</strong>: DynamoDB manages throughput using multiple <strong>token buckets</strong>:</p>
<ul>
<li><p><strong>Each partition has two token buckets</strong>: one for allocated capacity and another for burst capacity. <strong>Each storage node has a token bucket</strong> that controls the overall load across partitions hosted on that node.</p>
</li>
<li><p>When a read or write request arrives at a storage node, the system first checks the partition’s token bucket. If the allocated capacity has been exhausted, burst capacity can be used, but only if there are available tokens at both the burst token level and the node level.</p>
</li>
<li><p><strong>Additional Check for Write Requests</strong>: When using burst capacity for write requests, an additional check is performed to ensure that other replica nodes for the partition also have sufficient capacity. This ensures that the write operation can be completed safely and consistently across all replicas. The leader replica periodically gathers information about the node-level capacity of other members in the replication group to facilitate this process.</p>
</li>
</ul>
</li>
<li><p><strong>Adaptive (deprecated):</strong> DynamoDB launched adaptive capacity to better absorb <strong>longlived</strong> spikes that cannot be absorbed by the burst capacity. Adaptive capacity allowed DynamoDB to better absorb workloads that had heavily skewed access patterns across partitions. Adaptive capacity actively monitored the provisioned and consumed capacity of all the tables.</p>
<ul>
<li><p>If a table experienced throttling and the table level throughput was not exceeded, then it would automatically increase (boost) the allocated throughput of the partitions of the table using a proportional control algorithm.</p>
</li>
<li><p>If the table was consuming more than its provisioned capacity then capacity of the partitions which received the boost would be decreased. The autoadmin system ensured that partitions receiving boost were relocated to an appropriate node that had the capacity to serve the increased throughput.</p>
</li>
</ul>
</li>
<li><p>GAC, how does it work:</p>
<ul>
<li><p><strong>Global Throughput Tracking and Management</strong></p>
<ul>
<li><p><strong>Global Token Management</strong>: GAC uses a <strong>token bucket system</strong> to manage the overall throughput (RCUs and WCUs) of a DynamoDB table.</p>
</li>
<li><p><strong>Token Buckets</strong>: Each request router maintains a <strong>local token bucket</strong> to handle requests. When tokens are depleted locally, the router requests more tokens from GAC, which manages the global distribution of these tokens across partitions.</p>
</li>
</ul>
</li>
<li><p><strong>Dynamic Token Allocation</strong></p>
<ul>
<li><p><strong>Periodic Replenishment</strong>: GAC regularly communicates with the request routers every few seconds to replenish their token buckets. The amount of tokens allocated is based on the overall resource consumption of the table, particularly when certain partitions are experiencing high traffic.</p>
</li>
<li><p><strong>Handling Hot Partitions</strong>: When specific partitions become hot, GAC dynamically allocates additional tokens to those partitions.</p>
</li>
</ul>
</li>
<li><p><strong>Capacity Limits and Isolation</strong></p>
<ul>
<li><p><strong>Global Throughput Limits</strong>: GAC ensures that the total number of tokens allocated to partitions does not exceed the <strong>provisioned capacity</strong> for the entire table.</p>
</li>
<li><p><strong>Node-Level Limits</strong>: Although GAC allocates tokens globally, each partition is subject to the <strong>maximum throughput capacity of its storage node</strong>. This ensures that no single partition can consume more than its node’s allowable resources.</p>
</li>
</ul>
</li>
<li><p><strong>Stateless and Distributed Design</strong></p>
<ul>
<li><p><strong>Stateless Operation</strong>: GAC operates in a <strong>stateless manner</strong>, meaning it calculates token allocations in real-time based on incoming client requests. It doesn’t rely on long-term stored states, so GAC servers can be restarted or stopped without affecting the system’s overall operation.</p>
</li>
<li><p><strong>Distributed Architecture</strong>: GAC uses a distributed architecture, where multiple GAC instances coordinate using a <strong>hash ring</strong>. This allows GAC to scale horizontally and handle requests from multiple routers efficiently.</p>
</li>
</ul>
</li>
<li><p><strong>Defense-in-Depth with Partition-Level Token Buckets</strong></p>
<ul>
<li><p><strong>Partition-Level Control</strong>: Even though GAC manages tokens globally, DynamoDB still retains <strong>partition-level token buckets</strong> for additional protection. These buckets ensure that no single partition consumes excessive resources, offering a secondary layer of isolation and control.</p>
</li>
<li><p><strong>Resource Isolation</strong>: Partition-level token buckets prevent any single application or partition from monopolizing the resources of the storage node.</p>
</li>
</ul>
</li>
<li><p><strong>Token Consumption and Replenishment Process</strong></p>
<ul>
<li><p>When a request is made, the request router checks its local token bucket for available tokens. If enough tokens are present, the request is processed.</p>
</li>
<li><p>If the local tokens are depleted, the request router asks GAC for more tokens.</p>
</li>
<li><p>GAC calculates the global consumption of tokens for the table and allocates more tokens to the router based on overall resource usage.</p>
</li>
<li><p>Once tokens are used up or expire, the process repeats, with the router requesting new tokens from GAC.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Proactive load balancing mechanism</p>
<ul>
<li><p>Independent Monitoring: Each storage node independently monitors the total throughput (read&#x2F;write requests) and data size of all the partition replicas it hosts.</p>
</li>
<li><p>Threshold Detection: When the throughput or data size of a partition replica approaches or exceeds a predefined threshold of the node’s capacity, that partition replica is flagged as a candidate for migration.</p>
</li>
<li><p>Reporting to Autoadmin Service: The storage node reports the list of over-utilized partition replicas to the Autoadmin service, which manages the load balancing process.</p>
</li>
<li><p>Automatic Migration: The Autoadmin service finds a new storage node, usually located in a different Availability Zone, that can accommodate the migrating partition replica. This new node must have enough spare capacity to handle the increased load.</p>
</li>
<li><p>GAC 和 bursting 都擅长处理 短期或临时的高负载，但如果某个分区长期处于高负载状态（例如一个分区持续有热点键），这些机制可能无法完全消除该分区对特定节点的影响。在这种情况下，自动迁移分区副本是更长期有效的解决方案。</p>
</li>
</ul>
</li>
<li><p>Even with GAC and partition bursting capacity, DynamoDB tables may still experience throttling if traffic is heavily concentrated on a specific set of items. When the throughput for a partition exceeds a certain threshold, the system splits the partition according to the observed key distribution, rather than simply splitting the key range in the middle. These smaller partitions are typically distributed to different storage nodes. However, some workloads may not benefit from this mechanism, such as:</p>
<ul>
<li><p>A partition where traffic is concentrated on a single item.</p>
</li>
<li><p>A partition where the key range is accessed sequentially.</p>
</li>
</ul>
</li>
<li><p>DynamoDB’s <strong>on-demand tables</strong> eliminate the need for customers to manually set throughput. The system automatically adjusts resources based on actual read and write requests, enabling it to quickly adapt to sudden traffic increases. Specifically:</p>
<ul>
<li><p>DynamoDB automatically scales up to <strong>twice the previous peak traffic</strong> to handle more requests instantly.</p>
</li>
<li><p>If traffic continues to increase, DynamoDB further allocates more resources to prevent throttling and maintain performance.</p>
</li>
</ul>
<p>The scaling mechanism for on-demand tables is achieved through <strong>partition splitting</strong>, where partitions are split based on traffic patterns to ensure each partition has sufficient resources. At the same time, <strong>GAC (Global Admission Control)</strong> monitors the system to prevent any single application from consuming too many resources, maintaining overall system stability.</p>
</li>
</ul>
<p><strong>机制之间的关联总结：</strong></p>
<ul>
<li><strong>On-demand tables</strong> 依赖 <strong>GAC</strong> 和 <strong>bursting</strong> 来动态扩展资源，处理流量波动。</li>
<li><strong>GAC</strong> 管理整个系统的全局资源分配，确保突发和按需扩展时不影响其他应用，同时在必要时与 <strong>proactive load balancing</strong> 机制配合，进行分区迁移。</li>
<li><strong>Bursting</strong> 提供短期解决方案，而当负载持续增加时，系统会通过 <strong>主动负载平衡</strong> 来长期调整资源分配，防止系统瓶颈。</li>
</ul>
<h1 id="Durability-and-correctness"><a href="#Durability-and-correctness" class="headerlink" title="Durability and correctness"></a>Durability and correctness</h1><h2 id="Hardware-failures"><a href="#Hardware-failures" class="headerlink" title="Hardware failures"></a>Hardware failures</h2><p>The write-ahead logs in DynamoDB are crucial for ensuring data durability and crash recovery. Each partition has three replicas that store the write-ahead logs. To enhance durability, the logs are periodically archived to Amazon S3. The unarchived logs typically amount to a few hundred megabytes.</p>
<p>In large-scale systems, hardware failures such as memory or disk failures are common. When a node fails, all replication groups hosted on that node are reduced to two copies. The process of repairing a storage replica can take several minutes, as it involves copying both the B-tree and the write-ahead logs.</p>
<p>When the system detects an unhealthy replica, the leader of the replication group adds a log replica to ensure data durability is not compromised. Since only the recent write-ahead logs need to be copied without the B-tree, adding the log replica takes just a few seconds. This quick addition helps restore the affected replication group, ensuring that the most recent writes remain highly durable.</p>
<h2 id="Silent-data-errors"><a href="#Silent-data-errors" class="headerlink" title="Silent data errors"></a>Silent data errors</h2><p>Hardware failures can cause incorrect data storage: In DynamoDB, errors may occur due to issues with storage media, CPU, or memory, and these errors are often difficult to detect.</p>
<p>Extensive use of checksums: DynamoDB maintains checksums for every log entry, message, and log file to detect silent errors and ensure data integrity during each data transfer. When messages are transmitted between nodes, checksums verify whether errors occurred during transmission.</p>
<p>Log archiving and validation: Each log file archived to S3 has a manifest that records details such as the table, partition, and data markers. Before uploading, the archiving agent performs various checks, including checksum validation, verifying that the log belongs to the correct table and partition, and ensuring that there are no gaps in the sequence numbers.</p>
<p>Multiple replica log archiving: Log archiving agents run on all three replicas. If one agent finds that a log file has already been archived, it downloads the file and compares it with the local write-ahead log to verify data integrity.</p>
<p>Checksum validation during S3 upload: Every log file and manifest file is uploaded to S3 with a content checksum. S3 verifies this checksum during the upload process to catch any errors in data transmission.</p>
<h2 id="Continuous-verification"><a href="#Continuous-verification" class="headerlink" title="Continuous verification"></a>Continuous verification</h2><p>Continuous Data Integrity Verification: DynamoDB continuously verifies data at rest to detect silent data errors and bit rot, which can occur due to hardware failures or data corruption. This is a critical defense mechanism for maintaining data reliability.</p>
<p>Scrub Process: The scrub process is central to detecting unforeseen errors. It checks two main aspects:</p>
<ul>
<li><strong>Replica Consistency</strong>: Ensures that all three replicas in a replication group have identical data.</li>
<li><strong>Archived Log Reconstruction</strong>: Rebuilds an offline replica using archived write-ahead logs from S3 and verifies that it matches the live replica.</li>
</ul>
<p>Verification Mechanism: Scrub computes checksums for the live replicas and compares them with those generated from replicas built using archived logs.</p>
<p>Defense in Depth: This mechanism ensures that live storage replicas and those rebuilt from historical logs remain consistent, providing confidence in the system’s integrity and reliability.</p>
<h2 id="Backups-and-restores"><a href="#Backups-and-restores" class="headerlink" title="Backups and restores"></a>Backups and restores</h2><p>Backup and Restore Mechanism: DynamoDB supports backup and restore to protect against logical corruption caused by bugs in customer applications. Backups and restores are built using write-ahead logs stored in S3 and do not affect table performance or availability.</p>
<p>Backup Consistency: Backups are full copies of DynamoDB tables, consistent across multiple partitions to the nearest second, and stored in Amazon S3. Data can be restored to a new DynamoDB table at any time.</p>
<p>Point-in-Time Restore: DynamoDB supports point-in-time restore, allowing customers to restore a table to any point within the last 35 days. This feature creates periodic snapshots of table partitions and stores them in S3.</p>
<p>Snapshots and Write-Ahead Logs: For point-in-time restore, DynamoDB identifies the closest snapshots to the requested time, applies the corresponding write-ahead logs, and restores the table to the desired state.</p>
<h1 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a>Availability</h1><h2 id="Write-and-consistent-read-availability"><a href="#Write-and-consistent-read-availability" class="headerlink" title="Write and consistent read availability"></a>Write and consistent read availability</h2><p>Write Availability: DynamoDB partition write availability depends on having a healthy leader and a healthy write quorum. A write quorum in DynamoDB requires two out of three replicas across different Availability Zones (AZs) to be healthy.</p>
<p>Handling Write Quorum Failures: <strong>If one replica becomes unresponsive, the leader adds a log replica, which is the fastest way to meet the quorum requirement and minimize write disruptions caused by an unhealthy quorum.</strong></p>
<p>Consistent Reads: Consistent reads are served by the leader replica. <strong>If the leader fails, other replicas detect the failure and elect a new leader to minimize disruptions to consistent read availability.</strong></p>
<p>Impact of Log Replicas: The introduction of log replicas was a significant system change. The use of the formally proven Paxos protocol provided confidence to safely implement this change, increasing system availability. DynamoDB can run millions of Paxos groups with log replicas in a single region.</p>
<p>Eventually Consistent Reads: Eventually consistent reads can be served by any of the replicas.</p>
<h2 id="Failure-detection"><a href="#Failure-detection" class="headerlink" title="Failure detection"></a>Failure detection</h2><p>New Leader Waits for Lease Expiry: A newly elected leader must wait for the old leader’s lease to expire before handling traffic, causing a few seconds of disruption where no new writes or consistent reads can be processed.</p>
<p>Importance of Leader Failure Detection: Quick and robust leader failure detection is crucial for minimizing disruptions. False positives in failure detection can lead to unnecessary leader elections, further disrupting availability.</p>
<p>Impact of Gray Network Failures: Gray network failures, such as communication issues between nodes or routers, can result in false or missed failure detections. These failures can trigger unnecessary leader elections, causing availability interruptions.</p>
<p>Improved Failure Detection Algorithm: To address the availability issues caused by gray failures, DynamoDB’s failure detection algorithm was improved. <strong>When a follower attempts to trigger a failover, it first checks with other replicas to see if they can still communicate with the leader. If they report the leader is healthy, the follower cancels the failover attempt.</strong> This change significantly reduced false leader elections and minimized availability disruptions.</p>
<h2 id="Metadata-availability"><a href="#Metadata-availability" class="headerlink" title="Metadata availability"></a>Metadata availability</h2><p>Metadata Needs for Request Routers: DynamoDB’s request routers require metadata mapping between table primary keys and storage nodes. Initially, this metadata was stored in DynamoDB, and the routers cached it locally. Although the cache hit rate was high, cache misses or cold starts caused metadata lookup traffic spikes, potentially destabilizing the system.</p>
<p>Caching Challenges: When caches failed or during cold starts, request routers frequently queried the metadata service, putting immense pressure on it and leading to cascading failures in other parts of the system.</p>
<p>Introduction of MemDS: <strong>To reduce reliance on local caches, DynamoDB introduced MemDS, a distributed in-memory data store for storing and replicating metadata.</strong> MemDS scales horizontally to handle all incoming requests and stores data in a compressed format. It uses a Perkle tree structure, combining Patricia and Merkle tree features for efficient key lookups and range queries.</p>
<p>Perkle Tree Operations: MemDS supports efficient key lookups, range queries, and special operations like floor (find the largest key ≤ given key) and ceiling (find the smallest key ≥ given key) for metadata retrieval.</p>
<p>New Partition Map Cache: DynamoDB implemented a new cache on request routers, addressing the issues of bimodal behavior. Even when a cache hit occurs, an asynchronous call is made to MemDS to refresh the cache. This ensures that MemDS consistently handles a steady volume of traffic, preventing reliance on cache hit ratios and avoiding cascading failures when caches become ineffective.</p>
<p>Partition Membership Updates: DynamoDB storage nodes, the authoritative source of partition membership data, push updates to MemDS. If a request router queries an incorrect storage node due to outdated information, the node provides updated membership data or triggers a new MemDS lookup.</p>
<h1 id="Programming-Interface"><a href="#Programming-Interface" class="headerlink" title="Programming Interface"></a>Programming Interface</h1><ol>
<li><p><strong>Key-Value Store</strong></p>
<p>DynamoDB allows users to create tables that can grow almost indefinitely. Each table is a collection of items, and each item is a collection of attributes. Each item is uniquely identified by a primary key, ensuring uniqueness within the table. DynamoDB provides a simple interface to store or retrieve items from a table or an index.</p>
</li>
<li><p><strong>Read and Write Operations</strong></p>
<p>DynamoDB operates as a key-value store, and the most common operations used by applications involve reading and writing data. These operations include:</p>
<ul>
<li><p><strong>GetItem</strong>: Retrieves an item with a given primary key.</p>
</li>
<li><p><strong>PutItem</strong>: Inserts a new item or replaces an existing one.</p>
</li>
<li><p><strong>UpdateItem</strong>: Updates an existing item, or adds it if it doesn’t exist.</p>
</li>
<li><p><strong>DeleteItem</strong>: Deletes an item from the table based on the primary key.</p>
</li>
</ul>
<p>These last three operations (PutItem, UpdateItem, and DeleteItem) are collectively referred to as writes. A write operation can optionally include conditions that must be satisfied for the operation to be executed successfully. For instance, you could specify that a PutItem operation should only succeed if the item doesn’t already exist.</p>
</li>
<li><p><strong>Transactional Operations</strong></p>
<p>DynamoDB supports transactions through two key operations:</p>
<ul>
<li><p><strong>TransactGetItems</strong>: Used for reading multiple items atomically. It retrieves the latest versions of items from one or more tables at a single point in time, ensuring consistency. If any conflicting operation is modifying an item that’s being read, the transaction will be rejected.</p>
</li>
<li><p><strong>TransactWriteItems</strong>: This is used for performing atomic writes across multiple items and tables. It allows you to create, update, or delete multiple items in one or more tables within a single atomic transaction. This ensures that either all changes happen, or none do. The operation is synchronous and idempotent (meaning it can be retried without causing duplicate effects). TransactWriteItems can include conditions on the current values of the items, and the operation is rejected if these conditions aren’t met.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/dynamodb_txn.png" alt="img"></p>
<p><strong>Request Router (RR)</strong>:</p>
<ul>
<li>The <strong>Request Router</strong> is the first major component that handles incoming requests after they pass through the network.</li>
<li><strong>Authentication and Authorization</strong>: RR typically interacts with an <strong>Authentication System</strong> to ensure that the request is valid and the user has the proper permissions to access or modify the data.</li>
<li><strong>Routing Requests</strong>: Once a request is authenticated, the RR determines which <strong>Storage Nodes</strong> the request should be forwarded to. It uses the <strong>Metadata System</strong> to map the key(s) involved in the request to the correct storage nodes, as the data is distributed across many nodes.</li>
<li><strong>Forwarding Requests</strong>: Depending on whether the operation is a simple read&#x2F;write or part of a larger transaction, the RR may route the request directly to storage nodes or to the Transaction Coordinator.</li>
</ul>
<p><strong>Transaction Coordinator (TC)</strong>:</p>
<ul>
<li>The <strong>Transaction Coordinator</strong> plays a central role in handling transactions that span multiple items or storage nodes.</li>
<li><strong>Transaction Management</strong>: For requests that involve multiple storage nodes or require consistency (e.g., multi-item writes in a transaction), the RR forwards the request to the TC. The TC is responsible for breaking down the transaction into individual operations and coordinating these operations across the necessary storage nodes.</li>
<li><strong>Distributed Transaction Execution</strong>: The TC ensures that the operations follow the appropriate protocol (e.g., two-phase commit) to guarantee atomicity and consistency, ensuring that all parts of the transaction are either completed successfully or rolled back.</li>
<li><strong>Timestamp Assignment and Conflict Resolution</strong>: In a timestamp-based system like DynamoDB, the TC may assign timestamps to ensure the correct ordering of operations and manage any potential conflicts between concurrent transactions.</li>
</ul>
<p>In summary:</p>
<ul>
<li><strong>Request Router (RR)</strong> handles initial authentication and routing of requests to the appropriate storage nodes or transaction coordinator.</li>
<li><strong>Transaction Coordinator (TC)</strong> manages distributed transactions, ensuring data consistency and handling multi-node operations.</li>
</ul>
<h1 id="Transaction-execution"><a href="#Transaction-execution" class="headerlink" title="Transaction execution"></a>Transaction execution</h1><ol>
<li><p><strong>Transaction Routing</strong></p>
<p>All operation requests first reach a set of frontend hosts known as request routers. These routers are responsible for authenticating the requests and routing them to the appropriate storage nodes. Storage nodes are mapped based on key ranges. For transaction management, the routers forward transaction operations to transaction coordinators.</p>
<p>Transaction coordinators break down the transaction into multiple operations targeting different items and coordinate the execution of these operations across the storage nodes using a distributed protocol.</p>
</li>
<li><p><strong>Timestamp Ordering</strong></p>
<p>Each transaction is assigned a timestamp that defines its logical execution order. Multiple transaction coordinators operate in parallel, and different coordinators assign timestamps to different transactions. As long as transactions execute in the assigned order, serializability is maintained.</p>
<p>The storage nodes are responsible for ensuring that the operations on the items they manage are executed in the correct order and rejecting transactions that cannot be properly ordered.</p>
</li>
<li><p><strong>Write Transaction Protocol</strong></p>
<p>DynamoDB uses a two-phase commit protocol to ensure that the write operations within a transaction are atomic and executed in the correct order. In the prepare phase, the coordinator prepares all the write operations. If all storage nodes accept the operations, the transaction is committed; otherwise, it is canceled.</p>
<p>The storage nodes record the timestamp and metadata of each item involved in the transaction to ensure the transaction is handled correctly.</p>
</li>
<li><p><strong>Read Transaction Protocol</strong></p>
<p>Read transactions also use a two-phase protocol, but it differs from the write transaction protocol. DynamoDB designed a two-phase read protocol without write operations to avoid adding latency and costs to reads.</p>
<p>In the first phase, the coordinator reads all the items involved in the transaction, along with their Log Sequence Numbers (LSN). In the second phase, if the LSN has not changed, the read is successful; otherwise, the read is rejected.</p>
</li>
<li><p><strong>Recovery and Fault Tolerance</strong></p>
<p>If a storage node fails, the leadership role transfers to another storage node within the same replication group, with transaction metadata persistently stored and replicated across the nodes.</p>
<p>Transaction coordinator failures are more complex. Coordinators maintain a persistent record of each transaction to ensure atomicity and completeness. Recovery managers periodically scan these transaction records, looking for incomplete transactions, and reassign them to new coordinators to resume execution.</p>
</li>
</ol>
<h1 id="Two-phase-commit-2PC"><a href="#Two-phase-commit-2PC" class="headerlink" title="Two-phase commit (2PC)"></a>Two-phase commit (2PC)</h1><ol>
<li><p><strong>Prepare Phase</strong></p>
<p>In the prepare phase, the transaction coordinator (TC) is responsible for sending the transaction’s write operations to all the participating storage nodes. The coordinator breaks down the transaction into individual operations targeting specific data items and sends a prepare message to each storage node involved. This message includes:</p>
<ul>
<li><p>The transaction’s timestamp.</p>
</li>
<li><p>The transaction’s unique identifier (ID).</p>
</li>
<li><p>The specific operation to be performed on the data item (such as insert, update, or delete).</p>
</li>
</ul>
<p>Upon receiving the prepare message, each storage node evaluates whether it can accept the transaction. The storage node will accept the transaction’s write operation if all of the following conditions are met:</p>
<ul>
<li><p><strong>Preconditions</strong> are satisfied (e.g., a condition might be that the item must exist, or that it has a certain value).</p>
</li>
<li><p>The write operation does not violate any <strong>system restrictions</strong> (e.g., exceeding the maximum item size).</p>
</li>
<li><p>The transaction’s timestamp is <strong>greater than</strong> the item’s last write timestamp, indicating that this operation is the most recent.</p>
</li>
<li><p>There are no <strong>ongoing transactions</strong> attempting to write to the same item.</p>
</li>
</ul>
<p>If all participating storage nodes accept the transaction during the prepare phase, the coordinator moves to the commit phase. If any node rejects the transaction (e.g., due to a failed precondition or timestamp conflict), the transaction is canceled.</p>
</li>
<li><p><strong>Commit Phase</strong></p>
<p>Once the transaction has been accepted by all storage nodes during the prepare phase, the coordinator enters the commit phase. During this phase, the coordinator sends a commit message to all the storage nodes, instructing them to apply the write operations. Each storage node then:</p>
<ul>
<li><p>Applies the prepared write operations to the local items.</p>
</li>
<li><p>Updates the <strong>timestamp</strong> of the item to reflect the transaction’s timestamp.</p>
</li>
<li><p>Updates the timestamps of any items where preconditions were checked, even if no write operation was performed.</p>
</li>
</ul>
<p>If any node rejects the transaction during the prepare phase, the coordinator sends a cancel message to all storage nodes, instructing them to discard any prepared changes. No writes are applied, ensuring atomicity.</p>
</li>
</ol>
<h1 id="Adapting-timestamp-ordering-for-key-value-operations"><a href="#Adapting-timestamp-ordering-for-key-value-operations" class="headerlink" title="Adapting timestamp ordering for key-value operations"></a>Adapting timestamp ordering for key-value operations</h1><ol>
<li><p><strong>Individual Item Read Operations</strong></p>
<p>In DynamoDB, even if there is a prepared transaction attempting to read to a particular data item, the system still allows read operations on that item. Specifically:</p>
<ul>
<li><p><strong>Bypassing the transaction coordinator</strong>: Non-transactional <code>GetItem</code> operations are routed directly to the storage node responsible for the item, bypassing the transaction coordinator. This avoids potential transaction locks or delays.</p>
</li>
<li><p><strong>Returning the latest data immediately</strong>: The storage node immediately returns the latest committed value of the item, regardless of whether a prepared transaction may later update it.</p>
</li>
<li><p><strong>Timestamp assignment</strong>: This read operation is assigned a timestamp that is after the last write operation’s timestamp but before the prepared transaction’s commit timestamp. This ensures the read operation is serializable, meaning it is placed between the last completed write and the pending write.</p>
</li>
</ul>
</li>
<li><p><strong>Individual Item Write Operations</strong></p>
<p>In most cases, DynamoDB allows individual item write operations to be executed immediately, often before prepared transactions:</p>
<ul>
<li><p><strong>Directly routed to the storage node</strong>: Non-transactional <code>PutItem</code> and other modification operations are routed directly to the storage node, bypassing the transaction coordinator.</p>
</li>
<li><p><strong>Timestamp ordering</strong>: The storage node assigns a timestamp to the write operation that is typically earlier than any prepared transactions (since those have not yet written).</p>
</li>
<li><p><strong>Exceptions</strong>: If a prepared transaction includes a condition check on the item (e.g., checking a bank account balance), the system will not allow a new write to bypass the prepared transaction. For example, if a transaction is checking that there are enough funds to withdraw $100, a new transaction cannot make a withdrawal or delete the item during that check.</p>
</li>
</ul>
</li>
<li><p><strong>Delayed Execution of Write Operations</strong></p>
<p>In certain scenarios, the system can delay write operations instead of rejecting them:</p>
<ul>
<li><p><strong>Buffering writes</strong>: If a new write operation conflicts with a prepared transaction’s conditions (e.g., by modifying the item’s state), the storage node can buffer the write operation in a queue until the prepared transaction is complete. This prevents the need to reject the write and require the client to resubmit it.</p>
</li>
<li><p><strong>Processing buffered writes after the transaction completes</strong>: Once the prepared transaction completes (committed or canceled), the buffered write can be assigned a new timestamp and executed. Typically, the delay caused by waiting for the transaction to complete is short, so this strategy doesn’t significantly increase latency.</p>
</li>
<li><p><strong>Unconditional writes</strong>: If the storage node receives a <code>PutItem</code> or <code>DeleteItem</code> operation without any preconditions, these operations can be executed immediately. They are assigned a timestamp later than any prepared transactions, ensuring the correctness of transactions. If a previously prepared transaction is committed with an earlier timestamp, its write operations will be ignored.</p>
</li>
</ul>
</li>
<li><p><strong>Write Transactions with Older Timestamps</strong></p>
<p>DynamoDB supports accepting write transactions with older timestamps:</p>
<ul>
<li><p><strong>Handling after already committed writes</strong>: If a write transaction with an older timestamp arrives at a storage node where a later write has already been processed, the node can still accept the older transaction and mark it as prepared. If the transaction is eventually committed, its write will be ignored, as the earlier write has already been overwritten by the newer one.</p>
</li>
<li><p><strong>Exceptions for partial updates</strong>: This rule applies to full overwrites of data items (like <code>PutItem</code>), but not to partial updates (like <code>UpdateItem</code>). If the last write was a partial update, the operations must be executed in strict timestamp order to ensure correctness.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple Transactions Writing to the Same Item</strong></p>
<p>DynamoDB allows multiple transactions to simultaneously prepare to write the same data item:</p>
<ul>
<li><p><strong>Simultaneous transaction preparation</strong>: For a given item, a series of transactions can enter the prepared state simultaneously, without waiting for the previous transaction to commit. This increases concurrency and allows multiple transactions to proceed in parallel.</p>
</li>
<li><p><strong>Order of transaction commits</strong>: If the write operations are full item overwrites (like <code>PutItem</code> or <code>DeleteItem</code>), the transactions can be committed in any order, as long as the last <code>PutItem</code> or <code>DeleteItem</code> operation (with the latest timestamp) is the final one executed.</p>
</li>
<li><p><strong>Restrictions for partial updates</strong>: For transactions performing partial updates (like <code>UpdateItem</code>), the transactions must be executed in timestamp order, as the final state of the item depends on the sequence of updates.</p>
</li>
</ul>
</li>
<li><p><strong>Optimized Single-Phase Read Transactions</strong></p>
<p>DynamoDB introduces optimizations for read transactions, allowing certain read transactions to be completed in a single phase without requiring a two-phase commit protocol:</p>
<ul>
<li><p><strong><code>GetItemWithTimestamp</code></strong>: Assuming storage nodes support the <code>GetItemWithTimestamp</code> operation, it allows a read timestamp to be passed as a parameter. This operation returns the latest value of the item, provided its last write timestamp is earlier than the given read timestamp and any prepared transactions have timestamps later than the read timestamp; otherwise, the request is rejected.</p>
</li>
<li><p><strong>Single-phase completion of read transactions</strong>: When a read transaction involves multiple items, the transaction coordinator issues <code>GetItemWithTimestamp</code> requests for each item and buffers the returned values. If all storage nodes accept the requests without conflict, the coordinator can return the buffered values to the client, completing the transaction. If any node rejects the request, the read transaction fails.</p>
</li>
<li><p><strong>Serialization issues</strong>: This optimization is optimistic but can lead to potential serialization issues. If a storage node later accepts a write with a timestamp earlier than a previously executed read transaction, it may cause the transaction to be non-serializable. To avoid this, storage nodes need to track both the last read and write timestamps for each item. Future write transactions must ensure that their timestamps are later than the last read&#x2F;write timestamps of all the items they modify.</p>
</li>
</ul>
</li>
<li><p><strong>Optimizations for Single-Partition Write Transactions</strong></p>
<p>DynamoDB further optimizes write transactions that involve multiple items within a single partition, allowing them to be completed in a single phase without a two-phase commit protocol:</p>
<ul>
<li><p><strong>Single-partition transaction processing</strong>: If all the items being written in a transaction reside within the same partition (and thus are stored on the same storage node), there is no need for separate prepare and commit phases. The storage node can perform all the necessary precondition checks and immediately execute the write operations.</p>
</li>
<li><p><strong>Reduced communication overhead</strong>: This approach significantly reduces the communication overhead between the transaction coordinator and storage nodes, especially in highly concurrent environments, improving system performance.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Why does DynmoDB not use the two-phase locking protocol?</strong> </p>
<p>While two-phase locking is used traditionally to prevent concurrent transactions from reading and writing the same data items, it has drawbacks. Locking <strong>restricts concurrency</strong> and can lead to <strong>deadlocks</strong>. Moreover, it requires <strong>a recovery mechanism</strong> to release locks when an application fails after acquiring locks as part of a transaction but before that transaction commits. To simplify the design and take advantage of low-contention workloads, DynamoDB uses an optimistic concurrency control scheme that avoids locking altogether.</p>
<p><strong>With DynamoDB, what is the role of a transaction coordinator?</strong></p>
<ul>
<li>The <strong>Transaction Coordinator</strong> plays a central role in handling transactions that span multiple items or storage nodes. The TC is responsible for</li>
<li><ul>
<li>breaking down the transaction into individual operations and coordinating these operations across the necessary storage nodes.</li>
<li>ensuring that the operations follow two-phase commit and all parts of the transaction are either completed successfully or rolled back.</li>
<li>assigning timestamps to ensure the correct ordering of operations and managing any potential conflicts between concurrent transactions.</li>
</ul>
</li>
</ul>
<p><strong>Is DynamoDB a relational database management system?</strong></p>
<p>No, DynamoDB is not a relational database management system (RDBMS). It is a NoSQL database, specifically a key-value and document store. Here’s how it differs from an RDBMS:</p>
<ol>
<li><strong>Data Model</strong>: DynamoDB does not use tables with fixed schemas like relational databases. Instead, it stores data as key-value pairs or documents (JSON-like structure). Each item can have different attributes, and there’s no need for predefined schemas.</li>
<li><strong>Relationships</strong>: Relational databases focus on managing relationships between data (using joins, foreign keys, etc.), while DynamoDB is optimized for storing large amounts of data without complex relationships between the data items.</li>
<li><strong>Querying</strong>: RDBMSs typically use <strong>SQL</strong> for querying data, which allows for complex joins and aggregations. DynamoDB uses its own API for querying and does not support SQL natively. While it allows querying by primary key and secondary indexes, it doesn’t support joins.</li>
<li><strong>Consistency and Transactions</strong>: DynamoDB supports <strong>eventual consistency</strong> or <strong>strong consistency</strong> for reads, while traditional relational databases typically ensure strong consistency through ACID transactions. DynamoDB has introduced <strong>transactions</strong>, but they work differently compared to those in relational databases.</li>
<li><strong>Scalability</strong>: DynamoDB is designed for horizontal scalability across distributed systems, allowing it to handle very large amounts of traffic and data by automatically partitioning data. In contrast, RDBMSs are typically vertically scaled and are not as naturally distributed.</li>
</ol>
<p><strong>How is DynamoDB’s transaction coordinator different than Gamma’s scheduler?</strong> </p>
<ul>
<li>DynamoDB’s transaction coordinator uses Optimistic Concurrency Control (OCC) to manage distributed transactions, ensuring atomicity without 2PC, focusing on scalability and performance in a globally distributed system.</li>
<li>Gamma’s scheduler, on the other hand, uses the traditional Two-Phase Locking (2PL) protocol to guarantee strong consistency in a distributed environment, prioritizing strict coordination across nodes.</li>
</ul>
<p><strong>Name one difference between FoundationDB and DynamoDB?</strong></p>
<p>FoundationDB: FoundationDB is a multi-model database that offers a core key-value store as its foundation, but it allows you to build other data models (such as documents, graphs, or relational) on top of this key-value layer. It’s highly flexible and provides transactional support for different types of data models via layers.</p>
<p>DynamoDB: DynamoDB is a NoSQL key-value and document store with a fixed data model designed specifically for highly scalable, distributed environments. It does not offer the flexibility of building different models on top of its architecture and is focused on high-performance operations with automatic scaling.</p>
<p><strong>What partitioning strategy does FoundationDB use to distribute key-value pairs across its StorageServers?</strong></p>
<p>FoundationDB uses a range-based partitioning strategy to distribute key-value pairs across its StorageServers.</p>
<p>Here’s how it works:</p>
<ol>
<li><strong>Key Ranges</strong>: FoundationDB partitions the key-value pairs by dividing the key space into <strong>contiguous ranges</strong>. Each range of keys is assigned to a specific <strong>StorageServer</strong>.</li>
<li><strong>Dynamic Splitting</strong>: The key ranges are <strong>dynamically split</strong> and adjusted based on data distribution and load. If a particular range grows too large or becomes a hotspot due to frequent access, FoundationDB will automatically split that range into smaller sub-ranges and distribute them across multiple <strong>StorageServers</strong> to balance the load.</li>
<li><strong>Data Movement</strong>: When a key range is split or needs to be rebalanced, the corresponding data is migrated from one <strong>StorageServer</strong> to another without manual intervention, ensuring even distribution of data and load across the system.</li>
</ol>
<p><strong>Why do systems such as Nova-LSM separate storage of data from its processing?</strong> </p>
<ul>
<li><strong>Independent Scaling</strong>: Storage and processing resources can scale independently to meet varying load demands.</li>
<li><strong>Resource Optimization</strong>: Storage nodes focus on data persistence and I&#x2F;O performance, while processing nodes handle computation, improving overall resource efficiency.</li>
<li><strong>Fault Tolerance</strong>: Data remains safe in storage even if processing nodes fail, ensuring high availability.</li>
</ul>
<p>Reference: </p>
<ul>
<li><a href="https://www.usenix.org/system/files/atc23-idziorek.pdf">https://www.usenix.org/system/files/atc23-idziorek.pdf</a></li>
<li><a href="https://www.usenix.org/system/files/atc22-elhemali.pdf">https://www.usenix.org/system/files/atc22-elhemali.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>BG Benchmark</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/25/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/BG%20Benchmark/</url>
    <content><![CDATA[<h1 id="Data-Model-and-Performance-Metrics"><a href="#Data-Model-and-Performance-Metrics" class="headerlink" title="Data Model and Performance Metrics"></a>Data Model and Performance Metrics</h1><ol>
<li>ER Diagram and Database Design</li>
</ol>
<ul>
<li>ER Diagram (Figure 1.a):Represents entities and relationships in the BG system.<ul>
<li>Member Entity:<ul>
<li>Represents users with a registered profile, including a unique ID and a set of adjustable-length string attributes to create records of varying sizes.</li>
<li>Each user can have up to two images:<ul>
<li>Thumbnail Image: Small (in KBs), used for displaying in friend lists.</li>
<li>High-Resolution Image: Larger (hundreds of KBs or MBs), displayed when visiting a user profile.</li>
<li>Using thumbnails significantly reduces system load compared to larger images.</li>
</ul>
</li>
</ul>
</li>
<li>Friend Relationship:<ul>
<li>Captures relationships or friend requests between users. An attribute differentiates between invitations and confirmed friendships.</li>
</ul>
</li>
<li>Resource Entity:<ul>
<li>Represents user-owned items like images, questions, or documents. Resources must belong to a user and can be posted on their profile or another user’s profile.</li>
</ul>
</li>
<li>Manipulation Relationship:<ul>
<li>Manages comments and restrictions (e.g., only friends can comment on a resource).</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li>BG Workload and SLA (Service-Level Agreement)</li>
</ol>
<ul>
<li><p>Workload: BG supports defining workloads at the granularity of:</p>
<ul>
<li>Actions: Single operations like “view profile” or “list friends.”</li>
<li>Sessions: A sequence of related actions (e.g., browsing a profile, sending a friend request).</li>
<li>Mixed Workloads: A combination of actions and sessions.</li>
</ul>
</li>
<li><p>Service-Level Agreement (SLA):</p>
<ul>
<li>Goal: Ensures the system provides reliable performance under specified conditions.</li>
<li>Example SLA Requirements: SLA, e.g., 95% of requests to observe a response time equal to or faster than 100 msec with at most 0.1% of requests observing unpredictable data for 10 minutes.</li>
</ul>
</li>
<li><p>Metrics:</p>
<ul>
<li><strong>SoAR (Social Action Rating): Measures the highest number of actions per second that meet the SLA.</strong></li>
<li><strong>Socialites: Measures the maximum number of concurrent threads that meet the SLA, reflecting the system’s multithreading capabilities.</strong></li>
</ul>
</li>
</ul>
<ol start="3">
<li>Performance Evaluation Example</li>
</ol>
<ul>
<li>SQL-X System Performance:SQL-X is a relational database with strict ACID compliance.<ul>
<li>Initially, throughput increases with more threads.</li>
<li>Beyond a certain threshold (e.g., 4 threads), request queuing causes response times to increase, reducing SLA compliance.</li>
<li>With 32 threads, 99.94% of requests exceed the 100-millisecond SLA limit, indicating significant performance degradation.</li>
</ul>
</li>
</ul>
<ol start="4">
<li>Concurrency and Optimization in BG</li>
</ol>
<ul>
<li><p><strong>Concurrency Management:</strong></p>
<ul>
<li><strong>BG prevents two threads from emulating the same user simultaneously to realistically simulate user behavior.</strong></li>
</ul>
</li>
<li><p><strong>Unpredictable Data Handling:</strong></p>
<ul>
<li><strong>Definition: Data that is stale, inconsistent, or invalid due to system limitations or race conditions.</strong></li>
<li><strong>Validation:</strong><ul>
<li><strong>BG uses offline validation to analyze read and write logs.</strong></li>
<li><strong>It determines acceptable value ranges for data and flags any reads that fall outside these ranges as unpredictable.</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>If SoAR is zero, the data store fails to meet SLA requirements, even with a single-threaded BGClient issuing requests.</p>
</blockquote>
<h1 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h1><h2 id="Performance-Analysis-of-View-Profile"><a href="#Performance-Analysis-of-View-Profile" class="headerlink" title="Performance Analysis of View Profile"></a>Performance Analysis of View Profile</h2><p>Performance of VP is influenced by whether profile images are included and their sizes.</p>
<p><strong>Experiment Setup</strong>:</p>
<ul>
<li>Profile data tested with:</li>
<li><ul>
<li><strong>No images</strong>.</li>
<li><strong>2 KB thumbnails</strong> combined with profile images of <strong>2 KB, 12 KB, and 500 KB</strong> sizes.</li>
</ul>
</li>
<li>Metrics: SoAR (Social Action Rating) measures the number of VP actions per second that meet the SLA (response time ≤ 100 ms).</li>
</ul>
<p><strong>Results</strong>:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/experiment1.png" alt="img"></p>
<ol>
<li><p><strong>No Images</strong>:</p>
<ul>
<li>MongoDB performed the best, outperforming SQL-X and CASQL by almost 2x.</li>
</ul>
</li>
<li><p><strong>12 KB Images</strong>:</p>
<ul>
<li>SQL-X’s SoAR dropped significantly, from thousands of actions per second to only hundreds.</li>
</ul>
</li>
<li><p><strong>500 KB Images</strong>:</p>
<ul>
<li><p><strong>SQL-X failed to meet the SLA (SoAR &#x3D; 0) because transmitting large images caused significant delays.</strong></p>
</li>
<li><p>MongoDB and CASQL also experienced a decrease in SoAR but performed better than SQL-X.</p>
</li>
</ul>
</li>
</ol>
<p><strong>Role of CASQL</strong>:</p>
<ul>
<li><p><strong>CASQL outperformed SQL-X due to its caching layer (memcached):</strong></p>
<ul>
<li><p>During a warm-up phase, 500,000 requests populate the cache with key-value pairs for member profiles.</p>
</li>
<li><p>Most requests are serviced by memcached instead of SQL-X, significantly improving performance with larger images (12 KB and 500 KB).</p>
</li>
</ul>
</li>
</ul>
<h2 id="Performance-Analysis-of-List-Friends"><a href="#Performance-Analysis-of-List-Friends" class="headerlink" title="Performance Analysis of List Friends"></a><strong>Performance Analysis of List Friends</strong></h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/experiment2.png" alt="img"></p>
<p><strong>1. SQL-X</strong></p>
<ul>
<li><strong>Process</strong>:<ul>
<li>Joins the <code>Friends</code> table with the <code>Members</code> table to fetch the friend list.</li>
<li>Friendship between two members is represented as a single record in the <code>Friends</code> table.</li>
</ul>
</li>
<li><strong>Performance</strong>:<ul>
<li><strong>When ϕ (number of friends) is 1000, SQL-X struggles due to the overhead of joining large tables and fails to meet SLA requirements.</strong></li>
</ul>
</li>
</ul>
<p><strong>2. CASQL</strong></p>
<ul>
<li><strong>Process</strong>:<ul>
<li>Uses a memcached caching layer to store and retrieve results of the LF action.</li>
<li>Results are cached as key-value pairs.</li>
</ul>
</li>
<li><strong>Performance</strong>:<ul>
<li>Outperforms SQL-X when ϕ is 50 or 100 by a small margin (&lt;10% improvement).</li>
<li><strong>At ϕ&#x3D;1000, memcached’s key-value size limit (1 MB) causes failures, as the data exceeds this limit.</strong></li>
<li>Adjusting memcached to support larger key-value pairs (e.g., 2 MB for 1000 friends with 2 KB thumbnails) could improve performance.</li>
</ul>
</li>
</ul>
<p><strong>3. MongoDB</strong></p>
<ul>
<li><strong>Process</strong>:<ul>
<li>Retrieves the <code>confirmedFriends</code> array from the referenced member’s document.</li>
<li>Can fetch friends’ profile documents one by one or as a batch.</li>
</ul>
</li>
<li><strong>Performance</strong>:<ul>
<li>Performs no joins, but its SLA compliance is poor for larger friend counts.</li>
<li>SoAR is zero for ϕ&#x3D;50,100,1000, as it fails to meet the 100 ms response time requirement.</li>
<li>For smaller friend lists (ϕ&#x3D;10), MongoDB achieves a SoAR of 6 actions per second.</li>
</ul>
</li>
</ul>
<h2 id="Mix-of-Read-and-Write-Actions"><a href="#Mix-of-Read-and-Write-Actions" class="headerlink" title="Mix of Read and Write Actions"></a><strong>Mix of Read and Write Actions</strong></h2><ul>
<li><strong>Purpose</strong>: Evaluates the performance of data stores under different ratios of read and write operations.</li>
<li><strong>Categories</strong>:<ul>
<li><strong>Read actions</strong>: Include operations like View Profile (VP), List Friends (LF), and View Friend Requests (VFR).</li>
<li><strong>Write actions</strong>: Modify friendship relationships and invalidate cached key-value pairs (e.g., Invite Friend, Accept Friend Request).</li>
</ul>
</li>
<li><strong>Mix Variations</strong>:<ul>
<li><strong>Very low writes (0.1%)</strong>: Dominantly read-heavy workloads.</li>
<li><strong>Low writes (1%)</strong>: Slightly higher frequency of write actions.</li>
<li><strong>High writes (10%)</strong>: Write-intensive workloads.</li>
</ul>
</li>
</ul>
<p><strong>Performance Analysis (Mix of Read and Write Actions)</strong></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/experiment3.png" alt="img"></p>
<ul>
<li><strong>SoAR Comparison</strong>:<ul>
<li><strong>CASQL</strong> consistently achieves the highest SoAR for all write mixes due to its caching mechanism.</li>
<li><strong>MongoDB</strong> outperforms <strong>SQL-X</strong> by a factor of 3 across all workloads.</li>
</ul>
</li>
</ul>
<p><strong>Observations by Write Percentage:</strong></p>
<ol>
<li><p><strong>0.1% Writes (Read-Dominant)</strong>:</p>
<ul>
<li><p>CASQL significantly outperforms MongoDB due to efficient use of cached key-value pairs.</p>
</li>
<li><p>SQL-X lags due to the overhead of processing read actions directly from the RDBMS.</p>
</li>
</ul>
</li>
<li><p><strong>1% Writes</strong>:</p>
<ul>
<li><p>CASQL remains the best performer but shows sensitivity to increasing writes as it invalidates cached data, redirecting more queries to the RDBMS.</p>
</li>
<li><p>MongoDB maintains a consistent performance advantage over SQL-X.</p>
</li>
</ul>
</li>
<li><p><strong>10% Writes (Write-Heavy)</strong>:</p>
<ul>
<li><p><strong>CASQL slightly outperforms MongoDB, but the gap narrows due to the higher frequency of cache invalidations.</strong></p>
</li>
<li><p>SQL-X continues to struggle with write-heavy workloads due to its lack of caching.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h1><blockquote>
<p>Definition: A session is a sequence of actions performed by a socialite (user) in the social network.</p>
<p>Key Concepts:</p>
<ol>
<li>Think Time: Delay between consecutive actions within a session.</li>
<li>Inter-Arrival Time: Delay between sessions initiated by different socialites.</li>
</ol>
</blockquote>
<p><strong>Key Considerations</strong></p>
<ol>
<li><p><strong>Dependencies</strong>:</p>
<ul>
<li><p>Some sessions rely on specific database states (e.g., friends or pending requests).</p>
</li>
<li><p>For example, if m_i has no friends or pending requests, certain sessions terminate early.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Handling</strong>:</p>
<ul>
<li><p>BG uses in-memory data structures to simulate database states and prevent conflicts (e.g., multiple threads deleting the same comment).</p>
</li>
<li><p>Ensures integrity by managing semaphores and detecting unpredictable data.</p>
</li>
</ul>
</li>
<li><p><strong>Extensibility</strong>:</p>
<ul>
<li>BG allows developers to define new sessions by combining different mixes of actions.</li>
</ul>
</li>
</ol>
<h1 id="Parallelism"><a href="#Parallelism" class="headerlink" title="Parallelism"></a>Parallelism</h1><h2 id="BG’s-Scalable-Benchmarking-Framework"><a href="#BG’s-Scalable-Benchmarking-Framework" class="headerlink" title="BG’s Scalable Benchmarking Framework"></a>BG’s Scalable Benchmarking Framework</h2><p>To address these limitations, BG employs <strong>a shared-nothing architecture</strong> with the following components:</p>
<p><strong>1. BGCoord (Coordinator)</strong></p>
<ul>
<li><strong>Role</strong>: Oversees and coordinates the benchmarking process.</li>
<li><strong>Responsibilities</strong>:<ul>
<li><strong>Computes SoAR and Socialites ratings.</strong></li>
<li><strong>Assigns workloads to BGClients and monitors their progress.</strong></li>
<li><strong>Aggregates results (e.g., response times, throughput) for visualization.</strong></li>
</ul>
</li>
<li><strong>Process</strong>:<ul>
<li><strong>Splits the workload among N BGClients.</strong></li>
<li><strong>Ensures each BGClient works independently to prevent resource contention.</strong></li>
</ul>
</li>
</ul>
<p><strong>2. BGClient</strong></p>
<ul>
<li><strong>Role</strong>: Executes tasks assigned by BGCoord.</li>
<li><strong>Responsibilities</strong>:<ul>
<li><strong>Creates a database based on BG specifications.</strong></li>
<li><strong>Simulates workload actions and computes metrics like unpredictable data volume.</strong></li>
<li><strong>Periodically reports metrics to BGCoord for aggregation.</strong></li>
</ul>
</li>
</ul>
<p><strong>3. Visualization Deck</strong></p>
<ul>
<li><strong>Role</strong>: Provides a user interface for monitoring and controlling the benchmarking process.</li>
<li><strong>Features</strong>:<ul>
<li>Allows users to configure parameters (e.g., SLA, workloads).</li>
<li>Visualizes the ratings (SoAR, Socialites) and progress of the benchmarking.</li>
</ul>
</li>
</ul>
<p><strong>Scaling with BGClients</strong></p>
<ul>
<li><strong>Fragmentation</strong>:<ul>
<li><strong>The database is split into N logical fragments, each assigned to a BGClient.</strong></li>
<li><strong>Each fragment includes unique members, friendships, and resources, ensuring no overlap between BGClients.</strong></li>
</ul>
</li>
<li><strong>Decentralized D-Zipfian Distribution</strong>:<ul>
<li><strong>Used to balance workloads across nodes with different processing speeds.</strong></li>
<li><strong>Faster nodes handle larger fragments, ensuring equal workload completion times.</strong></li>
</ul>
</li>
</ul>
<h1 id="Unpredictable-Data"><a href="#Unpredictable-Data" class="headerlink" title="Unpredictable Data"></a>Unpredictable Data</h1><p><strong>Definition</strong>: <strong>Data that is stale, inconsistent, or invalid, produced due to race conditions, dirty reads, or eventual consistency.</strong></p>
<h2 id="BG’s-Validation-Process"><a href="#BG’s-Validation-Process" class="headerlink" title="BG’s Validation Process"></a>BG’s Validation Process</h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart1.png" alt="img"></p>
<h2 id="Validation-Implementation"><a href="#Validation-Implementation" class="headerlink" title="Validation Implementation"></a>Validation Implementation</h2><ol>
<li><strong>Log Generation</strong>:<ul>
<li><strong>BG generates read log records (observed values) and write log records (new or delta values).</strong></li>
</ul>
</li>
<li><strong>Offline Validation</strong>:<ul>
<li><strong>For each read log entry:</strong><ul>
<li><strong>BG computes a range of valid values using overlapping write logs.</strong></li>
<li><strong>If the observed value is outside this range, it is flagged as unpredictable.</strong></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Impact-of-Time-to-Live-TTL-on-Unpredictable-Data"><a href="#Impact-of-Time-to-Live-TTL-on-Unpredictable-Data" class="headerlink" title="Impact of Time-to-Live (TTL) on Unpredictable Data"></a>Impact of Time-to-Live (TTL) on Unpredictable Data</h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart2.png" alt="img"></p>
<p><strong>Results</strong>:</p>
<ol>
<li><p><strong>Higher TTL Increases Stale Data</strong>:</p>
<ul>
<li><p>A higher TTL (e.g., 120 seconds) results in more stale key-value pairs, increasing the percentage of unpredictable data.</p>
</li>
<li><p>For T&#x3D;100T &#x3D; 100T&#x3D;100, unpredictable data is:</p>
<ul>
<li>~79.8% with TTL &#x3D; 30 seconds.</li>
<li>~98.15% with TTL &#x3D; 120 seconds.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Performance Trade-off</strong>:</p>
<ul>
<li><p><strong>A higher TTL improves performance (fewer cache invalidations) but increases stale data.</strong></p>
</li>
<li><p><strong>Lower TTL reduces stale data but impacts cache performance.</strong></p>
</li>
</ul>
</li>
</ol>
<h1 id="Heuristic-Search-for-Rating"><a href="#Heuristic-Search-for-Rating" class="headerlink" title="Heuristic Search for Rating"></a>Heuristic Search for Rating</h1><p><strong>Why Use Heuristic Search?</strong></p>
<ul>
<li>Exhaustive search starting from T&#x3D;1 to the maximum T is time-consuming.</li>
<li>MongoDB with T&#x3D;1000 and Δ&#x3D;10 minutes would take 7 days for exhaustive testing.</li>
</ul>
<p><strong>Steps in Heuristic Search</strong>:</p>
<ol>
<li><p><strong>Doubling Strategy</strong>:</p>
<ul>
<li><p><strong>Start with T&#x3D;1, double T after each successful experiment.</strong></p>
</li>
<li><p><strong>Stop when SLA fails, narrowing down T to an interval.</strong></p>
</li>
</ul>
</li>
<li><p><strong>Binary Search</strong>:</p>
<ul>
<li><p><strong>Identify the T corresponding to max throughput within the interval.</strong></p>
</li>
<li><p><strong>Used for both SoAR (peak throughput) and Socialites (maximum concurrent threads).</strong></p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>What system metrics does BG quantify?</strong></p>
<p><strong>SoAR (Social Action Rating):</strong></p>
<ul>
<li>The highest throughput (actions per second) that satisfies a given SLA, ensuring at least α% of requests meet the response time β, with at most τ% of requests observing unpredictable data.</li>
</ul>
<p><strong>Socialites Rating:</strong></p>
<ul>
<li>The maximum number of simultaneous threads (or users) that a data store can support while still meeting the SLA requirements.</li>
</ul>
<p><strong>Throughput</strong>:</p>
<ul>
<li>Total number of completed actions per unit of time.</li>
</ul>
<p><strong>Response Time:</strong></p>
<ul>
<li>Average or percentile-based latency for each action.</li>
</ul>
<p><strong>Unpredictable Data:</strong></p>
<ul>
<li>The percentage of actions that observe stale, inconsistent, or invalid data during execution.</li>
</ul>
<p><strong>How does BG scale to generate a large number of requests?</strong></p>
<p>BG employs <strong>a shared-nothing architecture</strong> with the following mechanisms to scale effectively:</p>
<ol>
<li><p><strong>Partitioning Members and Resources:</strong></p>
<ul>
<li><p>BGCoord <strong>partitions</strong> the database into <strong>logical fragments</strong>, each containing <strong>a unique subset</strong> of members, their resources, and relationships.</p>
</li>
<li><p>These fragments are assigned to individual BGClients.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple BGClients:</strong></p>
<ul>
<li><p>Each BGClient operates <strong>independently</strong>, generating workloads for its assigned logical fragment.</p>
</li>
<li><p>By running <strong>multiple</strong> BGClients <strong>in parallel</strong> across different nodes, BG can scale horizontally to handle millions of requests.</p>
</li>
</ul>
</li>
<li><p><strong>D-Zipfian Distribution:</strong></p>
<ul>
<li><p>To ensure realistic and scalable workloads, BG uses a decentralized Zipfian distribution (D-Zipfian) that <strong>dynamically assigns</strong> requests to BGClients based on node performance.</p>
</li>
<li><p>Faster nodes receive a larger share of the logical fragments, ensuring even workload distribution.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Control:</strong></p>
<ul>
<li>BG <strong>prevents simultaneous threads from issuing actions for the same user</strong>, maintaining the integrity of modeled user interactions and avoiding resource contention.</li>
</ul>
</li>
</ol>
<p><strong>If two modeled users, A and B, are already friends, does BG generate a friend request from A to B?</strong></p>
<p>No, BG does not generate a friend request from A to B if they are already friends.</p>
<p>Before generating a friend request, BG <strong>validates</strong> whether the relationship between A and B is pending or already confirmed. For example, in the <code>InviteFrdSession</code>, BG only selects users who have no existing “friend” or “pending” relationship with the requester to receive a new friend request.</p>
<p>Reference: <a href="https://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper93.pdf">https://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper93.pdf</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>FoundationDB</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/FoundationDB/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>FoundationDB的研究意义在于，它成功地将NoSQL的灵活性与ACID事务的强大功能结合在一起，提供了一种模块化的架构，使得各个子系统可以独立配置和扩展。这种设计不仅提高了系统的可扩展性和可用性，还增强了故障容忍能力。此外，FoundationDB采用了严格的模拟测试框架，确保了系统的稳定性和高效性，使得开发者能够快速引入和发布新特性。FoundationDB的快速恢复机制显著提高了系统的可用性，简化了软件升级和配置变更的过程，通常在几秒钟内完成。</p>
<p>The main design principles are:</p>
<ol>
<li>Divide-and-Conquer (or separation of concerns). FDB decouples the transaction management system (write path) from the distributed storage (read path) and scales them independently. Within the transaction management system, processes are assigned various roles representing different aspects of transaction management. Furthermore, cluster-wide orchestrating tasks, such as overload control and load balancing are also divided and serviced by additional heterogeneous roles.</li>
<li>Make failure a common case. For distributed systems, failure is a norm rather than an exception. To cope with failures in the transaction management system of FDB, we handle all failures through the recovery path: the transaction system proactively shuts down when it detects a failure. Thus, all failure handling is reduced to a single recovery operation, which becomes a common and well-tested code path. To improve availability, FDB strives to minimize Mean-Time-To-Recovery (MTTR). In our production clusters, the total time is usually less than five seconds.</li>
<li>Simulation testing. FDB relies on a randomized, deterministic simulation framework for testing the correctness of its distributed database. Simulation tests not only expose deep bugs, but also boost developer productivity and the code quality of FDB.</li>
</ol>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/FDB_arch.png" alt="img"></p>
<ul>
<li><p>The control plane is responsible for persisting critical system metadata, that is, the configuration of transaction systems, on Coordinators.</p>
<ul>
<li><p>These <strong>Coordinators</strong> form a Paxos group and elect a ClusterController.</p>
</li>
<li><p>The <strong>ClusterController</strong> monitors all servers in the cluster and recruits three processes, Sequencer, DataDistributor, and Ratekeeper, which are re-recruited if they fail or crash.</p>
</li>
<li><p>The <strong>DataDistributor</strong> is responsible for monitoring failures and balancing data among StorageServers.</p>
</li>
<li><p><strong>Ratekeeper</strong> provides overload protection for the cluster.</p>
</li>
</ul>
</li>
<li><p>The data plane is responsible for transaction processing and data storage. FDB chooses an unbundled architecture:</p>
<ul>
<li><p>A distributed transaction management system (TS) consists of a Sequencer, Proxies, and Resolvers, all of which are stateless processes.</p>
<ul>
<li><p>The Sequencer assigns a read and a commit version to each transaction.</p>
</li>
<li><p>Proxies offer MVCC read versions to clients and orchestrate transaction commits.</p>
</li>
<li><p>Resolvers check for conflicts among transactions.</p>
</li>
</ul>
</li>
<li><p>A log system (LS) stores Write-Ahead-Log (WAL) for TS, and a separate distributed storage system (SS) is used for storing data and servicing reads. The LS contains a set of LogServers and the SS has a number of StorageServers. LogServers act as replicated, sharded, distributed persistent queues, each queue storing WAL data for a StorageServer.</p>
</li>
</ul>
</li>
</ul>
<p><strong>Clients read from sharded StorageServers, so reads scale linearly with the number of StorageServers.</strong></p>
<p><strong>Writes are scaled by adding more Proxies, Resolvers, and LogServers.</strong></p>
<p>The control plane’s singleton processes (e.g., ClusterController and Sequencer) and Coordinators are not performance bottlenecks; they only perform limited metadata operations. 因为元数据操作少且简单，且与两者无关的数据读写是并行扩展的（如上面两行加粗字体所述）。</p>
<h1 id="Bootstrapping"><a href="#Bootstrapping" class="headerlink" title="Bootstrapping"></a>Bootstrapping</h1><p>FDB has no dependency on external coordination services. All user data and most system metadata (keys that start with 0xFF prefix) are stored in StorageServers. The metadata about StorageServers is persisted in LogServers, and the LogServers configuration data is stored in all Coordinators.</p>
<ol>
<li>The Coordinators are a disk Paxos group; servers attempt to become the ClusterController if one does not exist.</li>
<li>A newly elected ClusterController reads the old LS configuration from the Coordinators and spawns a new TS and LS.</li>
<li>Proxies recover system metadata from the old LS, including information about all StorageServers.</li>
<li>The Sequencer waits until the new TS finishes recovery, then writes the new LS configuration to all Coordinators. The new transaction system is then ready to accept client transactions.</li>
</ol>
<h1 id="Reconfiguration"><a href="#Reconfiguration" class="headerlink" title="Reconfiguration"></a>Reconfiguration</h1><p>The Sequencer process monitors the health of Proxies, Resolvers, and LogServers. Whenever there is a failure in the TS or LS, or the database configuration changes, the Sequencer terminates. The ClusterController detects the Sequencer failure, then recruits and bootstraps a new TS and LS. In this way, transaction processing is divided into epochs, where each epoch represents a generation of the transaction management system with its own Sequencer.</p>
<h1 id="End-to-end-transaction-processing"><a href="#End-to-end-transaction-processing" class="headerlink" title="End-to-end transaction processing"></a>End-to-end transaction processing</h1><ol>
<li><p><strong>Transaction Start and Read Operations:</strong></p>
<ul>
<li><p>A client starts a transaction by contacting a <strong>Proxy</strong> to obtain a read version (timestamp).</p>
</li>
<li><p>The <strong>Proxy</strong> requests a read version from the <strong>Sequencer</strong> that is greater than all previously issued commit versions and sends it to the client.</p>
</li>
<li><p>The client then reads from <strong>StorageServers</strong> at this specific read version.</p>
</li>
</ul>
</li>
<li><p><strong>Buffered Write Operations</strong>:</p>
<ul>
<li><p>Client writes are buffered locally and not sent to the cluster immediately.</p>
</li>
<li><p>Read-your-write semantics are preserved by combining the database lookups with the client’s uncommitted writes.</p>
</li>
</ul>
</li>
<li><p><strong>Transaction Commit</strong>:</p>
<ul>
<li><p>When the client commits, it sends the transaction data (read and write sets) to a <strong>Proxy</strong>, waiting for either a commit or abort response.</p>
</li>
<li><p>The <strong>Proxy</strong> commits a transaction in three steps:</p>
<ol>
<li><p><strong>Obtain Commit Version</strong>: The Proxy requests a commit version from the <strong>Sequencer</strong> that is larger than all current read or commit versions.</p>
</li>
<li><p><strong>Conflict Check</strong>: The Proxy sends transaction data to the partitioned <strong>Resolvers</strong>, which check for read-write conflicts. If no conflicts are found, the transaction proceeds; otherwise, it is aborted.</p>
</li>
<li><p><strong>Persist to Log Servers</strong>: The transaction is sent to <strong>LogServers</strong> for persistence, and after all LogServers acknowledge, the transaction is considered committed. The Proxy then reports the committed version to the <strong>Sequencer</strong> and sends the response back to the client.</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Applying Writes</strong>:</p>
<ul>
<li><strong>StorageServers</strong> continuously pull mutation logs from <strong>LogServers</strong> and apply the committed changes to disk.</li>
</ul>
</li>
<li><p><strong>Read-Only Transactions and Snapshot Reads</strong>:</p>
<ul>
<li><p>Read-only transactions are <strong>serializable</strong> (at the read version) and <strong>high-performance</strong> (thanks to MVCC), allowing the client to commit locally without contacting the database, which is particularly important since most transactions are read-only.</p>
</li>
<li><p><strong>Snapshot reads</strong> relax the isolation property of a transaction, reducing conflicts by allowing concurrent writes without conflicting with snapshot reads.</p>
</li>
</ul>
</li>
</ol>
<p><strong>FoundationDB (FDB) using Serializable Snapshot Isolation (SSI) by combining Optimistic Concurrency Control (OCC) with Multi-Version Concurrency Control (MVCC).</strong></p>
<h2 id="Transaction-Versions"><a href="#Transaction-Versions" class="headerlink" title="Transaction Versions"></a>Transaction Versions</h2><ul>
<li>Each transaction receives a <strong>read version</strong> and a <strong>commit version</strong> from the <strong>Sequencer</strong>.</li>
<li>The read version ensures that the transaction observes the results of all previously committed transactions, and the commit version is greater than all current read or commit versions, establishing a serial order for transactions.</li>
</ul>
<h2 id="Log-Sequence-Number-LSN"><a href="#Log-Sequence-Number-LSN" class="headerlink" title="Log Sequence Number (LSN)"></a>Log Sequence Number (LSN)</h2><ul>
<li>The <strong>commit version</strong> serves as the <strong>LSN</strong>, defining a serial history of transactions.</li>
<li>To ensure no gaps between LSNs, the Sequencer also returns the previous LSN with each commit. Both the LSN and previous LSN are sent to <strong>Resolvers</strong> and <strong>LogServers</strong> to enforce serial processing of transactions.</li>
</ul>
<h2 id="Conflict-Detection"><a href="#Conflict-Detection" class="headerlink" title="Conflict Detection"></a>Conflict Detection</h2><ul>
<li>FDB uses a lock-free conflict detection algorithm similar to <strong>write-snapshot isolation</strong>, but the commit version is chosen before conflict detection, enabling efficient batch processing of version assignments and conflict detection.</li>
<li>The key space is divided among multiple <strong>Resolvers</strong>, allowing conflict detection to be parallelized. A transaction can commit only if all Resolvers confirm no conflicts.</li>
</ul>
<h2 id="Handling-Aborted-Transactions"><a href="#Handling-Aborted-Transactions" class="headerlink" title="Handling Aborted Transactions"></a>Handling Aborted Transactions</h2><ul>
<li>If a transaction is aborted, some Resolvers may have already updated their history, leading to possible “false positive” conflicts for other transactions. However, this is rare because most transactions’ key ranges fall within one Resolver, and the effects of false positives are limited to a short MVCC window (5 seconds).</li>
</ul>
<h2 id="Efficiency-of-OCC"><a href="#Efficiency-of-OCC" class="headerlink" title="Efficiency of OCC"></a>Efficiency of OCC</h2><ul>
<li>The OCC design avoids the complexity of acquiring and releasing locks, simplifying interactions between the <strong>Transaction System (TS)</strong> and <strong>Storage Servers (SS)</strong>.</li>
<li>While OCC may result in some wasted work due to aborted transactions, FDB’s conflict rate in production is low (less than 1%), and clients can simply restart aborted transactions.</li>
</ul>
<h1 id="Logging-protocol"><a href="#Logging-protocol" class="headerlink" title="Logging protocol"></a>Logging protocol</h1><p>Commit Logging:</p>
<ul>
<li>Once a <strong>Proxy</strong> decides to commit a transaction, it sends the transaction’s changes (mutations) to the <strong>LogServers</strong> responsible for the modified key ranges. Other LogServers receive an empty message.</li>
<li>The log message includes the current and previous <strong>Log Sequence Number (LSN)</strong> from the <strong>Sequencer</strong> and the largest known committed version (KCV) of the Proxy.</li>
<li>The <strong>LogServers</strong> reply to the Proxy once the log data is durably stored. The Proxy updates its KCV if all replica LogServers acknowledge and the LSN is larger than the current KCV.</li>
</ul>
<p>Shipping Redo Logs:</p>
<ul>
<li>Shipping the redo log from LogServers to <strong>StorageServers</strong> happens in the background and is not part of the commit path, improving performance.</li>
</ul>
<p>Applying Redo Logs:</p>
<ul>
<li><strong>StorageServers</strong> apply non-durable redo logs from LogServers to an in-memory index. In most cases, this happens before any client reads are processed, ensuring low-latency multi-version reads.</li>
<li>If the requested data is not yet available on a StorageServer, the client either waits or retries at another replica. If both reads time out, the client can restart the transaction.</li>
</ul>
<p>I&#x2F;O Efficiency:</p>
<ul>
<li>Since log data is already durable on LogServers, StorageServers can buffer updates in memory and write batches to disk periodically, improving input&#x2F;output (I&#x2F;O) efficiency.</li>
</ul>
<p><strong>What if a StorageServer is lagging behind on applying the redo logs and a client requests a version of a key pair it does not have?</strong></p>
<ol>
<li>Wait for a threshold for when known-committed-version is greater than or equal to the read version</li>
<li>If timeout, the client asks another StorageServer that stores the key</li>
<li>Return error “request for a future version” (FDB error code 1009)</li>
</ol>
<p><strong>What if there is no further transaction logs to redo?</strong></p>
<ul>
<li>Without new transactions issued from the client, proxies still generate empty transactions to advance the known-committed-version</li>
<li>Known-committed-version and LSN of each transaction are sent to all LogServers (limit scalability on writes)</li>
</ul>
<h1 id="Transaction-system-recovery"><a href="#Transaction-system-recovery" class="headerlink" title="Transaction system recovery"></a>Transaction system recovery</h1><h2 id="Simplified-Recovery"><a href="#Simplified-Recovery" class="headerlink" title="Simplified Recovery"></a>Simplified Recovery</h2><ul>
<li>Unlike traditional databases that require <strong>undo log processing</strong>, FoundationDB avoids this step by making the <strong>redo log processing</strong> the same as the normal log forward path. StorageServers pull logs from LogServers and apply them in the background.</li>
</ul>
<h2 id="Failure-Detection-and-New-Transaction-System-TS"><a href="#Failure-Detection-and-New-Transaction-System-TS" class="headerlink" title="Failure Detection and New Transaction System (TS)"></a>Failure Detection and New Transaction System (TS)</h2><ul>
<li>Upon detecting a failure, a new TS is recruited. The new TS can start accepting transactions even before all old logs are fully processed. Recovery focuses on finding the end of the redo log, allowing StorageServers to asynchronously replay the logs from that point.</li>
</ul>
<h2 id="Epoch-based-Recovery"><a href="#Epoch-based-Recovery" class="headerlink" title="Epoch-based Recovery"></a>Epoch-based Recovery</h2><ul>
<li>The recovery process is handled per <strong>epoch</strong>. The <strong>ClusterController</strong> locks the old TS configuration, stops old LogServers from accepting new transactions, recruits a new set of transaction components (Sequencer, Proxies, Resolvers, and LogServers), and writes the new TS configuration to the <strong>Coordinators</strong>.</li>
<li>Stateless components like Proxies and Resolvers don’t require special recovery, but LogServers, which store committed transaction logs, must ensure all data is durable and retrievable by StorageServers.</li>
</ul>
<h2 id="Recovery-Version-RV"><a href="#Recovery-Version-RV" class="headerlink" title="Recovery Version (RV)"></a>Recovery Version (RV)</h2><ul>
<li>The recovery focuses on determining the <strong>Recovery Version (RV)</strong>, which is essentially the end of the redo log. The <strong>Sequencer</strong> collects data from the old LogServers, specifically the <strong>Durable Version (DV)</strong> (maximum LSN persisted) and <strong>KCV</strong> (maximum committed version) from each.</li>
<li>Once enough LogServers have responded, the <strong>Previous Epoch Version (PEV)</strong> is established (the maximum of all KCVs). The start version of the new epoch is <code>PEV + 1</code>, and the minimum DV becomes the <strong>RV</strong>.</li>
</ul>
<h2 id="Log-Copying-and-Healing"><a href="#Log-Copying-and-Healing" class="headerlink" title="Log Copying and Healing"></a>Log Copying and Healing</h2><ul>
<li>Logs between <code>PEV + 1</code> and RV are copied from old LogServers to the new ones to restore replication in case of LogServer failures. This copying process is lightweight since it only covers a few seconds of logs.</li>
</ul>
<h2 id="Rollback-and-Transaction-Processing"><a href="#Rollback-and-Transaction-Processing" class="headerlink" title="Rollback and Transaction Processing"></a>Rollback and Transaction Processing</h2><ul>
<li>The first transaction after recovery is a special <strong>recovery transaction</strong> that informs StorageServers of the RV, so they can discard in-memory multi-versioned data beyond the RV. StorageServers then pull data larger than the PEV from the new LogServers.</li>
<li>The rollback process simply discards in-memory multi-versioned data, as persistent data is only written to disk once it leaves the MVCC window.</li>
</ul>
<h1 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h1><ol>
<li><p><strong>Metadata Replication</strong>:</p>
<ul>
<li><strong>System metadata</strong> related to the control plane is stored on <strong>Coordinators</strong> using the <strong>Active Disk Paxos</strong> protocol. As long as a majority (quorum) of Coordinators are operational, the metadata can be recovered in case of failure.</li>
</ul>
</li>
<li><p><strong>Log Replication</strong>:</p>
<ul>
<li>When a <strong>Proxy</strong> writes logs to <strong>LogServers</strong>, each log record is replicated synchronously across <strong>k &#x3D; f + 1</strong> LogServers (where <strong>f</strong> is the number of allowed failures). The Proxy only sends a commit response to the client after all <strong>k</strong> LogServers have successfully persisted the log. If a LogServer fails, a transaction system recovery is triggered.</li>
</ul>
</li>
<li><p><strong>Storage Replication</strong>:</p>
<ul>
<li>Each <strong>key range (shard)</strong> is asynchronously replicated across <strong>k &#x3D; f + 1 StorageServers</strong>. These StorageServers form a <strong>team</strong>. A StorageServer typically hosts multiple shards, distributing its data across several teams. If a StorageServer fails, the <strong>DataDistributor</strong> moves the data from teams with the failed server to other healthy teams.</li>
</ul>
</li>
</ol>
<p>To prevent data loss in case of simultaneous failures, FoundationDB ensures that no more than one process in a replica group is placed within the same fault domain (e.g., a host, rack, or availability zone). As long as one process in each team is operational, no data is lost, provided at least one fault domain remains available.</p>
<h1 id="Simulation-testing"><a href="#Simulation-testing" class="headerlink" title="Simulation testing"></a>Simulation testing</h1><ol>
<li><p><strong>Deterministic Simulation</strong>:</p>
<ul>
<li><p>FoundationDB uses <strong>deterministic discrete-event simulation</strong> to test its distributed system. This simulation runs real database code along with <strong>randomized synthetic workloads</strong> and <strong>fault injection</strong> to uncover bugs.</p>
</li>
<li><p>Determinism ensures that bugs are reproducible and can be investigated thoroughly.</p>
</li>
</ul>
</li>
<li><p><strong>Fault Injection</strong>:</p>
<ul>
<li><p>The simulation tests system resilience by injecting various faults, such as <strong>machine, rack, or data center failures</strong>, network issues, disk corruption, and delays.</p>
</li>
<li><p>Randomization of these faults increases the diversity of tested states, allowing for a wide range of potential issues to be examined.</p>
</li>
<li><p><strong>“Buggification”</strong> is a technique used to deliberately introduce rare or unusual behaviors (e.g., unnecessary delays, errors) in the system to stress-test its handling of non-standard conditions.</p>
</li>
</ul>
</li>
<li><p><strong>Swarm Testing</strong>:</p>
<ul>
<li><p><strong>Swarm testing</strong> increases simulation diversity by using random cluster sizes, configurations, workloads, and fault injection parameters.</p>
</li>
<li><p>This ensures that a broad range of scenarios is covered in testing, allowing for the discovery of rare bugs.</p>
</li>
</ul>
</li>
<li><p><strong>Test Oracles</strong>:</p>
<ul>
<li><p><strong>Test oracles</strong> are built into the system to verify key properties like <strong>transaction atomicity</strong>, <strong>isolation</strong>, and <strong>recoverability</strong>. Assertions check these properties to detect failures during simulation.</p>
</li>
<li><p>They help confirm that the system’s expected behaviors are maintained, even under stressful conditions.</p>
</li>
</ul>
</li>
<li><p><strong>Bug Detection Efficiency</strong>:</p>
<ul>
<li><p>The simulation runs faster than real-time, allowing FoundationDB to quickly discover and trace bugs. The <strong>parallel</strong> nature of testing accelerates the process of finding bugs, particularly before major releases.</p>
</li>
<li><p>This approach uncovers bugs that may not appear during real-time testing, especially for issues that require long-running operations.</p>
</li>
</ul>
</li>
<li><p><strong>Limitations</strong>:</p>
<ul>
<li><p>Simulation cannot reliably detect <strong>performance issues</strong> (like imperfect load balancing).</p>
</li>
<li><p>It cannot test <strong>third-party libraries</strong> or <strong>external dependencies</strong>, focusing mainly on FoundationDB’s internal code and behaviors.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Lessons-learned"><a href="#Lessons-learned" class="headerlink" title="Lessons learned"></a>Lessons learned</h1><ol>
<li><p><strong>Architecture Design</strong></p>
<ul>
<li><p><strong>Divide-and-Conquer Principle</strong>: Separating the transaction system from the storage layer allows for independent scaling and deployment of resources, enhancing both flexibility and performance.</p>
</li>
<li><p><strong>LogServers as Witness Replicas</strong>: In multi-region deployments, LogServers reduce the need for full StorageServer replicas while maintaining high availability.</p>
</li>
<li><p><strong>Role Specialization</strong>: The design enables the creation of specialized roles, like separating DataDistributor and Ratekeeper from the Sequencer, and separating Proxies into Get-Read-Version and Commit Proxies, which improves performance and makes the system extensible.</p>
</li>
<li><p><strong>Decoupling Enhances Extensibility</strong>: This design pattern allows features like replacing SQLite with RocksDB and adding new roles or functions without overhauling the entire system.</p>
</li>
</ul>
</li>
<li><p><strong>Simulation Testing</strong></p>
<ul>
<li><p><strong>High Productivity</strong>: FDB’s deterministic simulation testing enables bugs to be found and reproduced quickly. This approach has improved developer productivity and system reliability by reducing debugging time and improving test coverage.</p>
</li>
<li><p><strong>Reliability</strong>: FDB has operated without any data corruption over several years of deployment (e.g., CloudKit), thanks to rigorous simulation testing. Simulation has allowed ambitious rewrites and improvements to be made safely.</p>
</li>
<li><p><strong>Eliminating Dependencies</strong>: Simulation testing helped find bugs in external dependencies, leading to FDB replacing Apache Zookeeper with its own Paxos implementation. This change resulted in no further production bugs.</p>
</li>
</ul>
</li>
<li><p><strong>Fast Recovery</strong></p>
<ul>
<li><p><strong>Simplifies Upgrades</strong>: FDB allows fast recovery by restarting all processes simultaneously, typically within seconds, simplifying software upgrades and configuration changes. This method has been extensively tested and used in Apple’s production clusters.</p>
</li>
<li><p><strong>Bug Healing</strong>: Fast recovery can automatically resolve certain latent bugs, similar to software rejuvenation, by resetting system states.</p>
</li>
</ul>
</li>
<li><p><strong>5-Second MVCC Window</strong></p>
<ul>
<li><p><strong>Memory Efficiency</strong>: FDB uses a 5-second MVCC (Multi-Version Concurrency Control) window to limit memory usage in transaction systems and storage servers. This time window is long enough for most OLTP workloads, exposing inefficiencies if the transaction exceeds 5 seconds.</p>
</li>
<li><p><strong>TaskBucket Abstraction</strong>: Long-running processes, like backups, are broken into smaller transactions that fit within the 5-second window. FDB implements this through an abstraction called TaskBucket, which simplifies splitting large transactions into manageable jobs.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>With FDB, what operations does a transaction commit perform when the transaction only reads the value of data items?</strong></p>
<ul>
<li><strong>Read Version Retrieval</strong>: The client requests a read version from a <strong>Proxy</strong> via the <strong>Sequencer</strong>, which guarantees the read version is greater than or equal to any committed version.</li>
<li><strong>Read Operation</strong>: The client reads the requested data at this specific read version from the <strong>StorageServers</strong>. The reads are served by the StorageServers, which are guaranteed to provide data consistent with the requested version.</li>
<li><strong>No Writes or Conflicts</strong>: Since the transaction is read-only, there is no write set or conflicts to check. The transaction simply ends, and no data is written or modified, meaning it does not interact with LogServers or commit any changes.</li>
<li><strong>Commit</strong>: Even though no actual commit occurs (because there’s no data change), the transaction is marked as successfully completed after the reads are done.</li>
</ul>
<p><strong>With FDB, is it possible for multiple resolvers to participate in the decision whether to commit or abort a write transaction?</strong> </p>
<p>Yes, multiple Resolvers can participate in the decision to commit or abort a write transaction in FDB. Here’s how it works:</p>
<ul>
<li><strong>Conflict Detection</strong>: When a transaction writes data, the write set (the keys it wants to write) is sent to a set of <strong>Resolvers</strong>. Each Resolver is responsible for a specific portion of the key space. Multiple Resolvers can be involved in checking the transaction’s read and write sets to detect <strong>conflicts</strong> (read-write conflicts or write-write conflicts).</li>
<li><strong>Parallel Conflict Checking</strong>: Since the key space is partitioned, different Resolvers check different key ranges in parallel. A transaction can only commit if <strong>all</strong> Resolvers agree that there are no conflicts.</li>
</ul>
<p><strong>With FDB, what if a StorageServer is lagging behind on applying the redo logs and a client requests a version of a key pair it does not have?</strong></p>
<ul>
<li><strong>Client Waits</strong>: The client can choose to wait for the StorageServer to catch up by applying the redo logs. Once the StorageServer finishes replaying the logs and reaches the required version, it can serve the requested data.</li>
<li><strong>Retry at Another Replica</strong>: If the StorageServer does not have the requested version yet, the client can try to read from another <strong>replica</strong> of the key. FDB typically stores multiple replicas of data across different StorageServers, so the client can retry the request from a replica that is up to date.</li>
<li><strong>Transaction Restart</strong>: If neither replica has the requested version or the delay is too long, the client may restart the transaction. Since FoundationDB uses <strong>MVCC (Multi-Version Concurrency Control)</strong>, restarting the transaction allows it to obtain a fresh version of the key from an up-to-date StorageServer.</li>
</ul>
<p><strong>Consider a database for students enrolling in courses and professors teaching those courses. Provide a SDM model of this database?</strong></p>
<p>Students: base concrete object class.</p>
<p>member property: student_id, name, age, email, department_id.</p>
<p>identifier: student_id.</p>
<p>Professors: base concrete object class.</p>
<p>member property: professor_id, name, age, email, department_id.</p>
<p>identifier: professor_id.</p>
<p>Courses: base concrete object class</p>
<p>member property: course_id, name, location, start_time, end_time, department_id.</p>
<p>derived member property: professor as Professors.professor_id.</p>
<p>identifier: course_id.</p>
<p>Enrollment: base duration event class.</p>
<p>member property: enrollment_id, date_of_enrollment.</p>
<p>member participant: student in Students, course in Courses.</p>
<p>identifier: enrollment_id.</p>
<p>Departments: abstract Students and Professors on common value of department_id.</p>
<p>derived member property: department_id as distinct value of (Students.department_id union Professors.department_id).</p>
<p><strong>What is the difference between a monolithic database management system and a disaggregated database management system?</strong> </p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Monolithic DBMS</th>
<th>Disaggregated DBMS</th>
</tr>
</thead>
<tbody><tr>
<td>Architecture</td>
<td>All components tightly integrated into a single system</td>
<td>Components like storage, computation, and query processing are separated</td>
</tr>
<tr>
<td>Scalability</td>
<td>Scales through vertical scaling (adding resources to the single server)</td>
<td>Scales through horizontal scaling (independent scaling of storage and compute)</td>
</tr>
<tr>
<td>Performance Bottlenecks</td>
<td>May face bottlenecks as the system grows</td>
<td>Components are independently optimized, reducing bottlenecks</td>
</tr>
<tr>
<td>Resource Management</td>
<td>Storage and compute resources are tightly coupled, hard to manage separately</td>
<td>Storage and compute resources can be managed independently, offering flexibility</td>
</tr>
<tr>
<td>Complexity</td>
<td>Easier to deploy and manage initially, but complexity increases with scale</td>
<td>More complex to manage and coordinate different components</td>
</tr>
<tr>
<td>Cost</td>
<td>Pay for all resources, even if they are not fully utilized</td>
<td>Can optimize resource usage and costs by scaling components independently</td>
</tr>
<tr>
<td>Consistency</td>
<td>Strong data consistency due to tight integration</td>
<td>Requires additional mechanisms to ensure consistency across components</td>
</tr>
</tbody></table>
<p><strong>With Gamma and its data flow execution paradigm, how does the system know when the execution of a parallel query involving multiple operators is complete?</strong></p>
<p>Data Dependency Graph: The query execution is modeled as a directed acyclic graph (DAG), where each node represents an operator (e.g., selection, join). Data flows between operators, and the system tracks the completion of each operator based on this graph.</p>
<p>Completion Signals: Each parallel operator sends a “done” signal once it finishes processing its data partition. The system monitors these signals to determine when all operators have finished.</p>
<p>Coordinator: A central coordinator tracks the progress of parallel tasks. When all tasks report completion, the system declares the query execution as complete.</p>
<p>Reference: <a href="https://sigmodrecord.org/publications/sigmodRecord/2203/pdfs/08_fdb-zhou.pdf">https://sigmodrecord.org/publications/sigmodRecord/2203/pdfs/08_fdb-zhou.pdf</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>HRPS Background</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/13/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/HRPS%20Background/</url>
    <content><![CDATA[<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/index_and_page.png" alt="img"></p>
<p>Each page consits of the header, the record space and a pointer array. Each slot in this array points to a record within the page.</p>
<p>A record is located by providing its page address and the slot number. The combination is called RID.</p>
<p>Life Cycle of a record:</p>
<p>Insert: Find an empty space to put it and set a slot at the very end of the page to point to it.</p>
<p>Delete: Remove the record and reclaim this space, set its slot number to null. When there are too many garbage slots, the system will drop the index structure, do reorganization on this disk page by removing all null slots, and reconstruct the index structure from scratch.</p>
<p>Non-clustered Index: the data of the disk page is independent of the bucket or leaf node of index.</p>
<p>Clustered Index: the data of the disk page resides within the bucket or leaf node of index.</p>
<p>Hash Index: the item in buckets is not ordered by the attribute value of index.</p>
<p>B+Tree Index: the item in leaf nodes is ordered by the attribute value of index.</p>
<ul>
<li>Primary Index: the data in the disk page is ordered.</li>
<li>Secondary Index: the data in the disk page is not ordered.</li>
</ul>
<p>Reference: <a href="https://www.vldb.org/conf/1990/P481.PDF">https://www.vldb.org/conf/1990/P481.PDF</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>约束</title>
    <url>/MySQL/2024/09/19/MySQL/%E7%BA%A6%E6%9D%9F/</url>
    <content><![CDATA[<p>作用于表中字段上的规则，用于<strong>限制</strong>存储在表中的数据，保证表中数据的正确性、有效性和完整性。</p>
<ul>
<li>NOT NULL 非空约束</li>
<li>UNIQUE 唯一约束</li>
<li>PRIMARY KEY 主键约束</li>
<li>DEFAULT 默认约束</li>
<li>CHECK 检查约束</li>
<li>FOREIGN KEY 外键约束：让两张表的数据之间建立连接，具有外键的表是子表。</li>
</ul>
<p>声明外键：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> 表名 (</span><br><span class="line">  字段名 数据类型,</span><br><span class="line">  [<span class="keyword">CONSTRAINT</span>] [外键名称] <span class="keyword">FOREIGN KEY</span> (外键字段名) <span class="keyword">REFERENCES</span> 主表 (主表列名)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER TABLE</span> 表名 <span class="keyword">ADD CONSTRAINT</span> 外键名称 <span class="keyword">FOREIGN KEY</span> (外键字段名) <span class="keyword">REFERENCES</span> 主表 (主表列名);</span><br></pre></td></tr></table></figure>

<p>删除外键：<code>ALTER TABLE 表名 DROP FOREIGN KEY 外键名称;</code></p>
<p>删除&#x2F;更新行为：</p>
<ul>
<li>NO ACTION 在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键，如果有则不允许删除&#x2F;更新。（与RESTRICT一致）</li>
<li>RESTRICT</li>
<li>CASCADE 在父表中删除&#x2F;更新对应记录时，首先检查该记录是否有对应外键：如果有，也删除&#x2F;更新外键在子表中的记录。</li>
<li>SET NULL 在父表中删除对应记录时，首先检查该记录是否有对应外键：如果有，设置子表中该外键值为 null （要求外键允许取 null）。</li>
<li>SET DEFAULT 父表数据变更时，子表将外键列设置成一个默认的值（InnoDB不支持）。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER TABLE</span> 表名 <span class="keyword">ADD CONSTRAINT</span> 外键名称 <span class="keyword">FOREIGN KEY</span> (外键字段) <span class="keyword">REFERENCES</span> 主表名 (主表字段名) <span class="keyword">ON</span> <span class="keyword">UPDATE</span> [更新行为] <span class="keyword">ON</span> <span class="keyword">DELETE</span> [删除行为];</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title>CAMP</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/CAMP/</url>
    <content><![CDATA[<h1 id="Greedy-Dual-Size-GDS-algorithm"><a href="#Greedy-Dual-Size-GDS-algorithm" class="headerlink" title="Greedy Dual Size (GDS) algorithm"></a>Greedy Dual Size (GDS) algorithm</h1><p>Key Concepts:</p>
<ol>
<li><strong>Variable Size and Cost</strong>:<ul>
<li>Unlike simple algorithms that treat all objects equally, GDS takes into account:<ul>
<li><strong>Size of the object</strong> (<code>size(p)</code>): Larger objects take up more space in memory.</li>
<li><strong>Cost of the object</strong> (<code>cost(p)</code>): This can represent factors like time to retrieve the object, computational effort, or other resource usage.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Score H(p)</strong>:<ul>
<li>Each key-value pair ppp in the cache is assigned a score H(p). This score reflects the <strong>benefit of keeping the object</strong> in memory and is calculated using:<ul>
<li>A <strong>global parameter L</strong>, which adjusts dynamically based on cache state.</li>
<li>The <strong>size(p)</strong> of the object.</li>
<li>The <strong>cost(p)</strong> associated with the object.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Eviction Strategy</strong>:<ul>
<li>When the cache is full, and a new object needs to be added, GDS removes the object with the <strong>lowest score H(p)</strong>. This process continues until there is enough space for the new object.</li>
</ul>
</li>
</ol>
<h2 id="Proposition-1"><a href="#Proposition-1" class="headerlink" title="Proposition 1"></a>Proposition 1</h2><p><strong>L is non-decreasing over time.</strong></p>
<ul>
<li>The global parameter L, which reflects the minimum priority H(p) among all key-value pairs in the KVS, will either stay the same or increase with each operation. This ensures stability and helps prioritize eviction decisions consistently.</li>
</ul>
<p>For any key-value pair ppp in the KVS, the relationship holds:</p>
<p><strong>L ≤ H(p) ≤ L + cost(p) &#x2F; size(p)</strong></p>
<ul>
<li>H(p), the priority of p, always lies between the global minimum L and L + cost(p) &#x2F; size(p), ensuring H(p) reflects both its retrieval cost and size relative to other elements.</li>
</ul>
<p><strong>Intuition Behind Proposition 1:</strong></p>
<ul>
<li>As L increases over time (reflecting the minimum H(p)), less recently used or less “valuable” pairs become increasingly likely to be evicted. This ensures that newer and higher-priority pairs stay in the KVS longer.</li>
</ul>
<p><strong>Key Insights from Proposition 1:</strong></p>
<ol>
<li><strong>Delayed Eviction:</strong><ul>
<li>When p is requested again while in memory, its H(p) increases to L + cost(p) &#x2F; size(p), delaying its eviction.</li>
</ul>
</li>
<li><strong>Impact of Cost-to-Size Ratio:</strong><ul>
<li>Pairs with higher cost(p) &#x2F; size(p) stay longer in the KVS. For example, if one pair’s ratio is c times another’s, it will stay approximately c times longer.</li>
</ul>
</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart3.png" alt="img"></p>
<h2 id="Key-Points-in-the-Diagram"><a href="#Key-Points-in-the-Diagram" class="headerlink" title="Key Points in the Diagram"></a>Key Points in the Diagram</h2><ol>
<li><strong>Cost-to-Size Ratios</strong>:<ol>
<li>Key-value pairs are grouped into <strong>queues</strong> according to their cost-to-size ratio.</li>
<li>Each queue corresponds to a specific cost-to-size ratio.</li>
</ol>
</li>
<li><strong>Grouping by Ratio</strong>:<ol>
<li>Within each queue, key-value pairs are managed using the <strong>Least Recently Used (LRU)</strong> strategy.</li>
</ol>
</li>
<li><strong>Priority Management</strong>:<ol>
<li>The <strong>priority (H-value)</strong> of a key-value pair is based on: <strong>H(p) &#x3D; L + cost(p) &#x2F; size(p)</strong><ol>
<li>L: The global non-decreasing variable.</li>
<li>cost(p) &#x2F; size(p): The cost-to-size ratio of the key-value pair.</li>
</ol>
</li>
</ol>
</li>
<li><strong>Efficient Eviction</strong>:<ol>
<li>CAMP maintains a <strong>heap</strong> that points to the <strong>head of each queue</strong>, storing the minimum H(p) from every queue.</li>
<li>To identify the next key-value pair for eviction:<ol>
<li><strong>The algorithm checks the heap to find the queue with the smallest H(p).</strong></li>
<li><strong>It then evicts the key-value pair at the front of that queue (i.e., the least recently used pair in that cost-to-size group).</strong></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Rounding-in-CAMP"><a href="#Rounding-in-CAMP" class="headerlink" title="Rounding in CAMP"></a>Rounding in CAMP</h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/bg_bm_rounding.png" alt="img"></p>
<ol>
<li><strong>Purpose</strong>: To improve performance, CAMP <strong>reduces the number of LRU queues</strong> by grouping key-value pairs with <strong>similar cost-to-size ratios</strong> into the same queue.</li>
<li><strong>Key Idea</strong>: Preserve the most significant bits proportional to the value’s size.</li>
</ol>
<h2 id="Proposition-2-Explanation-of-Rounding-and-Distinct-Values"><a href="#Proposition-2-Explanation-of-Rounding-and-Distinct-Values" class="headerlink" title="Proposition 2: Explanation of Rounding and Distinct Values"></a>Proposition 2: Explanation of Rounding and Distinct Values</h2><h3 id="Implications"><a href="#Implications" class="headerlink" title="Implications"></a>Implications</h3><ol>
<li><p><strong>Trade-Off Between Precision and Efficiency</strong>:</p>
<ul>
<li><p>A higher p preserves more precision but increases the number of distinct values (and thus computational complexity).</p>
</li>
<li><p>Lower p reduces the number of distinct values, making CAMP more efficient but less precise.</p>
</li>
</ul>
</li>
<li><p><strong>Rounding Efficiency</strong>:</p>
<ul>
<li>By limiting the number of distinct values, CAMP minimizes the number of LRU queues, reducing overhead while still approximating GDS closely.</li>
</ul>
</li>
</ol>
<h2 id="Proposition-3-Competitive-Ratio-of-CAMP"><a href="#Proposition-3-Competitive-Ratio-of-CAMP" class="headerlink" title="Proposition 3: Competitive Ratio of CAMP"></a>Proposition 3: Competitive Ratio of CAMP</h2><h3 id="Practical-Implications"><a href="#Practical-Implications" class="headerlink" title="Practical Implications"></a>Practical Implications</h3><ol>
<li><p><strong>Precision ppp</strong>:</p>
<ul>
<li><p>The smaller the ϵ (higher ppp), the closer CAMP approximates GDS.</p>
</li>
<li><p>For sufficiently large p, CAMP performs nearly as well as GDS.</p>
</li>
</ul>
</li>
<li><p><strong>Trade-off</strong>:</p>
<ul>
<li>Higher p increases precision but also increases the number of distinct cost-to-size ratios and computational overhead.</li>
</ul>
</li>
</ol>
<h3 id="CAMP’s-Improvement-Over-GDS"><a href="#CAMP’s-Improvement-Over-GDS" class="headerlink" title="CAMP’s Improvement Over GDS:"></a>CAMP’s Improvement Over GDS:</h3><ol>
<li><strong>Approximation:</strong> CAMP simplifies H(p) by <strong>rounding</strong> the cost-to-size ratio, reducing the precision but making the algorithm more efficient.</li>
<li><strong>Grouping:</strong> Key-value pairs are <strong>grouped</strong> by similar cost-to-size ratios, reducing the number of queues and simplifying priority management.</li>
<li><strong>Tie-Breaking:</strong> CAMP uses <strong>LRU within each group</strong> to determine the eviction order, making it computationally cheaper.</li>
</ol>
<h3 id="Figure-4-Heap-Node-Visits"><a href="#Figure-4-Heap-Node-Visits" class="headerlink" title="Figure 4: Heap Node Visits"></a>Figure 4: Heap Node Visits</h3><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart4.png" alt="img"></p>
<p>This figure compares the number of heap node visits for GDS and CAMP as a function of cache size:</p>
<ol>
<li><p><strong>GDS</strong>:</p>
<ul>
<li><p><strong>Heap size equals the total number of key-value pairs in the cache.</strong></p>
</li>
<li><p>Every heap update (insertion, deletion, or priority change) requires visiting O(log⁡n) nodes, where n is the number of cache entries.</p>
</li>
<li><p>As cache size increases, GDS’s overhead grows significantly.</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>:</p>
<ul>
<li><p><strong>Heap size equals the number of non-empty LRU queues, which is much smaller than the total number of cache entries.</strong></p>
</li>
<li><p>Heap updates occur only when:</p>
<ul>
<li><p>The priority of the head of an LRU queue changes.</p>
</li>
<li><p>A new LRU queue is created.</p>
</li>
</ul>
</li>
<li><p>As cache size increases, the number of non-empty LRU queues remains relatively constant, resulting in fewer heap updates.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart5.png" alt="img"></p>
<p><strong>(a) Cost-Miss Ratio vs Precision</strong></p>
<ul>
<li><strong>横轴</strong>：精度（Precision），从低到高。</li>
<li><strong>纵轴</strong>：成本未命中比（Cost-Miss Ratio）。</li>
<li><strong>结果</strong>：<ul>
<li>不同的缓存大小比（0.01、0.1 和 0.3）在较低精度下表现一致。</li>
<li>提高精度后，成本未命中比没有显著变化。</li>
<li>说明即使使用较低精度，CAMP 的成本未命中比也能接近 GDS（标准实现）。</li>
</ul>
</li>
</ul>
<p><strong>(b) LRU Queues vs Precision</strong></p>
<ul>
<li><strong>横轴</strong>：精度（Precision）。</li>
<li><strong>纵轴</strong>：CAMP 维护的非空 LRU 队列数量。</li>
<li><strong>结果</strong>：<ul>
<li><strong>低精度</strong>（1-5）：CAMP 维持稳定的少量 LRU 队列（约 5 个）。</li>
<li><strong>高精度</strong>（&gt;10）：队列数增加，尤其是在较大的缓存大小比（如 1.0）下。</li>
<li><strong>结论</strong>：<ul>
<li>在较低精度下，CAMP 能保持较低的计算开销，同时维持高效的队列管理。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>(c) Cost-Miss Ratio vs Cache Size Ratio</strong></p>
<ul>
<li><strong>横轴</strong>：缓存大小比（Cache Size Ratio），即缓存大小与 trace 文件中唯一键值对总大小的比值。</li>
<li><strong>纵轴</strong>：成本未命中比（Cost-Miss Ratio）。</li>
<li><strong>结果</strong>：<ul>
<li><strong>CAMP</strong>：<ul>
<li>在所有缓存大小下，成本未命中比最低。</li>
<li>说明 CAMP 在高成本键值对管理上更具效率。</li>
</ul>
</li>
<li><strong>Pooled LRU</strong>：<ul>
<li>在较小缓存下表现稍差，但随着缓存增加，接近 CAMP。</li>
</ul>
</li>
<li><strong>LRU</strong>：<ul>
<li>成本未命中比始终最高。</li>
</ul>
</li>
<li><strong>结论</strong>：<ul>
<li>CAMP 优于 LRU 和 Pooled LRU，尤其是在小缓存下。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>(d) Miss Rate vs Cache Size Ratio</strong></p>
<ul>
<li><strong>横轴</strong>：缓存大小比（Cache Size Ratio）。</li>
<li><strong>纵轴</strong>：未命中率（Miss Rate）。</li>
<li><strong>结果</strong>：<ul>
<li><strong>CAMP</strong>：<ul>
<li>未命中率显著低于 LRU 和 Pooled LRU，尤其在小缓存下表现最优。</li>
</ul>
</li>
<li><strong>Pooled LRU</strong>：<ul>
<li>未命中率随着缓存增大而下降，但始终高于 CAMP。</li>
<li>最低成本池（cheapest pool）未命中率接近 100%，次低成本池未命中率达到 65%。</li>
</ul>
</li>
<li><strong>LRU</strong>：<ul>
<li>始终高于 CAMP 和 Pooled LRU。</li>
</ul>
</li>
<li><strong>结论</strong>：<ul>
<li>CAMP 在多种缓存大小下都保持较低的未命中率，且比 Pooled LRU 更均衡。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="CAMP-的适应能力：访问模式变化的分析"><a href="#CAMP-的适应能力：访问模式变化的分析" class="headerlink" title="CAMP 的适应能力：访问模式变化的分析"></a>CAMP 的适应能力：访问模式变化的分析</h1><p>实验设置：</p>
<ul>
<li>使用 10 个不同的 trace 文件，每个文件包含 400 万个键值对引用。</li>
<li>每个 trace 文件（如 TF1、TF2 等）中的请求在其结束后不会再被引用，模拟访问模式的突然变化。</li>
<li>访问模式具有倾斜分布（如 Zipf 分布），每个 trace 文件中的高成本对象可能在下一次访问中完全无效。</li>
</ul>
<p>目标：</p>
<ul>
<li>比较 <strong>CAMP</strong>、<strong>Pooled LRU</strong> 和 <strong>LRU</strong> 在不同缓存大小下对访问模式突变的适应能力。</li>
<li>评估三种算法在突然变化后清除旧的高成本键值对的效率，以及对总体性能（如成本未命中比和未命中率）的影响。</li>
</ul>
<p>不同算法的行为分析</p>
<ol>
<li><p><strong>LRU</strong>：</p>
<ul>
<li><p>按最近使用排序，当新请求的总大小超过缓存大小时清除旧数据。</p>
</li>
<li><p>当缓存大小比为 1 时，清除 TF1 数据的时间点对应于 TF3 开始请求的第一个键值对。</p>
</li>
</ul>
</li>
<li><p><strong>Pooled LRU</strong>：</p>
<ul>
<li><p>将键值对按成本分组，每组分配固定比例的缓存空间。</p>
</li>
<li><p>高成本池占据 99% 的缓存空间，因此在每个新 trace 开始时会突然清除一批旧数据。</p>
</li>
<li><p>对于缓存大小比 2&#x2F;3 或更高的情况，直到 TF4（约 800 万请求后）才会清除所有 TF1 数据。</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>：</p>
<ul>
<li><p>对每个成本-大小比维护 LRU 队列，这些队列的大小可以动态调整。</p>
</li>
<li><p><strong>优先淘汰较低优先级的数据，但高成本数据即使来自旧 trace，也具有一定保留优先级。</strong></p>
</li>
<li><p><strong>当新数据的总大小超过缓存时，旧 trace 的高成本数据才会被逐步清除。</strong></p>
</li>
</ul>
</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart6.png" alt="img"></p>
<p><strong>图 6c：缓存比 0.25（小缓存）</strong></p>
<ol>
<li><p><strong>LRU</strong>：</p>
<ul>
<li><p>清除最快，仅需 <strong>2.1 万次请求</strong> 即完全清除 Trace 1 的所有键值对。</p>
</li>
<li><p>由于 LRU 优先淘汰最久未使用的数据，小缓存下表现最佳。</p>
</li>
</ul>
</li>
<li><p><strong>Pooled LRU</strong>：</p>
<ul>
<li><p>清除速度较慢，需要 <strong>13.1 万次请求</strong>。</p>
</li>
<li><p>原因：Pooled LRU 按成本对键值对分组，高成本池占用较多缓存空间，导致清除滞后。</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>：</p>
<ul>
<li><p>初期清除速度比 Pooled LRU 更快，但最后完全清除所有键值对需到 <strong>TF3 结束（770 万次请求）</strong>。</p>
</li>
<li><p>然而，这些未被清除的 Trace 1 数据仅占缓存的 <strong>2%</strong>，说明 CAMP 优先保留了高成本键值对。</p>
</li>
</ul>
</li>
</ol>
<p><strong>图 6d：缓存比 0.75（大缓存）</strong></p>
<ol>
<li><p><strong>LRU</strong>：</p>
<ul>
<li><p>同样清除最快，几乎在 Trace 2 开始时就清除掉大部分 Trace 1 的数据。</p>
</li>
<li><p>说明即使缓存较大，LRU 仍然倾向淘汰旧数据。</p>
</li>
</ul>
</li>
<li><p><strong>Pooled LRU</strong>：</p>
<ul>
<li><p>清除延迟显著，需要 <strong>730 万次请求</strong>，接近 TF3 结束。</p>
</li>
<li><p>原因：高成本池占用过多缓存空间，延迟清除低成本和无用数据。</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>：</p>
<ul>
<li><p>大部分 Trace 1 数据在较早阶段被淘汰，仅保留少量最昂贵的键值对（占缓存比小于 <strong>0.6%</strong>）。</p>
</li>
<li><p>即使在 <strong>4000 万次请求</strong>后，这些高成本键值对仍在缓存中，但对整体缓存利用影响极小。</p>
</li>
</ul>
</li>
</ol>
<p>针对不同大小但成本相同的键值对，CAMP 优先保留较小的键值对，从而降低未命中率和成本未命中比。</p>
<p>针对相同大小但成本不同的键值对，CAMP 优先保留高成本键值对，在成本未命中比上显著优于其他算法。</p>
<p>与其他算法的对比：</p>
<ul>
<li><p>LRU：适用于简单场景，但无法处理成本差异。</p>
</li>
<li><p>Pooled LRU：小缓存情况下表现不错，但静态分区策略限制了其大缓存场景的效率。</p>
</li>
</ul>
<p>CAMP 的适应性：在处理多样化的成本分布时，通过动态调整和四舍五入策略，CAMP 在复杂负载下表现出更高的灵活性和效率。</p>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>What is the time complexity of LRU to select a victim?</strong></p>
<p><strong>O(1)</strong> because the least recently used item is always at the tail of the list.</p>
<p><strong>What is the time complexity of CAMP to select a victim?</strong></p>
<p><strong>O(logk)</strong> CAMP identifies the key-value pair with the smallest priority from the heap, deletes it and then <strong>heapifies</strong>.</p>
<p><strong>Why does CAMP do rounding using the high order bits?</strong></p>
<ul>
<li>CAMP rounds cost-to-size ratios to <strong>reduce the number of distinct ratios (or LRU queues)</strong>.</li>
<li>High-order bits are retained because they represent the <strong>most significant portion of the value</strong>, ensuring that <strong>approximate prioritization is maintained</strong>.</li>
</ul>
<p><strong>How does BG generate social networking actions that are always valid?</strong></p>
<p><strong>Pre-Validation of Actions:</strong></p>
<ul>
<li>Before generating an action, BG <strong>checks</strong> the current state of the database to ensure the action is valid. For instance:<ul>
<li>A friend request is only generated if the two users are not already friends or in a “pending” relationship.</li>
<li>A comment can only be posted on a resource if the resource exists.</li>
</ul>
</li>
</ul>
<p><strong>Avoiding Concurrent Modifications:</strong></p>
<ul>
<li>BG <strong>prevents multiple threads from concurrently modifying the same user’s state</strong>.</li>
</ul>
<p><strong>How does BG scale to a large number of nodes?</strong></p>
<p>BG employs <strong>a shared-nothing architecture</strong> with the following mechanisms to scale effectively:</p>
<ol>
<li><p><strong>Partitioning Members and Resources:</strong></p>
<ul>
<li><p>BGCoord <strong>partitions</strong> the database into <strong>logical fragments</strong>, each containing <strong>a unique subset</strong> of members, their resources, and relationships.</p>
</li>
<li><p>These fragments are assigned to individual BGClients.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple BGClients:</strong></p>
<ul>
<li><p>Each BGClient operates <strong>independently</strong>, generating workloads for its assigned logical fragment.</p>
</li>
<li><p>By running <strong>multiple</strong> BGClients <strong>in parallel</strong> across different nodes, BG can scale horizontally to handle millions of requests.</p>
</li>
</ul>
</li>
<li><p><strong>D-Zipfian Distribution:</strong></p>
<ul>
<li><p>To ensure realistic and scalable workloads, BG uses a decentralized Zipfian distribution (D-Zipfian) that <strong>dynamically assigns</strong> requests to BGClients based on node performance.</p>
</li>
<li><p>Faster nodes receive a larger share of the logical fragments, ensuring even workload distribution.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Control:</strong></p>
<ul>
<li>BG <strong>prevents simultaneous threads from issuing actions for the same user</strong>, maintaining the integrity of modeled user interactions and avoiding resource contention.</li>
</ul>
</li>
</ol>
<p><strong>True or False: BG quantifies the amount of unpredictable data produced by a data store?</strong></p>
<p>True.</p>
<p>This is achieved through:</p>
<ul>
<li><strong>Validation Phase:</strong><ul>
<li>BG uses <strong>read and write log records</strong> to detect instances where a read operation observes a value <strong>outside the acceptable range</strong>, classifying it as “unpredictable data.”</li>
</ul>
</li>
<li><strong>Metrics Collection:</strong><ul>
<li>The percentage of requests that observe unpredictable data (τ) is a key metric used to evaluate the data store’s consistency.</li>
</ul>
</li>
</ul>
<p><strong>How is BG’s SoAR different than its Socialites rating?</strong></p>
<p>SoAR (Social Action Rating): Represents the <strong>maximum throughput</strong> (actions per second) a data store can achieve while meeting a given SLA.</p>
<p>Socialites Rating: Represents the <strong>maximum number of concurrent threads</strong> <strong>(users)</strong> a data store can support while meeting the SLA.</p>
<p>Reference: <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=d6f9678772a09ca29101f5efce583960ecf53745">https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=d6f9678772a09ca29101f5efce583960ecf53745</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>HRPS</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/13/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/HRPS/</url>
    <content><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>The HRPS declusters a relation into fragment based on the following criteria:</p>
<ul>
<li>Each fragment contains approximately FC tuples.</li>
<li>Each fragment contains a unique range of values of the partitioning attribute.</li>
</ul>
<p>The variable FC is determined based on the processing capability of the system and the resource requirements of the queries that access the relation (rather than the number of processors in the configuration).</p>
<p>A major underlying assumption of this partitioning strategy is that the selection operators which access the database retrieve and process the selected tuples using either a range predicate or an equality predicate.</p>
<p>For each query Qi, the workload defines the CPU processing time (CPUi), the Disk Processing Time (Diski), and the Network Processing time (Neti) of that query. Observe that these times are determined based on the resource requirements of each individual query and the processing capability of the system. Each query retrieves and processes (TuplesPerQi) tuples from the database. Furthermore, we assume that the workload defines the frequency of occurrence of each query (FreqQi).</p>
<p>Rather than describing the HRPS with respect to each query in the workload, we deline an average query (Qavg) that is representative of all the queries in the workload. The CPU, disk and network processing quanta for this query are:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/CPU_Disk_Net_TPQ.png" alt="截屏2025-05-30 18.28.03"></p>
<p>Assume that a single processor cannot overlap the use of two resources for an individual query. Thus, the execution time of Qavg on a single processor in a single user environment is:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/exe_time.png" alt="截屏2025-05-30 18.28.21"></p>
<p>As more processors are used for query execution, the response time decreases. However, this also incurs additional overhead, represented by the variable CP, which refers to the cost of coordinating the query execution across multiple processors (e.g., messaging overhead). The response time of the query on M processors can be described by the following formula:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/RT_M.png" alt="RT_M"></p>
<p>In a single-user environment, both HRPS and range partitioning perform similarly because they both efficiently execute the query on the required processor. However, in a multi-user environment, the range partitioning strategy is likely to perform better because it can distribute the workload across multiple processors, improving system throughput. In contrast, HRPS might not utilize all available processors as effectively, potentially leading to lower throughput.</p>
<p>Instead of M representing the number of processors over which a relation should be declustered, M is used instead to represent the number of processors that should participate in the execution of Qavg. Since Qavg processes TuplesPerQavg tuples, each fragment of the relation should contain FC &#x3D; TuplesPerQavg &#x2F; M tuples.</p>
<p>The process of fragmenting and distributing data in HRPS:</p>
<ol>
<li><strong>Sorting the relation</strong>: The relation is first sorted based on the partitioning attribute to ensure each fragment contains a distinct range of values.</li>
<li><strong>Fragmentation</strong>: The relation is then split into fragments, each containing approximately <strong>FC</strong> tuples.</li>
<li><strong>Round-robin distribution</strong>: These fragments are distributed to processors in a <strong>round-robin fashion</strong>, ensuring that adjacent fragments are assigned to different processors (unless the number of processors <strong>N</strong> is less than the required processors <strong>M</strong>).</li>
<li><strong>Storing fragments</strong>: All the fragments for a relation on a given processor are stored in the same physical file.</li>
<li><strong>Range table</strong>: The mapping of fragments to processors is maintained in a <strong>one-dimensional directory</strong> called the range table.</li>
</ol>
<p>This method ensures that at least M processors and at most M + 1 processors participate in the execution of a query.</p>
<p><strong>M &#x3D; N</strong>：系统和查询需求匹配，HRPS 调度所有处理器，达到最大并行度和最优性能。</p>
<p><strong>M &lt; N</strong>：HRPS 只调度一部分处理器执行查询，减少通信开销，但部分处理器资源可能闲置。</p>
<p><strong>M &gt; N</strong>：HRPS 将多个片段分配给处理器，尽量利用所有处理器，但每个处理器负担加重，查询执行速度可能受到影响。</p>
<p>HRPS in this paper supports only homogeneous nodes.</p>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>How does HRPS decide the ideal degree of parallelism for a query?</strong></p>
<p>HRPS (Hybrid-Range Partitioning Strategy) decides the ideal degree of parallelism by analyzing the resource requirements of the query, such as CPU, disk I&#x2F;O, and communication costs. It calculates the optimal number of processors (denoted as M) based on these factors. The strategy strikes a balance between minimizing query response time and avoiding excessive overhead from using too many processors.</p>
<p><strong>Why is it not appropriate to direct a query that fetches one record using an index structure to all the nodes of a system based on the shared-nothing architecture?</strong> </p>
<p>Fetching one record should only involve the node that contains the relevant data, as querying all nodes wastes resources and increases response time.</p>
<p><strong>How to extend HRPS to support heterogeneous nodes?</strong></p>
<ol>
<li>More powerful nodes would receive more fragments, while weaker nodes would handle fewer fragments.</li>
<li>The system could monitor node performance and dynamically adjust the degree of parallelism and fragment allocation based on current load and node availability.</li>
<li>Heavier tasks may be directed to more powerful nodes, while smaller or simpler queries could be executed on less powerful nodes.</li>
</ol>
<p>Reference: <a href="https://www.vldb.org/conf/1990/P481.PDF">https://www.vldb.org/conf/1990/P481.PDF</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>Gamma</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/06/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/Gamma/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>主要特点：</p>
<ol>
<li><strong>并行处理</strong>： Gamma 利用了分布式架构，通过将数据和计算任务分散到多个节点上并行处理，极大提高了查询性能和吞吐量。不同的节点可以同时处理不同的任务，从而加速整个系统的响应时间。</li>
<li><ul>
<li><strong>并行查询处理</strong>： Gamma 支持并行执行查询计划中的操作（如选择、投影、连接等）。系统采用流水线并行（pipelined parallelism）和分块并行（partitioned parallelism）技术来最大化资源利用率。</li>
<li><strong>流式处理（Pipelining）</strong>： Gamma 支持流式处理，即在一个操作产生部分结果时，直接将这些结果传递给下一个操作，而不是等待整个操作完成。这样可以减少内存占用，并加快查询处理速度。</li>
</ul>
</li>
<li><strong>数据分片（Declustering）</strong>： Gamma 系统通过数据分片将数据表水平拆分成多个片段，并将这些片段分布到不同的处理节点上。这种方式不仅均衡了负载，还支持并行的查询处理，避免单点瓶颈。</li>
<li><strong>动态负载均衡</strong>： Gamma 能够根据查询的工作负载，动态分配任务到不同的节点，确保整个系统的负载均衡，避免某些节点过载导致性能下降。通过监控每个节点的工作情况，Gamma 能够优化数据和任务分布。</li>
<li><strong>故障容错（Fault Tolerance）</strong>： Gamma 具有一定的故障容错能力，当某个节点出现故障时，系统可以通过冗余机制和数据复制，重新分配任务或从其他节点获取数据，避免系统中断。</li>
<li><strong>扩展性（Scalability）</strong>： Gamma 系统的设计能够随着节点的增加而线性扩展。通过增加处理节点，Gamma 可以处理更大规模的数据和更多的并发查询，保持高性能。</li>
</ol>
<p>Gamma is based on the concept of a shared-nothing architecture in which processors do not share disk drives or random access memory and can only communicate with one another by sending messages through an interconnection network. Mass storage in such an architecture is generally distributed among the processors by connecting one or more disk drives to each processor.</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/Gamma_Arch.png" alt="Gamma 架构"></p>
<p>Reasons why the shared-nothing approach has become the architecture of choice.</p>
<ul>
<li>There is nothing to prevent the architecture from <strong>scaling</strong> to 1000s of processors unlike shared-memory machines for which scaling beyond 30-40 processors may be impossible.</li>
<li>By associating a small number of disks with each processor and distributing the tuples of each relation across the disk drives, it is possible to achieve very high aggregate I&#x2F;O bandwidths without using custom disk controllers</li>
</ul>
<p>When Gamma’s system is figuring out the best way to run a query, it uses information about how the data is divided up (partitioned). This partitioning information helps the system decide how many processors (computers) need to be involved in running the query.</p>
<ul>
<li><strong>For hash partitioning</strong>: If a table (say, “X”) is divided based on a hash function applied to a certain column (like “y”), and the query asks for records where “X.y &#x3D; some value,” <strong>the system can directly go to the specific processor that holds the data matching that value</strong>.</li>
<li><strong>For range partitioning</strong>: If the table is divided based on ranges of values for a column, <strong>the system can limit the query to only the processors that have data within the relevant range</strong>. For example, if “X” is partitioned such that one processor handles values from 1 to 100, and another handles values from 101 to 200, then a query asking for “X.y between 50 and 150” will involve only the processors that have data in those ranges.</li>
</ul>
<p>Different processes in the Gamma system work together. Here’s a simplified explanation of the main types of processes and their roles:</p>
<ol>
<li><p><strong>Catalog Manager</strong>: Acts like a “database encyclopedia,” <strong>storing all the information about data tables and structures</strong>. It ensures that data remains consistent when multiple users access the database.</p>
</li>
<li><p><strong>Query Manager</strong>: Each user gets a query manager that handles query requests. It is responsible for <strong>parsing the query, optimizing it, and generating the execution plan</strong>.</p>
</li>
<li><p><strong>Scheduler Processes</strong>: When a query is executed, the scheduler <strong>coordinates the execution steps</strong>. It activates the necessary operator processes (such as scan, selection, etc.) and ensures that all steps are performed in the correct order.</p>
</li>
<li><p><strong>Operator Processes</strong>: These processes <strong>carry out specific database operations</strong>, like filtering data or joining tables. To reduce the startup delay during query execution, some operator processes are pre-initialized when the system starts.</p>
</li>
<li><p><strong>Other Processes</strong>:</p>
<ul>
<li><p><strong>Deadlock Detection Process</strong>: Detects situations where two or more processes are stuck waiting for each other to release resources.</p>
</li>
<li><p><strong>Recovery Process</strong>: Manages data recovery after a system failure.</p>
</li>
</ul>
</li>
</ol>
<p>How the Gamma system executes database queries?</p>
<ol>
<li><strong>Query Parsing and Optimization</strong>: When a user submits a query, Gamma first parses it to understand what the query is asking for. Then, the system optimizes the query to find the most efficient way to execute it.</li>
<li><strong>Query Compilation</strong>: After optimization, the query is compiled into an “<strong>operator tree</strong>“ made up of different operations (such as scan, selection, join, etc.). This tree outlines the steps and the order in which the query will be executed.</li>
<li><strong>Single-Site vs. Multi-Site Queries</strong>: If the query only involves data on a single node (e.g., querying a small table), the system executes it directly on that node. However, if the query involves data distributed across multiple nodes (e.g., joining large tables), the system uses a “scheduler process” to coordinate the execution.</li>
<li><strong>Scheduler Coordination</strong>: The scheduler process is responsible for activating various operator processes across the nodes, such as instructing one node to scan data while another filters it. The scheduler also manages the flow of data between these operations, ensuring they happen in the correct order.</li>
<li><strong>Returning the Results</strong>: Once all operations are completed, the query results are collected and returned to the user. For queries embedded in a program, the results are passed back to the program that initiated the query.</li>
</ol>
<p>Different operations (like scanning data, filtering, joining tables, etc.) are carried out in a parallel manner. Here’s a simplified explanation:</p>
<ol>
<li><strong>Operator Processes</strong>: In Gamma, each operation in a query is handled by something called an “operator process.” For example, if the query needs to scan data from a table, filter some rows, and then join with another table, there would be separate operator processes for scanning, filtering, and joining.</li>
<li><strong>Data Flow</strong>: The data flows from one operator process to the next. For instance, the scan operator reads data from the disk and sends it to the filter operator, which then passes the filtered results to the join operator. This creates a kind of “data pipeline.”</li>
<li><strong>Split Table</strong>: Gamma uses a “split table” to decide where the data should go next. Think of it like a routing table that directs the flow of data. For example, if the data needs to be sent to multiple nodes for parallel processing, the split table helps determine which node each piece of data should go to.</li>
<li><strong>End of Processing</strong>: Once an operator finishes processing all its data, it closes its output streams and sends a signal to the scheduler process (which coordinates the whole query) to let it know that this part of the work is done.</li>
</ol>
<p>In simple terms, the operator and process structure in Gamma is like an assembly line where data moves from one step (operator) to the next, with each operator performing a specific task, and the split table guiding the data flow. This setup allows the system to process data in parallel across multiple nodes, making it much faster.</p>
<h1 id="Operators"><a href="#Operators" class="headerlink" title="Operators"></a>Operators</h1><h2 id="Selection-Operator"><a href="#Selection-Operator" class="headerlink" title="Selection Operator"></a>Selection Operator</h2><p>Data Spread Across Multiple Disks: In Gamma, data tables are split up and stored across multiple disks (this is called “declustering”). Because of this, when you want to search (select) for specific data, the system can perform the search in parallel across multiple disks.</p>
<p>Parallel Selection Process:</p>
<ul>
<li>If the search condition (predicate) matches the way the data is divided (partitioned), the system can narrow down the search to just the relevant nodes (computers with disks) that have the data. For example:</li>
<li><ul>
<li>If the data is divided using a <strong>hash or range</strong> partitioning method based on a certain attribute (like “employee ID”), and the search is also based on that attribute (e.g., “employee ID &#x3D; 123”), then the search can be directed only to the node that holds data matching that condition.</li>
<li>If the data is divided using a <strong>round-robin</strong> method (spreading data evenly across all disks) or if the search condition <strong>doesn’t match the partitioning attribute</strong>, then the system has to search on all nodes.</li>
</ul>
</li>
</ul>
<p>Performance Optimization:</p>
<ul>
<li>To make the search faster, Gamma uses a “<strong>read-ahead</strong>“ technique. This means that when it reads one page of data, it starts loading the next page at the same time, so that the processing of data can keep going without waiting for the next page to load.</li>
</ul>
<h2 id="Join-Operator"><a href="#Join-Operator" class="headerlink" title="Join Operator"></a>Join Operator</h2><p>Using Hash Partitioning: The join algorithms in Gamma are based on a concept called “buckets.” This means splitting the two tables to be joined into separate groups (buckets) that don’t overlap. The groups are created by applying a hash function to the join attribute (e.g., Employee ID), so that data with the same hash value ends up in the same bucket.</p>
<p>By partitioning the data into different buckets, each bucket contains unique data subsets, allowing parallel processing of these buckets, which speeds up the join operation. Additionally, <strong>all data with the same join attribute value is in the same bucket</strong>, making it easier to perform the join.</p>
<p>Gamma implements four different parallel join algorithms:</p>
<ul>
<li><strong>Sort-Merge Join</strong>: Joins data by sorting and merging.</li>
<li><strong>Grace Join</strong>: A distributed hash-based join algorithm.</li>
<li><strong>Simple Hash Join</strong>: A straightforward hash-based partitioning join.</li>
<li><strong>Hybrid Hash Join</strong>: A combination of different join techniques.</li>
</ul>
<p><strong>Default to Hybrid Hash Join:</strong> Research showed that the Hybrid Hash Join almost always performs the best, so Gamma uses this algorithm by default.</p>
<p>Limitations: These hash-based join algorithms can <strong>only handle equi-joins</strong> (joins with equality conditions, like “Employee ID &#x3D; Department ID”). They currently don’t support non-equi-joins (conditions like “Salary &gt; Department Budget * 2”). To address this, Gamma is working on designing a new parallel non-equi-join algorithm.</p>
<h3 id="Hybrid-Hash-Join"><a href="#Hybrid-Hash-Join" class="headerlink" title="Hybrid Hash-Join"></a>Hybrid Hash-Join</h3><ul>
<li>In the first phase, the algorithm uses a hash function to partition the inner (smaller) relation, R, into N buckets. The tuples of the first bucket are used to build an in-memory hash table while the remaining N-1 buckets are stored in temporary files. A good hash function produces just enough buckets to ensure that each bucket of tuples will be small enough to fit entirely in main memory.</li>
<li>During the second phase, relation S is partitioned using the hash function from step 1. Again, the last N-1 buckets are stored in temporary files while the tuples in the first bucket are used to immediately probe the in-memory hash table built during the first phase.</li>
<li>During the third phase, the algorithm joins the remaining N-1 buckets from relation R with their respective buckets from relation S.</li>
</ul>
<p>The join is thus broken up into a series of smaller joins; each of which hopefully can be computed without experiencing join overflow. The size of the smaller relation determines the number of buckets; this calculation is independent of the size of the larger relation.</p>
<h3 id="Parallel-version-of-Hybrid-Hash-Join"><a href="#Parallel-version-of-Hybrid-Hash-Join" class="headerlink" title="Parallel version of Hybrid Hash-Join"></a>Parallel version of Hybrid Hash-Join</h3><p>Partitioning into Buckets: The data from the two tables being joined is first divided into N buckets (small groups). The number of buckets is chosen so that each bucket can fit in the combined memory of the processors that are handling the join.</p>
<p>Storage of Buckets: Out of the N buckets, N-1 buckets are stored temporarily on disk across different disk sites, while one bucket is kept in memory for immediate processing.</p>
<p>Parallel Processing: A joining split table is used to decide which processor should handle each bucket, helping to divide the work across multiple processors. This means that <strong>different processors can work on different parts of the join at the same time</strong>, speeding up the process.</p>
<p>Overlapping Phases for Efficiency:</p>
<ul>
<li>When partitioning the <strong>first table (R)</strong> into buckets, Gamma simultaneously builds a hash table for the first bucket in memory at each processor.</li>
<li>When partitioning the <strong>second table (S)</strong>, Gamma simultaneously performs the join for the first bucket from S with the first bucket from R. This way, partitioning and joining overlap, making the process more efficient.</li>
</ul>
<p>Adjusting the Split Table for Parallel Joining: The joining split table is updated to make sure that the data from the first bucket of both tables is sent to the right processors that will perform the join. When the remaining N-1 buckets are processed, only the routing for joining is needed.</p>
<h2 id="Aggregate-Operator"><a href="#Aggregate-Operator" class="headerlink" title="Aggregate Operator"></a>Aggregate Operator</h2><p>Parallel Calculation of Partial Results: Each processor in the Gamma system calculates the aggregate result for its own portion of the data simultaneously. For example, if the goal is to calculate a sum, each processor will first compute the sum for the data it is responsible for.</p>
<p>Combining Partial Results: After calculating their partial results, the processors send these results to a central process. This central process is responsible for combining all the partial results to produce the final answer.</p>
<p>Two-Step Computation:</p>
<ul>
<li><strong>Step 1</strong>: Each processor calculates the aggregate value (e.g., sum, count) for its data partition, resulting in partial results.</li>
<li><strong>Step 2</strong>: The processors then redistribute these partial results based on the “group by” attribute. This means that the partial results for each group are collected at a single processor, where the final aggregation for that group is completed.</li>
</ul>
<h2 id="Update-Operator"><a href="#Update-Operator" class="headerlink" title="Update Operator"></a>Update Operator</h2><p>For the most part, the update operators (replace, delete, and append) are implemented using standard techniques. The only exception occurs when a replace operator modifies the partitioning attribute of a tuple. In this case, rather than writing the modified tuple back into the local fragment of the relation, the modified tuple is passed through a split table to determine which site should contain the tuple.</p>
<h1 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a><strong>Concurrency Control</strong></h1><p>Gamma uses a two-phase locking strategy to manage concurrency. This means that before accessing data, a process must first acquire locks (first phase), and then release the locks after completing its operations (second phase). This ensures that multiple operations do not modify the same data at the same time, preventing conflicts.</p>
<p>Gamma supports two levels of lock granularity: file-level and page-level (smaller scope). There are also five lock modes:</p>
<ul>
<li><strong>S (Shared) Lock</strong>: Allows multiple operations to read the data simultaneously.</li>
<li><strong>X (Exclusive) Lock</strong>: Only one operation can modify the data, while others must wait.</li>
<li><strong>IS, IX, and SIX Locks</strong>: Used to manage locking at larger scopes, such as entire files, allowing different combinations of read and write permissions.</li>
</ul>
<p>Each node in Gamma has its own lock manager and deadlock detector to handle local data locking. The lock manager maintains a lock table and a transaction wait-for-graph, which tracks which operations are waiting for which locks.</p>
<p>The cost of setting a lock depends on whether there is a conflict:</p>
<ul>
<li><strong>No Conflict</strong>: Takes about 100 instructions.</li>
<li><strong>With Conflict</strong>: Takes about 250 instructions because the system needs to check the wait-for-graph for deadlocks and suspend the requesting transaction using a semaphore mechanism.</li>
</ul>
<p>Gamma uses a centralized deadlock detection algorithm to handle deadlocks across nodes:</p>
<ul>
<li>Periodically (initially every second), the centralized deadlock detector requests each node’s local wait-for-graph.</li>
<li>If no deadlock is found, the detection interval is doubled (up to 60 seconds). If a deadlock is found, the interval is halved (down to 1 second).</li>
<li>The collected graphs are combined into a global wait-for-graph. If a cycle is detected in this global graph, it indicates a deadlock.</li>
</ul>
<p>When a deadlock is detected, the system will abort the transaction holding the fewest locks to free up resources quickly and allow other operations to proceed.</p>
<h1 id="Recovery-and-Log"><a href="#Recovery-and-Log" class="headerlink" title="Recovery and Log"></a>Recovery and Log</h1><p>Logging Changes:</p>
<p>When a record in the database is updated, Gamma creates a log record that notes the change. Each log record has a unique identifier called a Log Sequence Number (LSN), which includes a node number (determined when the system is set up) and a local sequence number (which keeps increasing). These log records are used for recovery if something goes wrong.</p>
<p>Log Management:</p>
<ul>
<li>The system sends log records from query processors to <strong>Log Managers</strong>, which are separate processors that organize the logs into a single stream.</li>
<li>If there are multiple Log Managers (M of them), a query processor sends its logs to one of them based on a simple formula: <strong>processor number mod M</strong>. This way, each query processor always sends its logs to the same Log Manager, making it easy to find logs later for recovery.</li>
</ul>
<p>Writing Logs to Disk:</p>
<ul>
<li>Once a “page” of log records is filled, it is saved to disk.</li>
<li>The Log Manager keeps a <strong>Flushed Log Table</strong>, which tracks the last log record written to disk for each node. This helps know which logs are safely stored.</li>
</ul>
<p>Writing Data to Disk (WAL Protocol):</p>
<ul>
<li>Before writing any changed data (a <strong>dirty page</strong>) to disk, the system checks if the corresponding log records have already been saved.</li>
<li>If the logs are saved, the data can be safely written to disk. If not, the system must first ensure the logs are written to disk before proceeding.</li>
<li>To avoid waiting too long for log confirmations, the system always tries to keep a certain number of <strong>clean buffer pages</strong> (unused pages) available.</li>
</ul>
<p>Commit and Abort Handling:</p>
<ul>
<li><strong>Commit</strong>: If a transaction completes successfully, the system sends a commit message to all the relevant Log Managers.</li>
<li><strong>Abort</strong>: If a transaction fails, an <strong>abort message</strong> is sent to all processors involved, and each processor retrieves its log records to undo the changes using the <strong>ARIES algorithm</strong>, which rolls back changes in the reverse order they occurred.</li>
</ul>
<p>Recovery Process:</p>
<ul>
<li>The system uses the <strong>ARIES algorithms</strong> for undoing changes, checkpointing, and restarting after a crash.</li>
<li><strong>Checkpointing</strong> helps the system know the most recent stable state, reducing the amount of work needed during recovery.</li>
</ul>
<h1 id="Dataflow-scheduling-technologies"><a href="#Dataflow-scheduling-technologies" class="headerlink" title="Dataflow scheduling technologies"></a>Dataflow scheduling technologies</h1><ol>
<li><strong>Data-Driven Execution Instead of Operator Control</strong>: Gamma’s dataflow scheduling lets data automatically move between operators, forming a pipeline. Each operator acts like a step on an assembly line: when data reaches the operator, it processes the data and then passes the processed results to the next operator.</li>
<li><strong>Reducing Coordination Overhead</strong>: Because of this dataflow design, the system does not need to frequently coordinate or synchronize the execution of each operator. This approach reduces the complexity and overhead of scheduling, especially when multiple operators are running in parallel, and avoids performance bottlenecks caused by waiting or synchronization.</li>
<li><strong>Inherent Support for Parallelism</strong>: Dataflow scheduling is well-suited for parallel processing because data can flow between multiple operators at the same time. For example, a query can simultaneously perform scanning, joining, and aggregation across different processors. Each operator can independently process the data it receives without waiting for other operators to finish, allowing the system to efficiently utilize the computational power of multiple processors.</li>
<li><strong>Adaptability to Dynamic Environments</strong>: During query execution, dataflow scheduling can be adjusted based on the actual system load and data characteristics. This flexibility allows the system to dynamically optimize the performance of query execution, especially for large and complex queries, by better adapting to changing query demands and system conditions.</li>
</ol>
<p>Gamma’s unique dataflow scheduling techniques allow data to flow naturally between operators, reducing the need for direct control over operations. This significantly lowers coordination overhead in multi-processor environments, enhances the system’s parallel processing capabilities, and improves the efficiency of executing complex queries.</p>
<p>In Gamma’s dataflow scheduling techniques, parallelism is extensively used to improve query execution efficiency. Here’s where and how parallelism is applied:</p>
<ol>
<li><p><strong>Parallel Execution of Operators</strong>: Queries often involve multiple operators (e.g., scan, filter, join, aggregation). With dataflow scheduling, these operators can run in parallel:</p>
<ul>
<li><p><strong>Scan and Filter in Parallel</strong>: While one processor scans a data block, another processor can be filtering the data from previous blocks.</p>
</li>
<li><p><strong>Parallel Joins</strong>: If a join operation involves large datasets distributed across different nodes, Gamma can perform the join operation on these different parts of the data simultaneously. The result of the join is computed in parallel across multiple processors.</p>
</li>
</ul>
</li>
<li><p><strong>Data Partitioning for Parallelism</strong>: The relations (data tables) are often partitioned across multiple processors in Gamma. This means that different processors can work on different partitions of the data at the same time. For example:</p>
<ul>
<li><p><strong>Partitioned Hash Joins</strong>: Data can be split into “buckets” based on a hash function, and different processors can handle the join for different buckets simultaneously.</p>
</li>
<li><p><strong>Parallel Aggregation</strong>: When computing aggregate functions (e.g., sum or average), each processor calculates a partial result for its own partition of the data, and these partial results are later combined.</p>
</li>
</ul>
</li>
</ol>
<p>In summary, parallelism in Gamma is achieved through:</p>
<ul>
<li>Distributing query operators across multiple processors.</li>
<li>Partitioning data so different processors work on different sections simultaneously.</li>
<li>Enabling multiple stages of query execution (e.g., scanning, filtering, joining) to happen concurrently.</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>What is a fragment or a shard in Gamma?</strong> </p>
<p>A fragment or shard refers to a portion of a database relation that is horizontally partitioned across multiple disk drives.</p>
<p><strong>How does a Gamma operator know where to send its stream of records?</strong> </p>
<p>There is a structure called split table to determine where each tuple should be sent, based on the values of tuples.</p>
<p><strong>With interleaved declusttering, why not use a cluster size that includes all nodes in the system?</strong></p>
<p>If an interleaved declustteing system includes all nodes, it will become more vulnerable to failures. The failure of any two nodes could make the data inaccessible. A smaller cluster will limits the risk of complete data unavailability and balance the load.</p>
<p><strong>Hash-join is appropriate for processing equi-join predicates (Emp.dno &#x3D; Dept.dno). How can Gamma process nonequi-join predicates (Emp.Sal &gt; Dept.dno*1000) in a pipelined manner?</strong></p>
<p><strong>Range partitioning</strong>: Pre-partition the data based on ranges of values to reduce the search space.</p>
<p><strong>Broadcast join</strong>: When the smaller relation is broadcasted to all nodes, and then each node evaluates the nonequi-join predicate in parallel.</p>
<p><strong>Nested-loop join</strong>: Use a nested-loop join strategy where each tuple from one relation is compared against all tuples from the other relation.</p>
<p><strong>What is the difference between Gamma, Google MapReduce, Microsoft Dryad and Apache Flink?</strong></p>
<table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th>Gamma</th>
<th>MapReduce</th>
<th>Dryad</th>
<th>Flink</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Primary Use</strong></td>
<td>Parallel database queries</td>
<td>Batch processing</td>
<td>Graph-based parallel computation</td>
<td>Stream and batch processing</td>
</tr>
<tr>
<td><strong>Architecture</strong></td>
<td>Shared-nothing, partitioned data</td>
<td>Cluster-based, distributed</td>
<td>DAG of tasks</td>
<td>Distributed, supports DAG</td>
</tr>
<tr>
<td><strong>Data Model</strong></td>
<td>Relational operations (SQL-like)</td>
<td>Key-value pairs</td>
<td>Data flow in DAG</td>
<td>Stream processing with state</td>
</tr>
<tr>
<td><strong>Partitioning</strong></td>
<td>Horizontal partitioning</td>
<td>Data split into chunks</td>
<td>Data partitioned across graph</td>
<td>Data partitioned into streams</td>
</tr>
<tr>
<td><strong>Fault Tolerance</strong></td>
<td>Limited</td>
<td>Checkpointing</td>
<td>Task-level recovery</td>
<td>State snapshots, exactly-once</td>
</tr>
<tr>
<td><strong>Programming</strong></td>
<td>Relational (SQL-style)</td>
<td>Functional (Map&#x2F;Reduce)</td>
<td>Sequential tasks in DAG</td>
<td>Functional, stream APIs</td>
</tr>
<tr>
<td><strong>Scalability</strong></td>
<td>Hundreds of processors</td>
<td>Horizontally across many nodes</td>
<td>Scales with more nodes</td>
<td>Highly scalable, stream and batch</td>
</tr>
<tr>
<td><strong>Use Case</strong></td>
<td>Database query processing</td>
<td>Log processing, data aggregation</td>
<td>Scientific computing</td>
<td>Real-time analytics, event processing</td>
</tr>
</tbody></table>
<p><strong>Will a version of Gamma using FLOW be more modular than its current design?</strong></p>
<p>Yes. FLOW enables more fine-grained control over the data flow and process interactions, which could simplify the addition of new operators and functionalities. It would also make the system easier to maintain and extend, as each component could be developed and optimized independently.</p>
<p>Reference: <a href="https://pages.cs.wisc.edu/~dewitt/includes/paralleldb/ieee90.pdf">https://pages.cs.wisc.edu/~dewitt/includes/paralleldb/ieee90.pdf</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>IQ</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/08/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/IQ/</url>
    <content><![CDATA[<h1 id="What-is-the-IQ-Framework"><a href="#What-is-the-IQ-Framework" class="headerlink" title="What is the IQ Framework?"></a>What is the IQ Framework?</h1><p>The IQ framework is a solution designed for Cache-Augmented SQL (CASQL) systems, which combine relational databases (RDBMS) and key-value stores (KVS) to boost performance by caching database query results. However, CASQL systems often face challenges related to stale data and race conditions. The IQ framework ensures strong consistency while maintaining high performance.</p>
<h1 id="Challenges-in-CASQL-Systems"><a href="#Challenges-in-CASQL-Systems" class="headerlink" title="Challenges in CASQL Systems"></a>Challenges in CASQL Systems</h1><ol>
<li><p><strong>Stale Data in Cache</strong>:</p>
<ul>
<li><p>Cached data in the KVS can become outdated if updates to the RDBMS are not properly synchronized.</p>
</li>
<li><p>For example, if a record in the database is modified, but the corresponding cache entry isn’t updated, subsequent reads might return incorrect values.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Issues</strong>:</p>
<ul>
<li><p>Multiple sessions accessing and modifying the same key in KVS concurrently can lead to inconsistent results.</p>
</li>
<li><p>Example:</p>
<ul>
<li>One session updates a value while another session modifies it based on outdated data.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>RDBMS and Cache Coordination</strong>:</p>
<ul>
<li>While RDBMS ensures transactional consistency, KVS often lacks this capability, making it difficult to synchronize their states.</li>
</ul>
</li>
</ol>
<h1 id="Key-Features-of-the-IQ-Framework"><a href="#Key-Features-of-the-IQ-Framework" class="headerlink" title="Key Features of the IQ Framework"></a>Key Features of the IQ Framework</h1><ol>
<li><strong>Lease Mechanism: Inhibit (I) and Quarantine (Q)</strong>:<ol>
<li><strong>I Lease</strong> (for reads):<ol>
<li>Ensures that only one session can query the RDBMS for a cache miss and update the KVS.</li>
<li>Other sessions attempting to read the same key must “back off” and wait.</li>
</ol>
</li>
<li><strong>Q Lease</strong> (for writes):<ol>
<li>Required for modifying, deleting, or incrementally updating keys in the KVS.</li>
<li>If an I lease exists, the Q lease invalidates it to ensure the write operation’s integrity.</li>
<li>The KVS ignores I’s write operation because this I lease is no longer valid.</li>
</ol>
</li>
</ol>
</li>
<li><strong>Lease Expiry</strong>:<ol>
<li>A lease for a key has a fixed life time and is granted to one KVS connection (thread) at a time.</li>
<li>Expired leases are automatically released, ensuring system availability.</li>
<li>The finite life time enables the KVS to release the lease and continue processing operations in the presence of node failures hosting the application.</li>
</ol>
</li>
<li><strong>Session-based Model</strong>:<ol>
<li>The framework operates through sessions, similar to the <strong>two-phase locking protocol</strong>.</li>
<li>Leases can be acquired either before or during an RDBMS transaction, providing flexibility.</li>
</ol>
</li>
</ol>
<h2 id="Implementing-ACID-Properties"><a href="#Implementing-ACID-Properties" class="headerlink" title="Implementing ACID Properties"></a>Implementing ACID Properties</h2><p>原子性 (Atomicity)： IQ 框架确保事务的操作同时在数据库 (RDBMS) 和缓存 (KVS) 中执行。也就是说，操作不会只在数据库中完成而没有更新缓存。这种设计假设 KVS 中的数据是 RDBMS 数据的一部分，因此如果遇到问题，可以直接删除 KVS 中的数据来保持一致。</p>
<p>一致性 (Consistency)： IQ 框架保证事务在数据库和缓存中的数据状态从一个有效状态变为另一个有效状态。如果数据库的事务回滚 (abort)，那么缓存中的操作也不会被应用，确保不会留下无效的缓存数据。</p>
<p>隔离性 (Isolation)： 即使有多个会话 (session) 同时执行，IQ 框架也让每个会话看起来像是独立执行的，避免了并发问题。例如，即使两个用户同时读写相同的数据，他们看到的结果也是正确且一致的。</p>
<p>持久性 (Durability)： 持久性是由数据库 (RDBMS) 提供的，而缓存 (KVS) 则作为数据库的一部分镜像。KVS 存储的数据是在内存中的副本，但一旦数据库中的事务提交，数据就会被持久保存。</p>
<blockquote>
<p>CAS 操作只能保证单一操作的原子性，但无法在多个并发会话中保证强一致性。 由于数据库和缓存系统中的操作顺序可能不一致，会导致数据不同步。</p>
<p>在并发场景下：CAS 无法感知其他会话在其读取后对数据的更改。 多个会话同时执行 CAS 操作时，可能导致更新丢失或顺序混乱，如本例中 S2 的更新被 S1 覆盖。</p>
<p>Q 租约用于写操作，确保某一时刻只有一个会话能够修改目标键值。如果某个键值已有 Q 租约，其他会话（如 S1）会被要求退避（back off）或中止操作。</p>
</blockquote>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/RDBMS_KVS_OPER.png" alt="img"></p>
<h1 id="Invalidate"><a href="#Invalidate" class="headerlink" title="Invalidate"></a>Invalidate</h1><h2 id="What-is-Snapshot-Isolation"><a href="#What-is-Snapshot-Isolation" class="headerlink" title="What is Snapshot Isolation?"></a>What is Snapshot Isolation?</h2><p>Snapshot isolation is a multi-version concurrency control mechanism commonly used in RDBMS to allow concurrent transactions to execute efficiently. It guarantees:</p>
<ol>
<li><strong>Consistent Snapshot</strong>: All reads in a transaction observe the same consistent state of the database, as it existed at the transaction’s start.</li>
<li><strong>Conflict Detection</strong>: A transaction can only commit if its updates do not conflict with updates made by other transactions since its snapshot was taken.</li>
</ol>
<h3 id="The-Problem"><a href="#The-Problem" class="headerlink" title="The Problem"></a>The Problem</h3><p>Snapshot isolation can cause a race condition between a write session (S1) and a read session (S2) when KVS is involved. The issue unfolds as follows:</p>
<ol>
<li><p><strong>Write Session (S1)</strong>:</p>
<ul>
<li><p>S1 modifies the RDBMS and triggers a delete operation in the KVS to invalidate outdated key-value pairs.</p>
</li>
<li><p>S1 commits the transaction after completing its changes in the RDBMS.</p>
</li>
</ul>
</li>
<li><p><strong>Read Session (S2)</strong>:</p>
<ul>
<li><p>S2 starts after S1’s delete operation in the KVS. It observes a <strong>KVS miss</strong> for a key-value pair because S1 has invalidated it.</p>
</li>
<li><p>S2 queries the RDBMS to recompute the key-value pair. However, because snapshot isolation allows S2 to read an <strong>older snapshot of the database</strong>, it retrieves outdated (stale) data.</p>
</li>
<li><p>S2 inserts this <strong>stale data</strong> back into the KVS before S1 commits its changes to the RDBMS.</p>
</li>
</ul>
</li>
<li><p><strong>Inconsistency</strong>:</p>
<ul>
<li>After both sessions complete, the KVS contains a stale key-value pair inconsistent with the RDBMS, leading to incorrect results for future reads.</li>
</ul>
</li>
</ol>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/invalidate_tbl.png" alt="img"></p>
<p>I Lease (Inhibit Lease):</p>
<ul>
<li>Used by <strong>read sessions</strong> (e.g., S2).</li>
<li>When a read session observes a <strong>KVS miss</strong>, it requests an I lease for the key (<code>k_j</code>) from the KVS server.</li>
<li>The I lease allows the read session to query the RDBMS, compute a value, and insert the computed key-value pair into the KVS.</li>
<li>If a Q lease is already in place, the I lease is denied, and the read session is told to <strong>back off</strong> and retry later.</li>
</ul>
<p>Q Lease (Quarantine Lease):</p>
<ul>
<li>Used by <strong>write sessions</strong> (e.g., S1).</li>
<li>When a write session plans to invalidate a key in the KVS, it requests a Q lease for the key (<code>k_j</code>).</li>
<li>The Q lease prevents other sessions (including those holding I leases) from modifying or inserting the key in the KVS.</li>
<li>Multiple Q leases can be granted for the same key since deleting a key is idempotent (doesn’t create conflicts).</li>
</ul>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><h2 id="The-Problem-1"><a href="#The-Problem-1" class="headerlink" title="The Problem"></a>The Problem</h2><ul>
<li>In the original scenario, <strong>write sessions (e.g., S1)</strong> immediately delete key-value pairs in the KVS as soon as they acquire a Q lease (e.g., Step 1.3 in Figure 3).</li>
<li>This can cause <strong>read sessions (e.g., S2)</strong> to encounter KVS misses, triggering redundant operations like querying the RDBMS, recalculating values, and reinserting them into the KVS.</li>
</ul>
<h2 id="The-Proposed-Optimization"><a href="#The-Proposed-Optimization" class="headerlink" title="The Proposed Optimization"></a>The Proposed Optimization</h2><p><strong>Deferring Key Deletion Until Write Commit</strong></p>
<ol>
<li><p><strong>Key Changes</strong>:</p>
<ul>
<li><p>Instead of deleting the key immediately in Step 1.3, the write session (S1) holds the Q lease and <strong>defers the deletion</strong> until the write session commits (Step 1.5).</p>
</li>
<li><p>While S1 is mid-flight, the invalidated key-value pair remains in the KVS for other read sessions (S2) to observe.</p>
</li>
</ul>
</li>
<li><p><strong>Handling KVS Hits</strong>:</p>
<ul>
<li><p>Read sessions like S2 that encounter a <strong>KVS hit</strong> consume the “stale” key-value pair, treating it as valid.</p>
</li>
<li><p>This is acceptable because S2’s actions can be <strong>serialized to occur before</strong> S1, which is still in progress and has not yet committed its RDBMS changes.</p>
</li>
</ul>
</li>
<li><p><strong>Handling Write Aborts</strong>:</p>
<ul>
<li><p>If a write session (S1) encounters an exception and aborts, the Q lease is released without deleting the key.</p>
</li>
<li><p>The current key-value pair in the KVS remains valid and accessible to other sessions.</p>
</li>
</ul>
</li>
</ol>
<h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><ol>
<li><p><strong>Versioning Concept</strong>:</p>
<ul>
<li><p>The optimization can be conceptualized as maintaining a <strong>temporary version</strong> of the key-value pair for use by all sessions except the one currently invalidating it (S1).</p>
</li>
<li><p>Once S1 commits, the temporary version is removed.</p>
</li>
</ul>
</li>
<li><p><strong>Abort Command</strong>:</p>
<ul>
<li><p>If a write session (S1) aborts due to constraints or exceptions, an <strong>abort command</strong> releases all Q leases held by S1 without deleting the key-value pair.</p>
</li>
<li><p>Without this command, Q leases would expire naturally after a timeout, during which no other session could modify or access the key.</p>
</li>
</ul>
</li>
</ol>
<p><strong>Re-Arrangement Window</strong>:</p>
<ul>
<li>With this optimization, S2 and S1 can be <strong>re-arranged</strong> in a serializable schedule where S2 logically occurs before S1.</li>
<li>Without the optimization, the re-arrangement window shrinks to zero because S2 would have already queried the RDBMS for stale data, violating consistency.</li>
</ul>
<h1 id="Refresh-and-Incremental-Update"><a href="#Refresh-and-Incremental-Update" class="headerlink" title="Refresh and Incremental Update"></a>Refresh and Incremental Update</h1><h2 id="Key-Issues-with-Compare-and-Swap-CAS"><a href="#Key-Issues-with-Compare-and-Swap-CAS" class="headerlink" title="Key Issues with Compare-and-Swap (CAS)"></a>Key Issues with Compare-and-Swap (CAS)</h2><ul>
<li><p><strong>CAS Limitation</strong>:</p>
<ul>
<li>CAS alone cannot ensure strong consistency. It provides atomic updates to a single key-value pair but does not coordinate these updates with RDBMS transactions.</li>
</ul>
</li>
<li><p><strong>Example (Figure 2)</strong>:</p>
<ul>
<li><p>KVS writes can occur either:</p>
<ol>
<li><p><strong>Prior to</strong> the RDBMS transaction, or</p>
</li>
<li><p><strong>As part of</strong> the RDBMS transaction.</p>
</li>
</ol>
</li>
<li><p><strong>Problem</strong>: If the RDBMS transaction aborts, the KVS will retain the modified key-value pair, potentially exposing <strong>dirty reads</strong> to other sessions.</p>
</li>
</ul>
</li>
<li><p><strong>Figure 6 (Dirty Read Problem)</strong>:</p>
<ul>
<li>Write session S1 modifies a key-value pair in KVS.</li>
<li>S1’s transaction later aborts, but the intermediate KVS value is consumed by a read session S2 before the rollback, leading to inconsistencies.</li>
</ul>
</li>
<li><p><strong>Developer Responsibility</strong>:</p>
<ul>
<li>Without additional mechanisms, developers must implement complex logic to restore KVS key-value pairs to their original values when RDBMS transactions abort.</li>
</ul>
</li>
</ul>
<h2 id="Race-Conditions-with-Incremental-Updates-δ-Operations"><a href="#Race-Conditions-with-Incremental-Updates-δ-Operations" class="headerlink" title="Race Conditions with Incremental Updates (δ Operations)"></a>Race Conditions with Incremental Updates (δ Operations)</h2><ul>
<li><p><strong>Figure 7 (Snapshot Isolation with δ Operations)</strong>:</p>
</li>
<li><ul>
<li>Write session S1 updates the RDBMS and KVS using an incremental update (e.g., appending to a value).</li>
<li>Concurrently, read session S2 queries the RDBMS and overwrites the key-value pair in the KVS.</li>
<li><strong>Result</strong>: The KVS reflects inconsistent state, as S2’s overwrite may invalidate S1’s incremental change.</li>
</ul>
</li>
<li><p><strong>Figure 8 (Reordering KVS Operations)</strong>:</p>
</li>
<li><ul>
<li>Delaying KVS updates until after the RDBMS transaction doesn’t solve the problem.</li>
</ul>
</li>
<li><p><strong>Example in Figure 8</strong>:</p>
</li>
<li><ul>
<li>S1 appends a change to a value based on its RDBMS view.</li>
<li>S2 modifies the RDBMS during S1’s execution, which S1 unknowingly incorporates into its KVS update.</li>
<li><strong>Problem</strong>: S2’s modifications are reflected twice in the KVS, introducing inconsistencies.</li>
</ul>
</li>
</ul>
<h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h2><p><strong>Key Concepts in the Solution</strong></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/refresh_tbl.png" alt="img"></p>
<ol>
<li><p><strong>Q Leases for Write Sessions</strong>:</p>
<ul>
<li><p>A <strong>Q lease</strong> must be obtained for each key-value pair that a session intends to update.</p>
</li>
<li><p>This prevents race conditions by locking the key-value pair until the session completes its operations.</p>
</li>
</ul>
</li>
<li><p><strong>Steps for Write Sessions</strong>:</p>
<ul>
<li><p><strong>Step 1</strong>: Obtain Q leases for the keys to be updated before committing the RDBMS transaction. This can happen:</p>
<ul>
<li><p>Before starting the RDBMS transaction.</p>
</li>
<li><p>As part of the RDBMS transaction.</p>
</li>
</ul>
</li>
<li><p><strong>Step 2</strong>: Write the updated key-value pairs to the KVS after committing the RDBMS transaction.</p>
</li>
<li><p><strong>Step 3</strong>: Release the Q leases once the KVS is updated.</p>
</li>
<li><p><strong>Automatic Cleanup</strong>: If a Q lease expires, the KVS deletes the associated key-value pair to avoid stale data.</p>
</li>
</ul>
</li>
<li><p><strong>Command Design for Write Operations</strong>:</p>
<ul>
<li><p><strong>QaRead (Quarantine-and-Read)</strong>:</p>
<ul>
<li><p>Acquires a Q lease on the referenced key and reads its value from the KVS.</p>
</li>
<li><p>If a Q lease for the same key is already held by another session, the requesting session receives an <strong>abort message</strong>, must roll back its RDBMS transaction, release all leases, back off, and retry later.</p>
</li>
<li><p>If no value exists in the KVS (a <strong>KVS miss</strong>), the application can:</p>
<ul>
<li>Skip updating the key, or</li>
<li>Query the RDBMS, compute a new value, and insert it using <strong>SaR</strong> (below).</li>
</ul>
</li>
<li><p>If a <strong>QaRead lease</strong> encounters an <strong>I lease</strong> held by a read session, it invalidates the I lease to prevent race conditions.</p>
</li>
</ul>
</li>
<li><p><strong>SaR (Swap-and-Release)</strong>:</p>
<ul>
<li>Updates the value of a key in the KVS with the new value and releases the Q lease.</li>
<li>If the new value is <code>null</code>, the Q lease is simply released without updating the KVS.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Handling-Race-Conditions"><a href="#Handling-Race-Conditions" class="headerlink" title="Handling Race Conditions"></a>Handling Race Conditions</h2><ol>
<li><p><strong>Q Leases for Concurrent Write Sessions</strong>:</p>
<ul>
<li><p>If two write sessions request Q leases for the same key, the KVS resolves the conflict by:</p>
<ul>
<li>Aborting one session.</li>
<li>Ensuring the aborted session retries later, serializing its updates after the session holding the Q lease.</li>
</ul>
</li>
<li><p>This guarantees a valid serial schedule in the RDBMS and KVS.</p>
</li>
</ul>
</li>
<li><p><strong>Read Sessions and I Leases</strong>:</p>
<ul>
<li><p>Read sessions use <strong>I leases</strong> to avoid race conditions when querying the KVS.</p>
</li>
<li><p>If a write session issues a <strong>QaRead</strong> that encounters an existing <strong>I lease</strong>, the <strong>I lease</strong> is invalidated to ensure the KVS reflects the latest updates from the RDBMS.</p>
</li>
</ul>
</li>
</ol>
<h2 id="Integration-with-Two-Phase-Locking"><a href="#Integration-with-Two-Phase-Locking" class="headerlink" title="Integration with Two-Phase Locking"></a>Integration with Two-Phase Locking</h2><ul>
<li><p>The Q lease mechanism resembles <strong>two-phase locking</strong>:</p>
<ol>
<li><p><strong>Growing Phase</strong>: The session acquires all necessary Q leases using <strong>QaRead</strong> before committing its RDBMS transaction.</p>
</li>
<li><p><strong>Shrinking Phase</strong>: The session releases all Q leases using <strong>SaR</strong> after committing its RDBMS transaction.</p>
</li>
</ol>
</li>
<li><p>Flexibility:</p>
<ul>
<li>A session can issue <strong>QaRead</strong> commands either before starting the RDBMS transaction or as part of the transaction.</li>
</ul>
</li>
</ul>
<h2 id="Key-Concepts-of-Incremental-Updates"><a href="#Key-Concepts-of-Incremental-Updates" class="headerlink" title="Key Concepts of Incremental Updates"></a>Key Concepts of Incremental Updates</h2><ol>
<li><p><strong>Incremental Update Command: IQ-δ</strong>:</p>
<ul>
<li><p><strong>Purpose</strong>: Allows a write session to perform an incremental update, such as appending data to an existing key-value pair.</p>
</li>
<li><p><strong>Syntax</strong>: <code>IQ-δ(ki, δi)</code></p>
<ul>
<li><code>ki</code>: The key to be updated.</li>
<li><code>δi</code>: The incremental change to apply (e.g., the value to append).</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Similarities to QaRead</strong>:</p>
<ul>
<li><p><strong>Q Lease Requirement</strong>: Before issuing the <code>IQ-δ</code> command, the session must obtain a <strong>Q lease</strong> for the key <code>ki</code> to ensure exclusive access.</p>
</li>
<li><p><strong>Abort on Conflict</strong>:</p>
<ul>
<li><p>If another session already holds a Q lease on the same key (<code>ki</code>), the <strong>KVS returns an abort message</strong>.</p>
</li>
<li><p>The write session must:</p>
<ol>
<li><p>Release all its leases.</p>
</li>
<li><p>Abort its ongoing RDBMS transaction (if any).</p>
</li>
<li><p>Retry the operation later.</p>
</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="优化关键点总结"><a href="#优化关键点总结" class="headerlink" title="优化关键点总结"></a>优化关键点总结</h1><ol>
<li><p><strong>保留旧版本（Older Version）</strong>：</p>
<ul>
<li><p>当写会话（S1）更新某键值对 (<code>ki-vi</code>) 时，KVS 暂时保留该键值对的旧版本 (<code>ki-vi_old</code>)，直到 S1 提交。</p>
</li>
<li><p>这避免了读会话在写会话更新期间遇到 <strong>KVS miss</strong>。</p>
</li>
</ul>
</li>
<li><p>写会话的更新视图：</p>
<ul>
<li><p>写会话（S1）在更新期间必须能够看到自己的修改结果（<code>ki-vi_new</code>）。</p>
</li>
<li><p>KVS 确保为 S1 提供其最新的更新视图。</p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Why is it acceptable for invalidate to delete cache entries?</strong></p>
<p>Consistency Assurance: The cache entry being invalidated represents stale data that is no longer consistent with the current state of the RDBMS. Deleting it prevents read sessions from accessing outdated information.</p>
<p><strong>How is a lease different than a lock?</strong> </p>
<ul>
<li><strong>Lease</strong>: Has a fixed lifetime and expires automatically after a certain duration. This makes leases useful in distributed systems where failures or delays could otherwise cause indefinite blocking.</li>
<li><strong>Lock</strong>: Typically remains active until explicitly released, which can lead to deadlocks or indefinite resource contention if not managed properly.</li>
</ul>
<p><strong>True or False: IQ leases require changes to the RDBMS software.</strong></p>
<p>False:</p>
<p>IQ leases do not require changes to the RDBMS software.</p>
<p>Instead, they extend the functionality of the Key-Value Store (KVS) by introducing new lease-based commands (e.g., <code>QaRead</code> and <code>SaR</code>) to coordinate operations between the KVS and the RDBMS. This design leverages existing RDBMS features without altering its underlying implementation.</p>
<p><strong>What factors does CAMP consider when selecting a victim?</strong></p>
<p>H(p) &#x3D; L + size(p) &#x2F; cost(p)</p>
<p><strong>What is the definition of cost? Provide an example.</strong></p>
<ul>
<li><strong>Computation Time</strong>: The time required to regenerate or recompute the data if it is evicted from memory.</li>
<li><strong>Access Latency</strong>: The time it would take to fetch the data from disk or another slower storage tier.</li>
<li><strong>Importance</strong>: The priority or weight assigned to the data based on how frequently or critically it is used.</li>
</ul>
<p><strong>How does CAMP insert a key-value pair in memory?</strong></p>
<p>When a new key-value pair p needs to be inserted into memory, CAMP performs the following steps:</p>
<p><strong>1. Check Cache Capacity</strong></p>
<ul>
<li>If there is <strong>enough memory</strong> to store the new key-value pair:</li>
<li><ul>
<li>The pair is inserted directly into the appropriate <strong>priority group</strong> based on its cost-to-size ratio.</li>
<li>L is not updated.</li>
</ul>
</li>
<li>If the cache is <strong>full</strong>:</li>
<li><ul>
<li>CAMP selects one or more key-value pairs to <strong>evict</strong> based on their H(p) values.</li>
<li>It removes the pair(s) with the <strong>lowest H(p)</strong> values until there is sufficient space for the new pair.</li>
</ul>
</li>
</ul>
<p><strong>2. Insert the New Pair</strong></p>
<ul>
<li>The new key-value pair p is added to the cache, and its H(p) value is computed and recorded.</li>
<li>The pair is placed in the appropriate priority queue based on its cost-to-size ratio.</li>
</ul>
<p><strong>With BG, what is the definition of Service Level Agreement, SLA?</strong></p>
<p>SLA, e.g., 95% of requests to observe a response time equal to or faster than 100 msec with at most 0.1% of requests observing unpredictable data for 10 minutes.</p>
<p><strong>Name one reason why a system may produce unpredictable data?</strong></p>
<p>Eventual consistency. Or multiple threads are updating the same data item.</p>
<p>Reference: <a href="https://dl.acm.org/doi/abs/10.1145/2663165.2663318">https://dl.acm.org/doi/abs/10.1145/2663165.2663318</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>Nova-LSM</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/09/20/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/Nova-LSM/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>LSM-Tree（Log-Structured Merge Tree）的核心思想是将大量的随机写入转换为更高效的顺序写入。简单来说，它通过以下方式来实现：</p>
<ol>
<li><strong>写入内存</strong>：当有新的数据写入时，LSM-Tree首先将这些数据存储在内存中的缓冲区（称为MemTable）。这是一个有序的结构，数据按键排序。</li>
<li><strong>批量写入磁盘</strong>：当内存中的数据积累到一定程度时，整个MemTable会被一次性地写入磁盘，这个过程是<strong>顺序写入</strong>，非常高效。写入磁盘后，这个数据成为一个不可修改的文件，称为SSTable（Sorted String Table）。</li>
<li><strong>合并和压缩</strong>：随着时间的推移，磁盘上会产生多个SSTable。为了优化读取性能，系统会周期性地将这些SSTable进行合并和压缩，使得数据保持有序并减少冗余。</li>
</ol>
<p>这样，LSM-Tree通过将频繁的随机写操作缓存在内存中，最后批量顺序写入磁盘，大大提高了写入性能。这种方式适合写入密集型的工作负载，同时还能保证数据查询的效率。</p>
<p><strong>LSM-Tree的基础结构</strong>，特别是数据如何从内存（memtable）移动到磁盘，并经过多级的归并排序（compaction）过程来进行存储。</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/nova_lsm_basic_structure.png" alt="img"></p>
<ol>
<li><p>MemTable（内存表）</p>
<ul>
<li><p>数据的写入首先进入到内存中的memtable，通常是一个有序的数据结构（比如跳表或B+树），这使得数据在内存中是有序的，便于快速写入和查询。</p>
</li>
<li><p>当memtable满了或者系统需要将数据持久化时，memtable中的数据会被flush（刷新）到磁盘，形成第一层的SSTable。</p>
</li>
</ul>
</li>
<li><p>Level-0（磁盘上的第一层）</p>
<ul>
<li><p>数据从内存写入磁盘后，存储在Level-0层的SSTable中。此时，SSTable的数据顺序与memtable一致，但可能存在多个SSTable，且它们之间的键值范围可能重叠。</p>
</li>
<li><p>Level-0的SSTable是逐渐积累的，并不会自动排序或整理，直到执行compaction（归并操作）。</p>
</li>
</ul>
</li>
<li><p>Compaction（归并操作）</p>
<ul>
<li><p>当Level-0层的数据达到一定量时，系统会执行归并操作，将Level-0层的多个SSTable合并，并将合并后的有序数据移到Level-1层。</p>
</li>
<li><p>Level-1开始，所有的SSTable都是有序且互不重叠的。也就是说，每个SSTable都有自己独立的键值范围，不会与其他SSTable的键值范围重叠，这使得查询时能够快速定位到目标SSTable。</p>
</li>
</ul>
</li>
<li><p>逐级沉降</p>
<ul>
<li><p>数据会随着系统运行，从Level-0层逐步沉降到更深的层级（如Level-1、Level-2等）。在每一层，数据都通过归并操作变得更加有序且结构紧凑。</p>
</li>
<li><p>每次合并后，数据被重新整理，分配到新的不重叠的SSTable中，从而保持物理上的键值有序性。</p>
</li>
</ul>
</li>
</ol>
<p><strong>LSM-Tree查询</strong></p>
<p>基于LSM-Tree的查询可分为点查与范围查询两大类，对应的执行方式如下：</p>
<ul>
<li>点查（point lookup）：从上往下进行查询，先查memtable，再到L0层、L1层。因为上层的数据永远比下层版本新，所以在第一次发生匹配后就会停止查询。</li>
<li>范围查询（range lookup）：每一层都会找到一个匹配数据项的范围，再将该范围进行<strong>多路归并</strong>，归并过程中同一key只会保留最新版本。</li>
</ul>
<p><strong>LSM-Tree性能的衡量</strong>主要考虑三个因素：空间放大、读放大和写放大。</p>
<p>一是空间放大（space amplification）。LSM-Tree的所有写操作都是顺序追加写，对数据的更新并不会立即反映到数据既有的值里，而是通过分配新的空间来存储新的值，即out-place update。因此冗余的数据或数据的多版本，仍会在LSM-Tree系统里存在一定时间。这种实际的占用空间大于数据本身的现象我们称之为空间放大。因为空间有限，为了减少空间放大，LSM-Tree会从L1往L2、L3、L4不断做compaction，以此来清理过期的数据以及不同数据的旧版本，从而将空间释放出来。</p>
<p>二是读放大（read amplification）。假设数据本身的大小为1k，由于存储结构的设计，它所读到的值会触发多次IO操作，一次IO意味着一条读请求，这时它所读取到的则是在后端所需要做大的磁盘读的实际量，已经远大于目标数据本身的大小，从而影响到了读性能。这种现象我们称之为读放大。为了减轻读放大，LSM-Tree采用布隆过滤器来避免读取不包括查询键值的SST文件。</p>
<p>三是写放大（write amplification）。在每层进行compaction时，我们会对多个SST文件进行反复读取再进行归并排序，在删掉数据的旧版本后，再写入新的SST文件。从效果上看，每条key在存储系统里可能会被多次写入，相当于一条key在每层都会写入一次，由此带来的IO性能损失即写放大。</p>
<p>LSM-Tree最初的理念是用空间放大和读放大来换取写放大的降低，从而实现较好的写性能，但也需要做好三者的平衡。以下是两种假设的极端情况。</p>
<p>第一种极端情况是：如果完全不做compaction，LSM-Tree基本等同于log文件，当memtable不断刷下来时，由于不做compaction，只做L0层的文件，这时如果要读一条key，读性能会非常差。因为如果在memtable里找不到该条key，就要去扫描所有的SST文件，但与此同时写放大现象也将不存在。</p>
<p>第二种极端情况是：如果compaction操作做到极致，实现所有数据全局有序，此时读性能最优。因为只需要读一个文件且该文件处于有序状态，在读取时可以很快找到对应的key。但要达到这种效果，需要做非常多的compaction操作，要不断地把需要删的SST文件读取合并再来写入，这会导致非常严重的写放大。</p>
<h1 id="Nova-LSM架构设计"><a href="#Nova-LSM架构设计" class="headerlink" title="Nova-LSM架构设计"></a>Nova-LSM架构设计</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/nova_lsm_arch.png" alt="img"></p>
<p>第一部分是写日志的组件，将WAL写成功后再往LSM-Tree的memtable中查询新的数据。</p>
<p>第二部分是本身处理LSM-Tree写入的线程，其缩写为LTC(LSM-Tree Component)，代表着将该线程单独组件化。</p>
<p>第三部分则是底层的存储，负责把接收到的上层LTC组件下发下来，并提供标准的文件接口。</p>
<p><strong>Nova-LSM所解决的核心问题</strong></p>
<p>第一个核心问题是：基于LSM-Tree结构的存储系统，例如LevelDB、RocksDB等，都会不可避免地遇到缓写或者停写的问题。比如内存里的memtable，在配置时最多可以写8个，因为写入多，需要全部flush到磁盘上。与此同时，当前L0层的SST文件非常多，L0层即将开始做compaction。但compaction会涉及到磁盘IO，在还没做完时，就会阻塞内存中的memtable对L0层SST进行flush的过程。当flush无法进行时，就会发生缓写，随着阈值的推进，在实在写不进时甚至会停写，这种现象体现在客户端就是请求掉零。</p>
<p>为了解决LSM-Tree结构存储系统中的缓写、停写问题，该文章提出了两个解决办法：</p>
<ul>
<li>第一种方法是设计了<strong>存算分离</strong>的架构体系，具体如上图所示。该架构的重要作用之一，是把处理写入和处理磁盘IO的两大主力模块拆分，计算存储分离，<strong>哪个部分慢就为哪个部分增加节点</strong>以此来提高该部分的能力，这是比较亮眼的突破。</li>
<li>第二种方法是引入了<strong>动态分区</strong>，即Drange机制。该机制的目的是为了让业务的写入压力，在LTC即计算层的memtable上进行区间划分，每个range都有自己的memtable，通过区间划分，从而<strong>实现多个range之间进行并行compaction</strong>。以L0层为例，我们可以把L0层变成没有互相重叠的状态，这时我们就可以对L0层进行并行的compaction，可以加快L0层的文件的消化，从而减轻对memtable flush到磁盘上的过程的影响。</li>
</ul>
<p>第二个核心问题是：在这种方式下需要划分很多不同的Drange，每个range都会增加一定的memtable数量，memtable数量的增加会影响scan和get的性能。假设有一个主请求，在原来所有数据都写在一个memtable里的情况下，在读取时，索引只需要面向这个memtable，再根据跳表进行get，如果get到则可以马上返回。现在划分成不同的Drange，memtable数量增加，因此需要查找的memtable以及L0层的SST也会变多。解决办法是：实现了一个索引，可以查询到一个key在memtable或L0 SST中的最新值（若存在）。</p>
<h1 id="Nova-LSM-中的重要设计"><a href="#Nova-LSM-中的重要设计" class="headerlink" title="Nova-LSM 中的重要设计"></a>Nova-LSM 中的重要设计</h1><h2 id="LTC和StoCs之间的写数据流程"><a href="#LTC和StoCs之间的写数据流程" class="headerlink" title="LTC和StoCs之间的写数据流程"></a>LTC和StoCs之间的写数据流程</h2><p>第一个比较重要的设计是LTC和StoCs之间的写数据流程。该流程展示的是：当在客户端发起写请求时，计算节点和存储节点是以怎样的方式将数据写进去的过程。</p>
<p>首先是计算节点的客户端发起一个新的写请求操作。存储节点在接收到该请求后，基于RDMA交互，它会在buffer区域分配一个内存区域，并且为这块内存和偏移量（当前哪块内存可以写）分配一个id，告知客户端。客户端接到响应后就会开始写数据，完成后会通知存储节点。存储节点接收到信号后，将数据持久化并且再告知客户端。</p>
<p>上述流程是写一个数据文件即SSTable。写完后，我们要以同样的流程将元数据文件更新。因为底层是分布式架构，需要知道哪些文件写在哪里以及每个SST的范围、版本号。</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/nova_lsm_key_design.png" alt="img"></p>
<h2 id="动态区间划分"><a href="#动态区间划分" class="headerlink" title="动态区间划分"></a>动态区间划分</h2><p>第二个比较重要的设计是动态区间划分。假设业务的请求范围为0-1万，当前有10个计算节点，将这10个计算节点的区间划分为10等份，比如第一个key的空间范围为0-1000。在负责0-1000的计算节点里，它会再进行划分，这一层划分业务无感知。这就叫动态区间划分，简称Drange。其作用主要有以下几点：</p>
<p>首先，每个range都是一棵LSM-Tree，按照数据区间，不同的Drange都有自己的memtables。比如0-1000区间又可以划分为10个Drange，10个Drange之间的memtable相互独立。这样做的好处是这些Drange之间的key互不重叠，例如0-100、100-200、200-300。</p>
<p>其次，在Dranges下还有一层Tranges。如果发现Drange里的部分range比如890-895存在热点现象，而旁边的range并非热点，则可以用Tranges进行细粒度的复杂重均衡，实现动态均衡负载。</p>
<p>最后，在此基础上，因为Drange的key范围互不相交，当memtable变成immutable，不可再写后，它们需要独立地flush到磁盘上。这时，在L0层的SSTable来自不同的Drange，它们之间的key完全不相交，我们就可以进行并行的compaction。</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/nova_lsm_key_design_2.png" alt="img"></p>
<p>文章还将没有Drange划分和有Drange划分两种情况进行了对比：</p>
<ul>
<li>在没有Drange划分的情况下，L0的compaction无法很好并行。在这种情况下，如果遇到最坏的情况，L0层的某一个SST有可能覆盖了整个key空间，假设key范围为0-600，L0层的SST文件的范围是0-1000，当发生compaction时，它必须要跟其他4个SST做归并，这时不但要把L0层的其他SST全部读取比较一遍，还要把L1层所有的SST都读一遍再做归并排序。这时写放大会较为严重，意味着L0层到L1层的compaction会变慢，flush也会变慢，甚至flush不了时，前端就会出现缓写、停写现象。</li>
<li>有Drange划分后，相当于compaction可以分开区间，如下方的示意图所示。在0-100区间，L0到L1可以独立去compaction，100-200区间也可以独立去compaction，可以较好地实现并行compaction。而在原生的RocksDB里，只有从L1开始compaction，才能进行并行compaction操作。</li>
</ul>
<h2 id="索引查找以及Scan操作"><a href="#索引查找以及Scan操作" class="headerlink" title="索引查找以及Scan操作"></a>索引查找以及Scan操作</h2><p>因为划分了很多不同的动态区间，memtable的数量也会增加，意味着查询操作的耗时也会增加。所以要如何在原来的基础上维护好读性能？这篇文章提出了以下解决思路：</p>
<p>每个LTC维护了一个lookup index。如果这些数据存在于memtable和L0层的SST上，通过lookup index我们就可以快速查找到想要的数据。当某一个L0层SST被compaction到L1层时，索引上就会移除掉对应的key。</p>
<p>LTC同时还维护了一个范围索引即range index。因为知道每个Drange的范围，所以当一个scan请求所涉及到的key都可以在memtable和L0层SST中找到时，该范围索引就能快速响应scan操作。</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/nova_lsm_key_design_3.png" alt="img"></p>
<h2 id="SSTable的分布"><a href="#SSTable的分布" class="headerlink" title="SSTable的分布"></a>SSTable的分布</h2><p>最后一个比较重要的设计涉及到存储层。当某个SST文件要写到存储节点时，分布式系统首先要保证负载均衡，要保证数据避免单点故障不可恢复的场景。</p>
<p>该文章提出根据一定策略，将数据文件即SST打散写入到多个存储节点里。考虑到存储成本，每个SSTable采用纠删码（Erasure Coding）的方式进行编码然后分布式存放。默认情况下对每个 SSTable 采用 “3+1”的 EC 配置，将一个SSTable切分为3个数据块，根据一定算法，在这3个数据块里去计算出一个校验块，变成了“3+1”的形式。这种方式比传统的多副本可以<strong>节省更多空间</strong>。假设一个SSTable是3M，这种“3+1”的方式最终所占空间为4M，并且<strong>能容忍一个节点的丢失</strong>，与占用6M空间的双副本方案拥有同样的故障容忍等级。而元数据文件因为体积比较小，所以直接采用多副本存储的方式，比如1个元数据文件可以写3个副本。</p>
<h1 id="Challenges-and-Solutions"><a href="#Challenges-and-Solutions" class="headerlink" title="Challenges and Solutions"></a>Challenges and Solutions</h1><ol>
<li><p>Write Stalls, the solutions are:</p>
<ol>
<li><p>Vertical scaling: use large memory.</p>
</li>
<li><p>Horizontal scaling: use the bandwidth of many StoCs.</p>
</li>
</ol>
</li>
<li><p>Scans are slowed down, the solutions are:</p>
<ol>
<li><p>Construct Dranges at runtime based on workload. Drange faciliates parallel compaction.</p>
</li>
<li><p>Construct range index dynamically.</p>
</li>
</ol>
</li>
<li><p>Gets are slowed down, the solution is: Use lookup index.</p>
</li>
<li><p>Temporary Bottlenecks, the solution is:</p>
<ol>
<li><p>Scatter blocks of a SSTable across multiple StoCs.</p>
</li>
<li><p>Power-of-d: power-of-d is applied in Nova-LSM to help with load balancing during SSTable placement. When writing data to storage components (StoCs), Nova-LSM doesn’t randomly select just one StoC. Instead, it chooses d StoCs at random and writes to the one with the shortest queue. This method helps avoid bottlenecks and improves throughput, ensuring that data is distributed evenly across storage nodes without overwhelming any individual node.</p>
</li>
</ol>
</li>
<li><p>Logging, the solution is: Replicating Log records in the memory of StoCs to provide high availability.</p>
</li>
<li><p>Skewed Access Pattern, the solution is: Dranges enable LTC to write 65% less data to StoCs with skewed data access.</p>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Why do modern database systems disaggregate compute from storage?</strong></p>
<p>Modern database systems disaggregate compute from storage to improve scalability, resource utilization, and fault isolation. By separating compute (processing) and storage, the system can independently scale each based on demand. Compute nodes handle processing, while storage nodes handle data access, optimizing resources and ensuring that failures in one component don’t impact the other. This separation also benefits cloud environments, where elastic scaling of resources is crucial.</p>
<p><strong>How does Nova-LSM provide superior performance than monolithic data stores?</strong> </p>
<p>Nova-LSM improves performance by using a component-based architecture that disaggregates processing (LTC) and storage (StoC). It allows components to scale independently and uses RDMA for fast communication. Nova-LSM also introduces dynamic range partitioning (Dranges), allowing parallel compaction and reducing write stalls, which significantly enhances throughput. This architecture minimizes bottlenecks seen in monolithic stores like LevelDB and RocksDB, especially under skewed workloads.</p>
<p><strong>Why does the standard cost-based optimizer produce sub-optimal query plans? How does Kepler improve both the query planning time and query execution time?</strong></p>
<p>The standard cost-based optimizer can produce sub-optimal plans because it relies on simplified and static cost models that don’t always capture real execution costs, especially in dynamic environments. It also may lack up-to-date statistics, leading to inaccurate decisions. Kepler, on the other hand, uses machine learning to learn from past executions and adapts to current data distributions, improving query plan selection. By pruning the search space efficiently and using real-time data, it reduces both planning time and execution time while optimizing performance.</p>
<p>References: </p>
<ul>
<li><p><a href="https://cloud.tencent.com/developer/article/2002523?areaSource=102001.3&traceId=XIO8WvF-vqiMAsiAKu2Lv">https://cloud.tencent.com/developer/article/2002523?areaSource=102001.3&amp;traceId=XIO8WvF-vqiMAsiAKu2Lv</a></p>
</li>
<li><p><a href="https://dl.acm.org/doi/pdf/10.1145/3448016.3457297">https://dl.acm.org/doi/pdf/10.1145/3448016.3457297</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>MapReduce</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/MapReduce/</url>
    <content><![CDATA[<h1 id="Programming-Model"><a href="#Programming-Model" class="headerlink" title="Programming Model"></a>Programming Model</h1><p>Input and Output:</p>
<ul>
<li>Takes a <strong>set of input key&#x2F;value pairs</strong>.</li>
<li>Produces a <strong>set of output key&#x2F;value pairs</strong>.</li>
</ul>
<p>User-Defined Functions:</p>
<ul>
<li><strong>Map Function</strong>:<ul>
<li>Written by the user.</li>
<li>Processes each input key&#x2F;value pair.</li>
<li>Generates a set of <strong>intermediate key&#x2F;value pairs</strong>.</li>
</ul>
</li>
<li><strong>Reduce Function</strong>:<ul>
<li>Also written by the user.</li>
<li>Takes an intermediate key <code>I</code> and a <strong>set of values</strong> associated with it.</li>
<li>Merges these values to produce a <strong>smaller set</strong> of output values, often zero or one value.</li>
</ul>
</li>
</ul>
<p>Intermediate Data Handling:</p>
<ul>
<li>The <strong>MapReduce library</strong> groups intermediate values by their key (<code>I</code>) and sends them to the Reduce function.</li>
<li>Intermediate values are supplied to the Reduce function using an <strong>iterator</strong>, enabling efficient handling of data sets that are too large to fit into memory.</li>
</ul>
<p>Fault Tolerance and Scalability:</p>
<ul>
<li>By breaking tasks into smaller, independent computations, MapReduce ensures scalability and fault tolerance, even in large distributed environments.</li>
</ul>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/mapred_exe_overview.png" alt="img"></p>
<p>数据分割与任务分配：</p>
<ul>
<li>输入数据划分：MapReduce 库将输入文件自动分割成 M 个片段（通常每个片段16MB到64MB，可由用户控制）。</li>
<li>启动程序实例：在集群中启动多个程序副本。</li>
<li>角色分配：其中一个程序实例被指定为主节点（master），其余的作为工作节点（workers）。</li>
</ul>
<p>任务调度：</p>
<ul>
<li>主节点的职责：主节点负责管理 M 个 map 任务和 R 个 reduce 任务。</li>
<li>任务分配：主节点将空闲的工作节点分配给 map 任务或 reduce 任务。</li>
</ul>
<p>Map 阶段：</p>
<ul>
<li>读取数据：被分配 map 任务的工作节点读取对应的输入片段。</li>
<li>处理数据：解析出键&#x2F;值对，并将其传递给用户定义的 Map 函数。</li>
<li>生成中间结果：<strong>Map 函数产生的中间键&#x2F;值对会存储在本地磁盘中。</strong></li>
</ul>
<p>中间数据处理：</p>
<ul>
<li>写入本地磁盘：**缓存的中间结果会定期写入本地磁盘，**<strong>并根据分区函数划分为 R 个区域。</strong></li>
<li>通知主节点：工作节点将这些中间数据的位置告知主节点，主节点负责将这些信息传递给 reduce 工作节点。</li>
</ul>
<p>Reduce 阶段准备：</p>
<ul>
<li>读取中间数据：reduce 工作节点收到主节点的通知后，通过远程过程调用（RPC）从 map 工作节点的本地磁盘读取中间数据。</li>
<li>排序数据：reduce 工作节点将所有中间数据按键排序，以确保相同的键聚集在一起。如果数据量过大，无法全部加载到内存，会采用外部排序。</li>
</ul>
<p>Reduce 阶段：</p>
<ul>
<li>执行 Reduce 函数：reduce 工作节点遍历排序后的中间数据，对于每个唯一的中间键，将键和对应的值列表传递给用户定义的 Reduce 函数。</li>
<li>生成最终输出：Reduce 函数的输出被追加到该 reduce 分区的最终输出文件中。</li>
</ul>
<p>任务完成与结果返回：</p>
<ul>
<li>任务监控：当所有的 map 和 reduce 任务都完成后，主节点会唤醒用户程序。</li>
<li>返回结果：此时，用户程序中的 MapReduce 调用返回，用户可以获取 R 个输出文件，每个 reduce 任务对应一个输出文件。</li>
</ul>
<p>额外说明：</p>
<ul>
<li>数据处理链：通常用户不需要将这 R 个输出文件合并成一个文件，因为这些文件可以直接作为下一个 MapReduce 调用的输入，或者被能够处理多文件输入的分布式应用程序使用。</li>
<li>流程图参考：上图👆用于展示 MapReduce 操作的整体流程，对应上面的步骤1到7。</li>
</ul>
<h2 id="Master-Data-Structure"><a href="#Master-Data-Structure" class="headerlink" title="Master Data Structure"></a>Master Data Structure</h2><p>Task State Tracking:</p>
<ul>
<li>For each <strong>map</strong> and <strong>reduce task</strong>, the master stores:<ul>
<li><strong>State</strong>:<ul>
<li><code>idle</code>: Task is yet to be assigned.</li>
<li><code>in-progress</code>: Task is currently being executed.</li>
<li><code>completed</code>: Task has finished execution.</li>
</ul>
</li>
<li><strong>Worker Identity</strong>: The worker machine handling the task (for non-idle tasks).</li>
</ul>
</li>
</ul>
<p>Intermediate Data Management:</p>
<ul>
<li>The master acts as the <strong>conduit</strong> for transferring intermediate data from map tasks to reduce tasks.</li>
<li>For each completed map task:<ul>
<li>It records the <strong>locations</strong> and <strong>sizes</strong> of the <code>R</code> intermediate file regions generated.</li>
<li>This data is crucial for reduce tasks to fetch intermediate results from the corresponding map workers.</li>
</ul>
</li>
</ul>
<p>Dynamic Updates:</p>
<ul>
<li>As map tasks finish, the master continuously updates its records of intermediate file locations and sizes.</li>
<li>These updates are incrementally pushed to reduce workers that are currently in-progress.</li>
</ul>
<h2 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h2><p>Worker Failure</p>
<ul>
<li><strong>Failure Detection</strong>:<ul>
<li>The <strong>master node</strong> periodically <strong>pings every worker</strong>.</li>
<li>If a worker does not respond within a certain timeframe, the master marks the worker as <strong>failed</strong>.</li>
</ul>
</li>
<li><strong>Task Rescheduling</strong>:<ul>
<li><strong>Map Tasks</strong>:<ul>
<li><strong>Completed Map Tasks</strong>:<ul>
<li>If the failed worker had completed a map task, its output becomes inaccessible (stored on the failed machine’s local disk).</li>
<li>These tasks are reset to their <strong>idle state</strong> and re-executed on other workers.</li>
</ul>
</li>
<li><strong>In-Progress Map Tasks</strong>:<ul>
<li>Similarly, in-progress tasks are marked as idle and reassigned to available workers.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Reduce Tasks</strong>:<ul>
<li><strong>Completed Reduce Tasks</strong>:<ul>
<li>These do not require re-execution, as their output is stored in a <strong>global file system</strong>, which remains accessible despite the failure.</li>
</ul>
</li>
<li><strong>In-Progress Reduce Tasks</strong>:<ul>
<li>如果某些 reduce 节点尚未读取中间数据，它们会从新的执行结果中读取数据。</li>
</ul>
</li>
</ul>
</li>
<li><strong>Data Coordination</strong>:<ul>
<li>When a map task is re-executed on a new worker:<ul>
<li><strong>Notification</strong>: All reduce workers are informed of the re-execution.</li>
<li><strong>Data Redirection</strong>: Reduce workers that have not yet fetched the intermediate data from the failed worker will instead fetch it from the new worker.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Master Failure</p>
<p>It is easy to make the master write periodic checkpoints of the master data structures described above. If the master task dies, a new copy can be started from the last checkpointed state. However, given that there is only a single master, its failure is unlikely; <strong>therefore our current implementation aborts the MapReduce computation if the master fails.</strong> Clients can check for this condition and retry the MapReduce operation if they desire.</p>
<h2 id="Locality-数据本地化优化"><a href="#Locality-数据本地化优化" class="headerlink" title="Locality 数据本地化优化"></a>Locality 数据本地化优化</h2><p>存储设计： 数据存储在 Google 文件系统 (GFS) 中。 GFS 将每个文件分割为 64 MB 的块，并在不同的机器上保存多个副本（通常是 3 个）。</p>
<p>任务调度优先级：</p>
<ul>
<li>优先本地化调度：The master node prioritizes assigning map tasks to workers that are on the same machine containing the replica of the data block.</li>
<li>次优调度：If local scheduling is not possible (e.g., the worker with the data block is busy), the master assigns the task to a machine near the replica, such as within the same rack or data center.</li>
</ul>
<p>实际效果： 在运行大型 MapReduce 操作时，大部分输入数据会从本地磁盘读取。 因为数据本地化，减少了跨网络传输的数据量，从而节省网络带宽。</p>
<h2 id="Task-Granularity"><a href="#Task-Granularity" class="headerlink" title="Task Granularity"></a>Task Granularity</h2><ol>
<li>Map 和 Reduce 阶段的划分<ol>
<li>任务数量 (M 和 R)： Map 阶段被划分为 M 个任务。 Reduce 阶段被划分为 R 个任务。</li>
<li>划分原则：理想情况下，M 和 R 的数量应该远大于工作节点的数量（即机器的数量）。</li>
</ol>
</li>
<li>多任务划分的好处<ol>
<li>动态负载均衡： 每个工作节点可执行多个任务，这样可以动态调整任务分配，避免某些节点过载或闲置。</li>
<li>故障恢复加速： 如果某个工作节点失败，已完成的多个任务可以分散到其他节点重新执行，恢复速度更快。</li>
</ol>
</li>
<li>任务划分的实际限制<ol>
<li>调度开销： 主节点需要进行 O(M + R) 次调度决策，且需要存储 O(M × R) 的状态信息。 虽然每对 map&#x2F;reduce 任务对仅占用约 1 字节内存，但过多任务会增加内存需求和调度复杂性。</li>
<li>输出文件限制： R 的大小往往受到用户需求限制，因为每个 reduce 任务会生成一个独立的输出文件。 输出文件过多会导致文件管理复杂。</li>
</ol>
</li>
<li>实际任务大小选择</li>
<li><ol>
<li>Map 阶段： 每个 map 任务通常处理 16 MB 到 64 MB 的输入数据。 这样的任务大小可以充分利用数据本地化优化（即尽量从本地磁盘读取数据）。</li>
<li>Reduce 阶段： R 通常是工作节点数量的几倍，以充分利用并行能力。 在一个典型的大规模 MapReduce 计算中： M &#x3D; 200,000（Map 阶段任务数）。 R &#x3D; 5,000（Reduce 阶段任务数）。 工作节点 &#x3D; 2,000（机器数量）。</li>
</ol>
</li>
</ol>
<h2 id="Backup-Tasks"><a href="#Backup-Tasks" class="headerlink" title="Backup Tasks"></a>Backup Tasks</h2><ol>
<li><p>什么是拖后腿的任务（Straggler Tasks）？</p>
<ul>
<li>定义：A straggler task refers to a task (map or reduce) in a MapReduce job that runs much slower than other tasks, delaying the overall completion of the job.</li>
</ul>
</li>
<li><p>解决方法：</p>
<ul>
<li><p>备份任务机制：当 MapReduce 计算接近完成时，主节点会为未完成的任务安排备份执行（Backup Executions）。同一任务的多个副本在不同的工作节点上同时运行。只要其中一个副本完成，任务即被标记为完成。</p>
</li>
<li><p>资源开销：调整后的机制只增加少量（通常是几个百分点）的计算资源使用。通过备份执行，能够显著缩短总执行时间。</p>
</li>
</ul>
</li>
</ol>
<h1 id="Refinement"><a href="#Refinement" class="headerlink" title="Refinement"></a>Refinement</h1><p><strong>Partitioning Function</strong></p>
<ol>
<li>Reduce 任务与分区<ol>
<li>用户通过设置 R 来指定需要的 reduce 任务数或输出文件数。</li>
<li>数据在这些 reduce 任务之间分区，分区方式取决于分区函数（partitioning function）。</li>
</ol>
</li>
<li>默认分区方式<ol>
<li>默认使用 哈希函数（hash function）：</li>
<li>分区规则：hash(key) mod R。</li>
<li>优势：通常能实现较为均衡的分区（即数据均匀分布到不同 reduce 任务中）。</li>
</ol>
</li>
<li>自定义分区方式<ol>
<li>有时默认的哈希分区不满足实际需求，需要根据特定逻辑对数据进行分区。例如：数据的键是 URL，用户希望所有来自同一主机（host）的条目存储在同一个输出文件中。</li>
<li>解决方案：用户可以定义自己的分区函数，例如：hash(Hostname(urlkey)) mod R：根据 URL 的主机名分区。这样，来自同一主机的所有条目会被分配到相同的 reduce 任务中。</li>
</ol>
</li>
</ol>
<p><strong>Ordering Guarantees</strong></p>
<ol>
<li>排序保证<ol>
<li>在 MapReduce 的 每个分区内，中间的键&#x2F;值对（key&#x2F;value pairs）会按照键的递增顺序进行处理。</li>
<li>目标：确保每个分区的输出文件是有序的。</li>
</ol>
</li>
<li>排序的作用<ol>
<li>生成有序输出文件：每个 reduce 任务生成的输出文件是按键排序的，直接支持有序数据的存储。</li>
<li>支持高效随机访问：有序数据便于快速查找，比如通过键值实现高效的随机访问。</li>
<li>用户便利：用户使用这些输出文件时，不需要额外排序，使用起来更方便。</li>
</ol>
</li>
</ol>
<p><strong>Combiner Function</strong></p>
<ol>
<li>问题背景<ol>
<li>在某些情况下，中间键重复率较高，每个 map 任务可能会生成大量重复的中间键记录。示例：在单词计数任务中（例如 〈the, 1〉），常见单词（如 “the”）会频繁出现。</li>
<li>结果：这些重复记录需要通过网络传输到同一个 reduce 任务，增加了网络负载。</li>
</ol>
</li>
<li>Combiner 函数的解决方案<ol>
<li>定义：Combiner 是一个可选的、局部的聚合函数，<strong>用于在 map 任务所在机器上对中间数据进行部分合并。</strong></li>
<li>工作原理：</li>
<li><ol>
<li>执行位置：Combiner 在 map 任务的机器上运行。</li>
<li>功能：对重复键的中间结果进行局部汇总，减少需要传输的数据量。</li>
<li>例如：将 〈the, 1〉、〈the, 1〉、〈the, 1〉 合并为 〈the, 3〉。</li>
</ol>
</li>
</ol>
</li>
<li>Combiner 和 Reduce 的区别<ol>
<li>相同点：通常，Combiner 的代码与 Reduce 函数的代码相同。都用于对数据进行聚合处理。</li>
<li>不同点：<ol>
<li>Combiner：输出的是中间结果，数据会继续传递给 Reduce 任务。</li>
<li>Reduce：输出的是最终结果，数据写入最终的输出文件。</li>
</ol>
</li>
</ol>
</li>
<li>优化效果<ol>
<li>减少网络传输量：通过提前合并数据，Combiner 减少了从 map 任务到 reduce 任务的数据量。例如，不传输 1000 条 〈the, 1〉，而是只传输 1 条 〈the, 1000〉。</li>
<li>提升性能：对于重复率高的任务，Combiner 能显著加快 MapReduce 操作的速度。</li>
</ol>
</li>
</ol>
<p><strong>Input and Output Types</strong></p>
<ol>
<li>输入数据格式的支持<ol>
<li>预定义格式：<ol>
<li>文本模式（text mode）： 每行数据被视为一个键&#x2F;值对。<ol>
<li>键：文件中该行的偏移量（offset）。</li>
<li>值：该行的内容。 排序键&#x2F;值对模式： 存储的键&#x2F;值对按键排序，便于按范围处理。</li>
</ol>
</li>
<li>自动分割范围： 每种输入格式都有分割机制，可将输入数据划分为适合 map 任务处理的范围。 例如，文本模式会确保分割发生在行边界，而不是行中间，保证数据的完整性。</li>
</ol>
</li>
<li>用户自定义格式： 用户可以通过实现简单的读取接口（reader interface），支持新的输入类型。<ol>
<li>非文件输入：数据可以来自其他来源，如数据库或内存中的数据结构，而不一定是文件。</li>
</ol>
</li>
</ol>
</li>
<li>输出数据格式的支持<ol>
<li>类似输入格式，MapReduce 也支持多种输出格式：<ol>
<li>预定义格式：提供了一些常用的输出格式。</li>
<li>自定义格式：用户可以通过实现新的接口定义输出数据格式。</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Skipping Bad Records</strong></p>
<ol>
<li>问题背景<ol>
<li>用户代码缺陷：Map 或 Reduce 函数中可能存在错误（如某些记录引发崩溃）。</li>
<li>确定性崩溃：对特定记录，每次处理都会发生崩溃。</li>
<li>问题影响：这类错误可能阻止整个 MapReduce 操作完成。</li>
<li>无法修复的情况：错误可能在第三方库中，用户无法访问源代码。</li>
</ol>
</li>
<li>MapReduce 提供的解决方案<ol>
<li>跳过问题记录：MapReduce 允许系统检测引发崩溃的记录，并跳过这些记录以继续操作。</li>
<li>实现机制：<ol>
<li>信号处理：每个工作节点安装信号处理器（signal handler），捕获段错误（segmentation violations）和总线错误（bus errors）。</li>
<li>记录错误序号：在调用用户的 Map 或 Reduce 函数之前，系统将参数的序列号存储在全局变量中。</li>
<li>发送错误报告：如果用户代码触发错误，信号处理器会发送一个“最后的喘息”UDP 数据包，包含引发错误的记录序号，通知主节点。</li>
<li>主节点决策：If a record causes failures many times, the master node instructs the record to be skipped the next time the task is retried.</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><strong>Local Execution</strong></p>
<ol>
<li>分布式调试的挑战<ol>
<li>复杂性：Map 和 Reduce 函数的实际计算是在分布式系统上完成，涉及数千台机器。主节点动态分配任务，调试难以直接定位问题。</li>
<li>常见问题：分布式环境下的日志、任务状态和数据流使得问题排查更加困难。</li>
</ol>
</li>
<li>本地执行模式的设计<ol>
<li>功能：MapReduce 提供了一种 本地执行的替代实现，在单台机器上顺序执行整个 MapReduce 操作。</li>
<li>特点：所有任务按顺序运行，无需分布式调度。用户可以限制计算范围，仅调试特定的 map 任务。</li>
</ol>
</li>
</ol>
<p><strong>Counter</strong></p>
<ul>
<li>Counters are used to track occurrences of specific events during a MapReduce operation, such as:<ul>
<li><strong>Custom events</strong> defined by the user (e.g., counting words, detecting specific patterns).</li>
<li><strong>System-defined metrics</strong>, like the number of input&#x2F;output key-value pairs processed.</li>
</ul>
</li>
</ul>
<p><strong>How Counters Work</strong></p>
<ul>
<li><strong>Propagation to the Master</strong>:<ul>
<li>Counter values from individual workers are sent to the <strong>master node</strong> via <strong>ping responses</strong>.</li>
</ul>
</li>
<li><strong>Aggregation</strong>:<ul>
<li>The master aggregates counter values from all completed tasks.</li>
<li>It ensures <strong>no double counting</strong> by ignoring duplicate task executions (e.g., due to re-executions or backup tasks).</li>
</ul>
</li>
</ul>
<p><strong>Monitoring and Reporting</strong></p>
<ul>
<li><strong>Real-Time Monitoring</strong>:<ul>
<li>The current counter values are displayed on the <strong>master status page</strong>, allowing users to observe the progress of the computation.</li>
</ul>
</li>
<li><strong>Final Reporting</strong>:<ul>
<li>When the MapReduce job finishes, the aggregated counter values are returned to the user program.</li>
</ul>
</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Assuming M&#x3D;10 and R&#x3D;20, What is the total number of files produced by the mappers?</strong></p>
<p>Total Files &#x3D; M×R &#x3D; 10×20 &#x3D; 200</p>
<p><strong>Why does MapReduce store the output of Reduce in the Google File System?</strong> </p>
<ul>
<li><p><strong>High Availability</strong>: GFS provides fault tolerance by replicating data across multiple machines. This ensures the output is not lost if a machine fails.</p>
</li>
<li><p><strong>Scalability</strong>: GFS is designed to handle large-scale data storage, making it suitable for the massive outputs of MapReduce jobs.</p>
</li>
</ul>
<p><strong>What is the purpose of a straggler?</strong> </p>
<ul>
<li><strong>“Straggler” refers to</strong> <strong>slow-running</strong> <strong>tasks</strong>, often map or reduce tasks, that significantly delay the completion of a MapReduce job.</li>
<li>Solution:</li>
<li><ul>
<li><strong>Backup Execution</strong>: The master node schedules backup executions of straggler tasks on other available workers.</li>
</ul>
</li>
</ul>
<p><strong>True or False: One may use SQL++ with a CSV data file and no schema.</strong></p>
<p>True: SQL++ can operate on semi-structured data, including CSV files, without requiring a predefined schema.</p>
<p><strong>With SQL++, what is the difference between pivot and unpivot?</strong></p>
<p>Pivot:</p>
<ul>
<li><p><strong>Purpose</strong>: Transforms rows into attributes (columns).</p>
</li>
<li><p><strong>Example</strong>:</p>
<ul>
<li><p>Input: <code>[ &#123; &quot;symbol&quot;: &quot;amzn&quot;, &quot;price&quot;: 1900 &#125;, &#123; &quot;symbol&quot;: &quot;goog&quot;, &quot;price&quot;: 1120 &#125;, &#123; &quot;symbol&quot;: &quot;fb&quot;, &quot;price&quot;: 180 &#125; ]</code></p>
</li>
<li><p>Query: <code>PIVOT sp.price AT sp.symbol FROM today_stock_prices sp;</code></p>
</li>
<li><p>Output: <code>&#123; &quot;amzn&quot;: 1900, &quot;goog&quot;: 1120, &quot;fb&quot;: 180 &#125;</code></p>
</li>
</ul>
</li>
</ul>
<p>Unpivot:</p>
<ul>
<li><p><strong>Purpose</strong>: Transforms attributes (columns) into rows.</p>
</li>
<li><p><strong>Example</strong>:</p>
<ul>
<li><p>Input: <code>&#123; &quot;date&quot;: &quot;4/1/2019&quot;, &quot;amzn&quot;: 1900, &quot;goog&quot;: 1120, &quot;fb&quot;: 180 &#125;</code></p>
</li>
<li><p>Query: <code>UNPIVOT c AS price AT sym FROM closing_prices c WHERE sym != &#39;date&#39;;</code></p>
</li>
<li><p>Output: <code>[ &#123; &quot;date&quot;: &quot;4/1/2019&quot;, &quot;symbol&quot;: &quot;amzn&quot;, &quot;price&quot;: 1900 &#125;, &#123; &quot;date&quot;: &quot;4/1/2019&quot;, &quot;symbol&quot;: &quot;goog&quot;, &quot;price&quot;: 1120 &#125;, &#123; &quot;date&quot;: &quot;4/1/2019&quot;, &quot;symbol&quot;: &quot;fb&quot;, &quot;price&quot;: 180 &#125; ]</code></p>
</li>
</ul>
</li>
</ul>
<p><strong>Using BG, one may summarize the performance of a data store using its SoAR. What is the input to BG to compute the SoAR of a data store?</strong></p>
<h3 id="1-SLA-Specifications"><a href="#1-SLA-Specifications" class="headerlink" title="1. SLA Specifications"></a><strong>1.</strong> <strong>SLA Specifications</strong></h3><p>The Service Level Agreement (SLA) defines the conditions under which SoAR is computed. The SLA includes:</p>
<ul>
<li><strong>α:</strong> Percentage of requests that must observe a response time less than or equal to β (e.g., 95%).</li>
<li><strong>β:</strong> Maximum acceptable response time (e.g., 100 ms).</li>
<li><strong>τ:</strong> Maximum allowable percentage of requests that observe unpredictable (stale or inconsistent) data (e.g., 0.01%).</li>
<li><strong>Δ:</strong> Duration for which the SLA must be satisfied (e.g., 10 minutes).</li>
</ul>
<h3 id="2-Database-Configuration"><a href="#2-Database-Configuration" class="headerlink" title="2. Database Configuration"></a><strong>2. Database Configuration</strong></h3><p>Details about the data store being tested:</p>
<ul>
<li><strong>Logical Schema:</strong> The data model used by the data store (e.g., relational schema, JSON-like schema for NoSQL).</li>
<li><strong>Physical Setup:</strong> Hardware configuration, including:<ul>
<li>Number of nodes.</li>
<li>Storage and memory resources.</li>
<li>Networking capabilities.</li>
</ul>
</li>
<li><strong>Population Size:</strong><ul>
<li><strong>M:</strong> Number of members in the database.</li>
<li><strong>ϕ:</strong> Number of friends per member.</li>
<li><strong>ρ:</strong> Number of resources per member.</li>
</ul>
</li>
</ul>
<h3 id="3-Workload-Parameters"><a href="#3-Workload-Parameters" class="headerlink" title="3. Workload Parameters"></a><strong>3. Workload Parameters</strong></h3><p>The workload specifies the nature and intensity of the actions BG will simulate:</p>
<ul>
<li><strong>Mix of</strong> <strong>Actions</strong><strong>:</strong><ul>
<li>Types of social networking actions (e.g., View Profile, List Friends, View Friend Requests).</li>
<li>Percentage of each action type (read-heavy, write-heavy, or mixed workloads).</li>
</ul>
</li>
<li><strong>Think Time (ϵ):</strong><ul>
<li>The delay between consecutive actions performed by a single thread.</li>
</ul>
</li>
<li><strong>Inter-Arrival Time (ψ):</strong><ul>
<li>The delay between new user sessions.</li>
</ul>
</li>
</ul>
<h3 id="4-Environmental-Parameters"><a href="#4-Environmental-Parameters" class="headerlink" title="4. Environmental Parameters"></a><strong>4. Environmental Parameters</strong></h3><p>Details about how BG generates and manages workloads:</p>
<ul>
<li><strong>Number of BGClients (N):</strong> Instances responsible for generating requests.</li>
<li><strong>Number of Threads (T):</strong> Concurrency level (threads per BGClient).</li>
<li><strong>D-Zipfian Distribution Parameters (θ):</strong><ul>
<li>Defines the access pattern (e.g., frequency of popular vs. less popular data).</li>
</ul>
</li>
</ul>
<p><strong>Consider the following binary representation of the priority of a key-value pair, 00101001. What is its CAMP rounding with precision 4?</strong></p>
<p>00101000</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/bg_bm_rounding.png" alt="img"></p>
<p><strong>What is a thundering herd and how does the IQ framework prevent it from causing the persistent data store to become the bottleneck?</strong></p>
<p>Thundering Herd Problem:</p>
<ul>
<li>When a key-value pair is not found in the KVS (a <strong>KVS miss</strong>), multiple read sessions might simultaneously query the RDBMS to fetch the value.</li>
<li>This can overload the RDBMS and degrade performance under high concurrency.</li>
</ul>
<p>Solution:</p>
<ul>
<li>When the first read session encounters a KVS miss, it requests an I lease for the key.</li>
<li>Once the I lease is granted, the KVS prevents other read sessions from querying the RDBMS for the same key.</li>
<li>All other read sessions must “back off” and wait for the value to be updated in the KVS by the session holding the I lease.</li>
</ul>
<blockquote>
<p>A thundering herd happens when a specific key undergoes heavy read and write activity.</p>
<ul>
<li>Writes invalidate the cache repeatedly  </li>
<li>All reads are forced to query the database</li>
</ul>
<p>I lease solves this problem</p>
<ul>
<li>The first read for the specific key is granted the I lease  </li>
<li>All other reads observe a miss and back-off  </li>
<li>The read with the I lease queries the RDBMS, computes the missing value, and populates the cache  </li>
<li>All other reads observe a cache hit</li>
</ul>
</blockquote>
<p>Reference: <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf">https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>SDM</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/08/30/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SDM/</url>
    <content><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>Data Models: Conceptual -&gt; Logical -&gt; Physical.</p>
<p>SDM is a conceptual data modeling tool, at the intersection of conceptual and logical. It facilitates <strong>an understaning of the meaning of the data</strong>.</p>
<ul>
<li>Identify and classify principal intensional (semantic) structures of an application.</li>
</ul>
<p>A set of constructs that express the essential meaning and structure of different problem domains.</p>
<h1 id="Basic-Structure"><a href="#Basic-Structure" class="headerlink" title="Basic Structure"></a>Basic Structure</h1><ul>
<li>Classes: a collection of entities. Each class has:</li>
<li><ul>
<li>A name.</li>
<li>A collection of members: Its entities. 3 member types:</li>
<li><ul>
<li>Objects:</li>
<li><ul>
<li>Concrete.</li>
<li>Abstraction: a generalization of another entity.</li>
<li>Aggregate: a collection of another type of entity.</li>
</ul>
</li>
<li>Events: Action or activities in the application. Point and duration events.</li>
<li>Names are deginators for objects or events.</li>
</ul>
</li>
<li>Attributes.</li>
<li>A description: nature, purpose, and uses of the class.</li>
<li>Identified as either base or nonbase.</li>
</ul>
</li>
<li>Schema: a collection of classes.</li>
</ul>
<p>Class Attributes:</p>
<ol>
<li><strong>Member attributes</strong> link the member to one or more related entities in the same or another class.</li>
<li><strong>Class determined attribute</strong> is associated with the whole class and has the same value for all members of that class.</li>
<li><strong>Class attribute</strong> describes a property of a class taken as a whole.</li>
</ol>
<p>An attribute value is either a primitive (user defined) or derived (a value calculated from other information in the database).</p>
<h1 id="Base-class"><a href="#Base-class" class="headerlink" title="Base class"></a>Base class</h1><p>It is independent of other classes. In SDM, it may be a concrete object class, a point event class, a duration event, a name class.</p>
<p>It is specified as either containing duplicates or not containing duplicates. The latter models a multiset&#x2F;bag of entities.</p>
<p>It has an associated list of groups of member attributes. One or more may serve as the unique identifier of a member.</p>
<h1 id="Nonbase-class"><a href="#Nonbase-class" class="headerlink" title="Nonbase class"></a>Nonbase class</h1><p>Subclass of a parent class. Members of the subclass inherit all attributes of the parent class. Subclass may add new member attributes. 2 types:</p>
<ol>
<li>Restrict: a predicate identifies which members of the parent belong to a subclass.</li>
<li>Subset: A human user decides entities in the subclass as long as the subclass is a subset of its parent.</li>
</ol>
<p>Attribute value: either an entity or a class of entities. It can be UNKNOWN.</p>
<p>Semantic types:</p>
<ol>
<li>A componenet models a physical part of an object.</li>
<li>A participant of the event entity models an entity that plays a role in an event.</li>
<li>A property of an attribute is an attribute that provides further information on the relationship between the entity and the value of one of its attributes.</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Why is it important for a relational schema to satisfy the 5 normal forms?</strong></p>
<p>To ensure the data integrity and consistency, and minimize the loss and redundancy of information.</p>
<p>1NF: all occurences of a record type must contain the same number of fields.</p>
<p>2 and 3NF: a non-key attribute is a fact about only the whole key.</p>
<p>4NF: a record should not contain two or more independent multi-valued fact about an entity.</p>
<p>5NF: decompose a table into smaller ones to eliminate multi-valued dependencies, while ensuring that the original data can be losslessly reconstructed through join operation.</p>
<p><strong>With SDM, what is the unique identifier of a class containing duplicates?</strong></p>
<p>There is no unique identifier of a class containing duplicates since some of the members of this class are indistinguishable.</p>
<p><strong>Is SDM a competitor to the relational data model?</strong> </p>
<p>Yes. But SDM is not intended to be a direct competitor to the relational data model. The goal of SDM is to provide a more semantic way to model complex application environments, expressing the structure and meaning of data more effectively than tradtional relational models. It is designed to enhance the relational data model.</p>
<p><strong>A database represents a snapshot of the state of an application and the changes to the database over time. What is the change in a 3D display that illuminates animations using FLSs? Does an FLS display represent a database using drones?</strong> </p>
<p>A 3D FLS display illuminates animations by computing the flight paths of the FLS drones based on the dynamic attributes of objects, such as geometry, color, and movement over time. Changes in the display represent transitions in the object’s states or visual properties.</p>
<p>Yes, the FLS display act as a dynamic visualization tool that represents a database where each drone corresponds to data points or entities, displaying multimedia content in a 3D space.</p>
<p><strong>Section 1 of the SDM paper states: “SDM is not dependent on the successful implementation of a new database management system that directly supports the SDM. The latter approach would be impractical for a variety of reasons.” Why is it impractical to implement a new database management system that supports the SDM? Do you know of a system?</strong></p>
<p>SDM emphasizes the <strong>meaning</strong> and <strong>relationship</strong> of data, requiring sophisticated handling of semantics, which adds complexity. Integrating SDM into existing systems will cause incompatibility with current DBMS architectures.</p>
<p>While there is not a widely adopted DBMS that fully supports SDM, some graph databases or knowledge graph systems such as <strong>Neo4j</strong>, <strong>RDF stores</strong> partially align with SDM principles.</p>
<p>Reference: <a href="https://dl.acm.org/doi/pdf/10.1145/509252.509264">https://dl.acm.org/doi/pdf/10.1145/509252.509264</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>AnalyticDB - Real-time OLAP Database System at Alibaba Cloud</title>
    <url>/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/01/25/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20AnalyticDB%20Real-time%20OLAP%20Database%20System%20at%20Alibaba%20Cloud/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>The paper addresses 3 problems:</p>
<ol>
<li>How to process more complicated and diverse queries of users with low latency;</li>
<li>How to design a friendly and unified data layout that is compatible with column-stores and row-stores and is able to process the complex data type, with low latency;</li>
<li>How to process massive requests per second with low latency.</li>
</ol>
<p>The paper provides 5 novel ideas and elaborates the proposed approach for the implementation of each idea:</p>
<ol>
<li>Read&#x2F;write decoupling, a worker node processes either read or write operation, guarantees that read operations do not interfere with read operations. In the real-time read mode, version verification is introduced to ensure that each query retrieves the latest data, i.e., the $\text{latest version} &#x3D; max⁡(V_1,V_2)$. Furthermore, after each write operation is completed, the write node actively pulls the latest version to the corresponding read node to avoid high latency in subsequent read operations.</li>
<li>The hybrid row-columnar storage for storing complex-typed data, a detail file &#x3D; $n$ row groups, a row group &#x3D; $k$ data blocks, a data block &#x3D; $p$ FBlocks, a FBlock &#x3D; $x$ values of a certain column of a partial row to many rows.</li>
<li>Efficient index management includes the building and maintenance of indexes for both baseline data and incremental data. For baseline data, AnalyticDB builds inverted indexes and introduces a filter ratio to optimize read operations. For incremental data, AnalyticDB constructs lightweight sorted indexes on read nodes to expedite read operations before the asynchronous construction of the inverted index of this type of data is finished.</li>
<li>For the optimizer, AnalyticDB introduces the STARs framework to evaluate both the capability of storage and relational algebra ability and adopts dynamic programming, in order to achieve efficient predicate push-down. This database minimizes the cost of shuffling tables for join push-down. For index-based join and aggregation, it employs the LeftDeepTree to efficiently utilize index-on-all-columns while also pushing down predicates and aggregations. Furthermore, the optimizer and execution engine perform sampling-based cardinality estimation with caching previously sampled results, optimized sampling algorithm, and improved derived cardinality.</li>
<li>The execution engine is able to operate directly on serialized binary data rather than Java objects, eliminating the expensive costs of serialization and deserialization during the process of shuffling big data.</li>
</ol>
<p>Analytical and experimental findings: AnalyticDB outperforms PrestoDB, Druid, Spark-SQL, and Greenplum in performance on the 1TB dataset, and its performance is not affected dramatically when scaled to the 10TB dataset. As the number of write nodes increases, the write throughput of AnalyticDB exhibits steady growth. In the TPC-H Benchmark, AnalyticDB completes 20 out of 22 queries in half the time required by the second-fastest database. However, for Query 2, AnalyticDB is slower than PrestoDB and Greenplum due to selecting a different join order.</p>
<h1 id="Paper-Strength"><a href="#Paper-Strength" class="headerlink" title="Paper Strength"></a>Paper Strength</h1><ul>
<li>3 challenges outlined in the paper precisely target the critical obstacles faced by OLAP systems in achieving real-time and efficient response. Addressing these challenges not only significantly enhances the database’s compatibility with diverse queries and complex data types, but also improves its responsiveness in the production environment, underscoring the high research value. The integration of read-write decoupling with version verification ensures that read and write operations remain isolated, while consistently providing read queries with the latest data. The design of the hybrid data layout and indexes incorporates support for complex data types such as JSON, full-text, and vector data, enabling a unified access interface for diverse data operations. This significantly broadens the applicability of AnalyticDB to a wider range of use cases. Furthermore, the execution engine, which combines sampling-based cardinality estimation with caching and optimized sampling algorithms, etc, achieves high estimation accuracy at low overhead and minimal latency. In conclusion, the advancements and optimizations introduced in AnalyticDB represent a major step forward in enhancing data versatility and real-time responsiveness in OLAP systems, the comprehensive solution will fully demonstrate its capabilities in high-concurrency e-commerce scenarios.</li>
<li>The paper provides a comprehensive literature review. The 2. Related Work section discusses the shortcomings of different databases. For example, expensive index updates in OLTP databases can degrade throughput and increase latency, and column-store in OLAP databases, such as TeradataDB and Greenplum, causes high random I&#x2F;O costs for point-lookup queries. Those issues above are effectively addressed in AnalyticDB. Furthermore, the paper outlines AnalyticDB’s improvements over Amazon Redshift and differences in query and aggregation compared to Google BigQuery.</li>
<li>The paper provides the detailed description of processes of functionalities in AnalyticDB, including:<ol>
<li>A thorough explanation of the read-write decoupling process from both reading and writing perspectives, accompanied by a flowchart specifically illustrating the more complex read operations.</li>
<li>Pseudocode and comments of key instructions for 3 algorithms involved in Query Execution.</li>
<li>A diagram outlining the merging process of baseline data and incremental data, broken down into 3 phases.</li>
</ol>
</li>
</ul>
<h1 id="Paper-Weakness"><a href="#Paper-Weakness" class="headerlink" title="Paper Weakness"></a>Paper Weakness</h1><ul>
<li>The description of some functionalities in the paper is not complete. In Section 3.4 Read&#x2F;Write Decoupling of the paper, only the real-time read is mentioned, while the bounded-staleness read is missed. For OLAP, reading outdated data is acceptable, and the bounded-staleness read allows read nodes to only access the latest data on write nodes after a certain delay following write operations, so it ensures fast responses in AnalyticDB to some extent. However, this type of read is not explained in the paper.</li>
<li>The paper lacks clarity in its description of certain newly introduced concepts. In Section 5.1.1, the discussion on predicate push-down provides only a brief overview of the STARs framework, without explaining how it applies relational algebra, how cost calculations are performed, and how the optimizer uses dynamic programming to encapsulate relational algebra operators. These omissions may leave readers confused.</li>
<li>The performance evaluation experiments in the paper are relatively simplistic. First, only 3 SQL statements were tested, focusing on a narrow range of query types, specific table partitioning strategies, specific tables and fields, and specific data types. The tests did not address complex data types such as JSON. Second, the experiments only tested datasets of 1TB and 10 TB. However, in production environments, daily new data can reach tens or even hundreds of petabytes. On peak promotional days like Double 11 or 618, the daily data processing amount can reach hundreds or even thousands of petabytes. For example, during the 2022 Double 11, Taobao and Tmall experienced a peak order payment rate of 583000 transactions per second. Thus, the datasets used in the experiments fall far short of reflecting the scale of real-world scenarios. Third, the tests based on the TPC-H Benchmark evaluated only 22 queries, which is insufficient to comprehensively reflect the true performance of these 5 databases. Therefore, it is recommended to include all data and queries involved in the production environment, such as MySQL binlogs, ElasticSearch index logs and Kafka logs, etc, and a stability test lasting one week or longer should be conducted on these 5 databases to provide a more realistic evaluation.</li>
</ul>
<p>Reference: <a href="https://www.vldb.org/pvldb/vol12/p2059-zhan.pdf">https://www.vldb.org/pvldb/vol12/p2059-zhan.pdf</a></p>
]]></content>
      <categories>
        <category>高级数据存储</category>
      </categories>
  </entry>
  <entry>
    <title>OCC 和 MVCC</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/OCC%20%E5%92%8C%20MVCC/</url>
    <content><![CDATA[<h1 id="Time-Stamp-Based-Protocols"><a href="#Time-Stamp-Based-Protocols" class="headerlink" title="Time-Stamp Based Protocols"></a>Time-Stamp Based Protocols</h1><p>Suppose transaction Ti issues read(Q):</p>
<ul>
<li>If TS(Ti) &lt; W-TimeStamp(Q), then Ti needs to read the value of Q which was already overwritten. Hence the read request is rejected and Ti is rolled back.</li>
<li>If TS(Ti) &gt;&#x3D; W-TimeStamp(Q), then the read operation is executed and the R-timeStamp(Q) is set to the maximum of R-TimeStamp(Q) and TS(Ti).</li>
</ul>
<p>Suppose transaction Ti issues write(Q):</p>
<ul>
<li>If TS(Ti) &lt; R-TimeStamp(Q), then this implies that some transaction has already consumed the value of Q and Ti should have produced a value before that transaction read it. Thus, the write request is rejected and Ti is rolled back.</li>
<li>If TS(Ti) &lt; W-TimeStamp(Q), then Ti is trying to write an obsolete value of Q. Hence reject Ti’s request and roll it back. &#x2F; Ignore this write operation. (Tomas’s Write Rule)</li>
<li>Otherwise, execute the write(Q) operation and update W-TimeStamp(Q) to TS(Ti).</li>
</ul>
<h1 id="OCC"><a href="#OCC" class="headerlink" title="OCC"></a>OCC</h1><p>Each transaction Ti has three phases:</p>
<ul>
<li>Read phase: reads the value of data items and copies its contents to variables local to Ti. All writes are performed on the temporary local variables.</li>
<li>Validation phase: Ti determines whether the local variables whose values have been overwritten can be copied to the database. If not, then abort. Otherwise, proceed to Write phase.</li>
<li><ul>
<li>When validating transaction Tj, for all transactions Ti with TS(Ti) &lt; TS(Tj), one of the following must hold:</li>
<li><ul>
<li>Finish(Ti) &lt; Start(Tj), OR</li>
<li>Set of data items written by Ti does not intersect with the set of data items read by Tj, and Ti completes its write phase before Tj starts its validation phase.</li>
</ul>
</li>
</ul>
</li>
<li>Write phase: The values stored in local variables overwrite the value of the data items in the database.</li>
</ul>
<p>A transaction has three time stamps:</p>
<ul>
<li>Start(Ti): When Ti started its execution.</li>
<li>Validation(Ti): When Ti finished its read phase and started its validation.</li>
<li>Finish(Ti): Done with the write phase.</li>
</ul>
<h1 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h1><p>Assume that transaction Ti issues either a read(Q) or a write(Q) operation.</p>
<p>Let Qk denote the version of Q whose write timestamp is the largest write timestamp less than TS(Ti), i.e., W-TimeStamp(Qk) &lt; TS(Ti).</p>
<ul>
<li>If Ti issues a Read(Q), then return the value of Qk.</li>
<li>If Ti issues a write(Q), and TS(Ti) &lt; R-TimeStamp(Qk), then Ti is rolled back.</li>
<li>Otherwise, a new version of Qk is created.</li>
</ul>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>SQL++</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/15/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQL++/</url>
    <content><![CDATA[<h1 id="Data-Model"><a href="#Data-Model" class="headerlink" title="Data Model"></a>Data Model</h1><p><strong>SQL++ has a more flexible data model:</strong></p>
<ul>
<li>It relaxes traditional SQL’s strict rules to handle modern, semi-structured data like JSON or CBOR.</li>
<li>SQL++ databases can store self-describing data, meaning you don’t need a predefined schema (data structure).</li>
</ul>
<p><strong>Supports diverse data types:</strong></p>
<ul>
<li>Data can be single values (scalars), tuples (a set of key-value pairs), collections (like arrays or multisets), or combinations of these.</li>
<li>Unlike traditional SQL, tuples in SQL++ are <strong>unordered</strong>, which means the order of attributes doesn’t matter.</li>
</ul>
<p><strong>Allows duplicate attribute names but discourages them:</strong></p>
<ul>
<li>This is to accommodate non-strict formats like JSON.</li>
<li>However, duplicate names can lead to unpredictable query results, so they’re not recommended.</li>
</ul>
<p><strong>Two kinds of missing values:</strong> <strong><code>NULL</code> and <code>MISSING</code></strong>:</p>
<ul>
<li><strong><code>NULL</code></strong>: Means an attribute exists but has no value.</li>
<li><strong><code>MISSING</code></strong>: Means the attribute doesn’t exist at all.</li>
<li>This distinction is useful for clearer query results and error handling.</li>
</ul>
<p><strong>Importance of</strong> <strong><code>MISSING</code></strong>:</p>
<ul>
<li>SQL++ doesn’t stop processing if some data is missing; instead, it marks those cases as <code>MISSING</code> and continues.</li>
<li>This makes queries more robust and tolerant of data inconsistencies.</li>
</ul>
<h1 id="Accessing-Nested-Data"><a href="#Accessing-Nested-Data" class="headerlink" title="Accessing Nested Data"></a>Accessing Nested Data</h1><p><strong>SQL-92 vs. Modern Data:</strong></p>
<ul>
<li>SQL-92 only supports tables with rows (tuples) containing simple values (scalars).</li>
<li>Modern data formats often include <strong>nested structures</strong>, where attributes can hold complex data types like arrays, tables, or even arrays of arrays.</li>
</ul>
<p><strong>Nested Data Example:</strong></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code1.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code2.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code3.png" alt="img"></p>
<ul>
<li>In the example, the <code>projects</code> attribute of an employee is an <strong>array of tuples</strong>, representing multiple projects each employee is involved in.</li>
</ul>
<p><strong>Querying Nested Data in SQL++:</strong></p>
<ul>
<li>SQL++ can handle such nested data without adding new syntax to SQL.</li>
<li>For example, a query can find employees working on projects with “security” in their names and output both the employee’s name and the project’s name.</li>
</ul>
<p><strong>How It Works:</strong></p>
<ul>
<li>SQL++ uses <strong>left-correlation</strong>, allowing expressions in the <code>FROM</code> clause to refer to variables declared earlier in the same clause.</li>
<li>For instance, <code>e.projects</code> accesses the projects of an employee <code>e</code>.</li>
<li>This relaxes SQL’s restrictions and effectively enables a join between an employee and their projects.</li>
</ul>
<p><strong>Using Variables in Queries:</strong></p>
<ul>
<li>SQL++ requires <strong>explicit</strong> use of variables (e.g., <code>e.name</code> instead of just <code>name</code>) because schema is optional and cannot guarantee automatic disambiguation.</li>
<li>If a schema exists, SQL++ can still optimize by rewriting the query for clarity and execution.</li>
</ul>
<p><strong>Flexibility with Nested Collections:</strong></p>
<ul>
<li>Variables in SQL++ can represent any type of data—whether it’s a table, array, or scalar.</li>
<li>These variables can be used seamlessly in <code>FROM</code>, <code>WHERE</code>, and <code>SELECT</code> clauses.</li>
</ul>
<p><strong>Aliases Can Bind to Any Data Type:</strong></p>
<ul>
<li>In SQL++, variables (aliases) don’t have to refer only to tuples.</li>
<li>They can bind to <strong>arrays of scalars</strong>, <strong>arrays of arrays</strong>, or any combination of scalars, tuples, and arrays.</li>
</ul>
<p><strong>Flexibility in Querying Nested Data:</strong></p>
<ul>
<li>Users don’t need to learn new query syntax for different data structures.</li>
<li>The same <strong>unnesting feature</strong> is used regardless of whether the data is an array of tuples or an array of scalars.</li>
</ul>
<p>Example:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code4.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code5.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code6.png" alt="img"></p>
<ul>
<li>If the <code>projects</code> attribute is an <strong>array of strings</strong> (instead of tuples), SQL++ queries can still process it.</li>
<li>The query would range over <code>e.projects</code> and bind <code>p</code> to each project name (a string).</li>
</ul>
<p><strong>Relaxed Semantics Compared to SQL:</strong></p>
<ul>
<li>In traditional SQL, the <code>FROM</code> clause binds variables strictly to tuples.</li>
<li>SQL++ generalizes this by treating the <code>FROM</code> clause as a function that can bind variables to <strong>any type of data</strong>—not just tuples.</li>
</ul>
<p><strong>Practical Outcome:</strong></p>
<ul>
<li>In the example, the <code>FROM</code> clause produced variable bindings like <code>&#123;e: employee_data, p: project_name&#125;</code>.</li>
<li>This allows the query to handle data structures that SQL would not support without extensions.</li>
</ul>
<h1 id="ABSENCE-OF-SCHEMA-AND-SEMI-STRUCTURED-DATA"><a href="#ABSENCE-OF-SCHEMA-AND-SEMI-STRUCTURED-DATA" class="headerlink" title="ABSENCE OF SCHEMA AND SEMI-STRUCTURED DATA"></a>ABSENCE OF SCHEMA AND SEMI-STRUCTURED DATA</h1><p>Schemaless Data:</p>
<ul>
<li>Many modern data formats (e.g., JSON) don’t require a predefined schema to describe their structure.</li>
<li>This allows for <strong>flexible and diverse data</strong>, but it also introduces <strong>heterogeneity</strong>.</li>
</ul>
<p>Types of Heterogeneity:</p>
<ul>
<li><strong>Attribute presence</strong>: Some tuples may have a specific attribute (e.g., <code>x</code>), while others may not.</li>
<li><strong>Attribute type</strong>: The same attribute can have different types across tuples. For example:<ul>
<li>In one tuple, <code>x</code> might be a string.</li>
<li>In another tuple, <code>x</code> might be an array.</li>
</ul>
</li>
<li><strong>Element types in collections</strong>: A collection (e.g., an array or a bag) can have elements of different types. For example:<ul>
<li>The first element could be a string, the second an integer, and the third an array.</li>
</ul>
</li>
<li><strong>Legacy or data evolution</strong>: These heterogeneities often result from evolving requirements or data conversions (e.g., converting XML to JSON).</li>
</ul>
<p>Heterogeneity Is Not Limited to Schemaless Data:</p>
<ul>
<li>Even structured databases can have heterogeneity. For example:<ul>
<li>Hive’s <strong>union type</strong> allows an attribute to hold multiple types, like a string or an array of strings.</li>
</ul>
</li>
</ul>
<p>How SQL++ Handles It:</p>
<ul>
<li>SQL++ is designed to work seamlessly with <strong>heterogeneous data</strong>, whether the data comes from a schemaless format or a schema-based system.</li>
<li>It offers features and mechanisms to process such data flexibly, without enforcing rigid structure requirements.</li>
</ul>
<h2 id="Missing-Attributes"><a href="#Missing-Attributes" class="headerlink" title="Missing Attributes"></a>Missing Attributes</h2><ol>
<li><p><strong>Representation of Missing Information</strong>:</p>
<ul>
<li><p>In SQL, a missing value is typically represented as <code>NULL</code> (e.g., Bob Smith’s title in the first example).</p>
</li>
<li><p>In SQL++, there’s an additional option: simply omitting the attribute altogether (as seen in the second example for Bob Smith).</p>
</li>
</ul>
</li>
<li><p><code>NULL</code> <strong>vs.</strong> <code>MISSING</code>:</p>
<ul>
<li><p><code>NULL</code>: Indicates the attribute exists but has no value.</p>
</li>
<li><p><code>MISSING</code>: Indicates the attribute is entirely absent.</p>
</li>
<li><p>SQL++ supports distinguishing between these two cases, unlike traditional SQL.</p>
</li>
</ul>
</li>
<li><p><strong>Why This Matters</strong>:</p>
<ul>
<li><p>Some data systems or formats (e.g., JSON) naturally omit <strong>missing</strong> attributes rather than assigning a <code>NULL</code> value.</p>
</li>
<li><p>SQL++ makes it easy to work with both approaches by allowing queries to handle <code>NULL</code> and <code>MISSING</code> values distinctly.</p>
</li>
</ul>
</li>
<li><p><strong>Query Behavior</strong>:</p>
<ul>
<li><p>Queries in SQL++ can propagate <code>NULL</code> and <code>MISSING</code> values as they are.</p>
</li>
<li><p>The system introduces the special value <code>MISSING</code> to represent absent attributes, allowing clear differentiation from <code>NULL</code>.</p>
</li>
</ul>
</li>
</ol>
<h2 id="MISSING-as-a-Value"><a href="#MISSING-as-a-Value" class="headerlink" title="MISSING as a Value"></a>MISSING as a Value</h2><p>What Happens When Data is Missing:</p>
<ul>
<li>If a query references an attribute that doesn’t exist in a tuple (e.g., <code>e.title</code> for Bob Smith), SQL++ assigns the value <code>MISSING</code>.</li>
<li>This avoids query failures and ensures processing can continue.</li>
</ul>
<p><strong>Three Cases Where</strong> <code>MISSING</code> <strong>is Produced</strong>:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code7.png" alt="img"></p>
<ul>
<li><strong>Case 1</strong>: Accessing a missing attribute. For example, <code>&#123;id: 3, name: &#39;Bob Smith&#39;&#125;.title</code> results in <code>MISSING</code>.</li>
<li><strong>Case 2</strong>: Using invalid input types for functions or operators (e.g., <code>2 * &#39;some string&#39;</code>).</li>
<li><strong>Case 3</strong>: When <code>MISSING</code> is an input to a function or operator, it propagates as <code>MISSING</code> in the output.</li>
</ul>
<p>SQL Compatibility Mode:</p>
<ul>
<li>In SQL compatibility mode, <code>MISSING</code> behaves like <code>NULL</code> for compatibility. For instance, <code>COALESCE(MISSING, 2)</code> will return <code>2</code>, just as <code>COALESCE(NULL, 2)</code> does in SQL.</li>
</ul>
<p><strong>Propagation of</strong> <code>MISSING</code> <strong>in Queries</strong>:</p>
<ul>
<li>In queries, <code>MISSING</code> values flow naturally through transformations, enabling consistent handling of absent data.</li>
<li>For example, in a <code>CASE</code> statement, if <code>e.title</code> evaluates to <code>MISSING</code>, the result of the entire <code>CASE</code> expression will also be <code>MISSING</code>.</li>
</ul>
<p><strong>Results with</strong> <code>MISSING</code>:</p>
<ul>
<li>If a query result includes <code>MISSING</code>, SQL++ will omit the attribute from the result tuple.</li>
<li>In communication with external systems like JDBC&#x2F;ODBC, <code>MISSING</code> is transmitted as <code>NULL</code> to ensure compatibility.</li>
</ul>
<h1 id="RESULT-CONSTRUCTION-NESTING-AND-GROUPING"><a href="#RESULT-CONSTRUCTION-NESTING-AND-GROUPING" class="headerlink" title="RESULT CONSTRUCTION,NESTING, AND GROUPING"></a>RESULT CONSTRUCTION,NESTING, AND GROUPING</h1><h2 id="Creating-Collections-of-Any-Value"><a href="#Creating-Collections-of-Any-Value" class="headerlink" title="Creating Collections of Any Value"></a>Creating Collections of Any Value</h2><p><strong>Power of</strong> <code>SELECT VALUE</code>:</p>
<ul>
<li>The <code>SELECT VALUE</code> clause in SQL++ allows constructing collections of any type of data, not just tuples.</li>
<li>It enables creating outputs that match the structure of nested data without flattening it unnecessarily.</li>
</ul>
<p>Example Query:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code8.png" alt="img"></p>
<ul>
<li>The query in Listing 10 demonstrates how to use <code>SELECT VALUE</code> to extract only the “security” projects of employees, resulting in a nested structure.</li>
<li>Each employee’s tuple includes their ID, name, title, and a collection of their security-related projects.</li>
</ul>
<p>Result:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code9.png" alt="img"></p>
<ul>
<li>Listing 11 shows the result where each employee has a field <code>security_proj</code> containing a nested collection of projects that match the condition (e.g., projects with “Security” in the name).</li>
</ul>
<p>Key Difference from Standard SQL:</p>
<ul>
<li>SQL’s <code>SELECT</code> clause can be viewed as shorthand for <code>SELECT VALUE</code>, but with differences:<ul>
<li>SQL automatically coerces subquery results into scalar values, collections of scalars, or tuples based on context.</li>
<li>In contrast, <code>SELECT VALUE</code> in SQL++ consistently produces a collection and does not apply implicit coercion.</li>
</ul>
</li>
</ul>
<p>Flexibility:</p>
<ul>
<li>SQL++ avoids implicit “magic” by explicitly treating <code>SELECT</code> as shorthand for <code>SELECT VALUE</code>.</li>
<li>This approach aligns more closely with functional programming principles, making it easier to handle and compose nested data results.</li>
</ul>
<h2 id="GROUP-BY-and-GROUP-AS"><a href="#GROUP-BY-and-GROUP-AS" class="headerlink" title="GROUP BY and GROUP AS"></a>GROUP BY and GROUP AS</h2><p><strong>Introduction to</strong> <code>GROUP BY ... GROUP AS</code>:</p>
<ul>
<li>This feature extends SQL’s <code>GROUP BY</code> functionality, allowing groups (and their contents) to be directly accessible in the <code>SELECT</code> and <code>HAVING</code> clauses.</li>
<li>It is more efficient and intuitive for creating nested results compared to traditional SQL, especially when the output nesting doesn’t directly align with the input data structure.</li>
</ul>
<p>How It Works:</p>
<ul>
<li><strong>Generalization</strong>: Unlike SQL, which limits access to grouped data in <code>GROUP BY</code>, SQL++ allows accessing the full group details as part of the query.</li>
<li><strong>Pipeline Model</strong>: SQL++ processes queries in a step-by-step fashion, starting with <code>FROM</code>, followed by optional clauses like <code>WHERE</code>, <code>GROUP BY</code>, <code>HAVING</code>, and ending with <code>SELECT</code>.</li>
</ul>
<p>Example:</p>
<ul>
<li>In the query from <strong>Listing 12</strong>, employees are grouped by their project names (converted to lowercase), and a nested list of employees for each project is created.</li>
<li>The <code>GROUP BY LOWER(p) AS p GROUP AS g</code> clause groups data and stores each group in <code>g</code>.</li>
<li>The <code>SELECT</code> clause then extracts project names and employees.</li>
</ul>
<p>Result:</p>
<ul>
<li>The output (shown in <strong>Listing 13</strong>) contains nested objects:</li>
<li><ul>
<li>Each object has a <code>proj_name</code> (e.g., <code>&#39;OLTP Security&#39;</code>) and an <code>employees</code> field listing the names of employees associated with that project.</li>
</ul>
</li>
</ul>
<p><strong>Details of</strong> <code>GROUP BY ... GROUP AS</code>:</p>
<ul>
<li>The clause produces bindings like the ones in <strong>Listing 14</strong>, where each group (<code>g</code>) includes all the data for its corresponding key (<code>p</code>).</li>
<li>The result allows users to flexibly access and format the grouped data.</li>
</ul>
<p>SQL++ Flexibility:</p>
<ul>
<li>SQL++ allows placing the <code>SELECT</code> clause either at the start or the end of a query block, enhancing readability and flexibility.</li>
<li>This approach is more consistent with functional programming and reduces constraints found in traditional SQL.</li>
</ul>
<p>Advanced Features:</p>
<ul>
<li>SQL++ supports additional analytical tools like <code>CUBE</code>, <code>ROLLUP</code>, and <code>GROUPING SETS</code>, making it highly compatible with SQL but better suited for nested and semi-structured data.</li>
</ul>
<h2 id="Aggregate-Functions"><a href="#Aggregate-Functions" class="headerlink" title="Aggregate Functions"></a>Aggregate Functions</h2><p>Limitations of Traditional SQL Aggregate Functions:</p>
<ul>
<li>Aggregate functions like <code>AVG</code> and <code>MAX</code> in traditional SQL lack <strong>composability</strong>.</li>
<li>They work directly on table columns but don’t easily integrate with more complex expressions or subqueries.</li>
</ul>
<p>SQL++ Solution:</p>
<ul>
<li>SQL++ introduces <strong>composable aggregate functions</strong>, such as <code>COLL_AVG</code> (for calculating the average of a collection) and <code>COLL_MAX</code>.</li>
<li>These functions take a <strong>collection</strong> as input and return the aggregated value.</li>
</ul>
<p>Importance of Composability:</p>
<ul>
<li>In SQL++, data is conceptually <strong>materialized</strong> into a collection first, then passed to the composable aggregate function.</li>
<li>While this materialization is conceptual, SQL++ engines optimize the execution (e.g., using pipelined aggregation).</li>
</ul>
<p>Example 1: Calculating the Average Salary of Engineers:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code10.png" alt="img"></p>
<ul>
<li><strong>SQL Query</strong> (Listing 15): Uses <code>AVG(e.salary)</code> directly.</li>
<li><strong>SQL++ Core Query</strong> (Listing 16): Converts <code>e.salary</code> into a collection and applies the <code>COLL_AVG</code> function.</li>
<li>SQL++ clearly defines the flow of data, making it more intuitive and flexible.</li>
</ul>
<p>Example 2: Calculating the Average Salary of Engineers by Department:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code11.png" alt="img"></p>
<ul>
<li><strong>SQL Query</strong> (Listing 17): Uses <code>GROUP BY</code> and <code>AVG</code>.</li>
<li><strong>SQL++ Core Query</strong> (Listing 18):<ul>
<li>Uses <code>GROUP BY ... GROUP AS</code> to form groups.</li>
<li>Feeds each group into <code>COLL_AVG</code> to calculate the average salary.</li>
<li>Constructs the result using the <code>SELECT VALUE</code> clause, explicitly specifying the output format.</li>
</ul>
</li>
</ul>
<p>Flexibility of SQL++ Style:</p>
<ul>
<li>SQL++ allows the <code>SELECT</code> clause to be written at the end of a query block, consistent with functional programming styles.</li>
<li>This enhances readability and composability while maintaining compatibility with SQL.</li>
</ul>
<h1 id="Pivoting-and-Unpivoting"><a href="#Pivoting-and-Unpivoting" class="headerlink" title="Pivoting and Unpivoting"></a>Pivoting and Unpivoting</h1><h2 id="UNPIVOT-Transforming-Attributes-into-Rows"><a href="#UNPIVOT-Transforming-Attributes-into-Rows" class="headerlink" title="UNPIVOT: Transforming Attributes into Rows"></a>UNPIVOT: Transforming Attributes into Rows</h2><ol>
<li><p><strong>What is Unpivoting?</strong></p>
<ul>
<li><p>Unpivoting is the process of converting attribute names (used as keys) into data rows.</p>
</li>
<li><p>This is useful for cases where key-value pairs in the data need to be analyzed as individual rows.</p>
</li>
</ul>
</li>
<li><p><strong>Example (Listing 19-21)</strong>:</p>
</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code12.png" alt="img"></p>
<ul>
<li><p>Input: A <code>closing_prices</code> collection where stock symbols (<code>amzn</code>, <code>goog</code>, <code>fb</code>) are attributes with prices as values.</p>
</li>
<li><p>Query (Listing 20): The <code>UNPIVOT</code> clause transforms these attributes into rows with fields for <code>symbol</code> and <code>price</code>.</p>
</li>
<li><p>Output (Listing 21): A flattened structure where each row contains the date, stock symbol, and price.</p>
</li>
</ul>
<h2 id="Pivoting"><a href="#Pivoting" class="headerlink" title="Pivoting"></a>Pivoting</h2><ol>
<li><strong>Purpose of Pivoting</strong>:<ul>
<li>Pivoting transforms rows into attributes (columns).</li>
</ul>
</li>
<li><strong>Example from Listings 23-25</strong>:</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code13.png" alt="img"></p>
<ul>
<li><strong>Input (Listing 23)</strong>: Rows of <code>today_stock_prices</code> where each stock symbol and its price are separate rows.</li>
<li><strong>Query (Listing 24)</strong>: The <code>PIVOT</code> operation turns these rows into a single object, using <code>sp.symbol</code> as attribute names and <code>sp.price</code> as their values.</li>
<li><strong>Output (Listing 25)</strong>: A tuple where each stock symbol (<code>amzn</code>, <code>goog</code>, <code>fb</code>) is an attribute, and their corresponding prices are the values.</li>
</ul>
<p><strong>Combining Grouping and Pivoting</strong></p>
<ol>
<li><strong>Using Pivot with Grouping</strong>:</li>
<li><ul>
<li>Combining <code>GROUP BY</code> and <code>PIVOT</code> enables aggregation of grouped rows into a more structured output.</li>
<li>This is particularly useful when working with time-series data or hierarchical datasets.</li>
</ul>
</li>
<li><strong>Example Query (Listing 26)</strong>:</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code14.png" alt="img"></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/SQLPP_code15.png" alt="img"></p>
<ul>
<li>Input: Data from <code>stock_prices</code> (Listing 27), which includes stock prices for multiple dates as individual rows.</li>
<li>Query:<ul>
<li>Groups the data by <code>date</code> using <code>GROUP BY sp.date</code>.</li>
<li>Pivots the grouped rows to produce a nested structure where each date contains all its stock prices as attributes.</li>
</ul>
</li>
<li>Output (Listing 28): For each date, an object with a <code>prices</code> field lists the stock symbols as attributes and their respective prices as values.</li>
</ul>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>SQL++ identifies aggregate functions as an SQL violation of functional composability. Give an example of an aggregate function and describe how it violates SQL’s functional composability.</strong></p>
<ul>
<li><p><strong>Aggregate Function</strong>:<code>COLL_AVG()</code></p>
</li>
<li><p><strong>Violation Explanation</strong>:</p>
<ul>
<li><p>In traditional SQL, aggregate functions like <code>AVG</code> processes the column and returns a single value.</p>
</li>
<li><p>In SQL++, this issue is resolved by providing <strong>composable versions</strong> of aggregate functions, such as <code>COLL_AVG</code>, which operate on collections, allowing intermediate results to flow naturally into the aggregation.</p>
</li>
</ul>
</li>
</ul>
<p><strong>With SQL++, what is the difference between NULL and Missing?</strong></p>
<p><code>NULL</code>: Indicates that an attribute exists but has no value.</p>
<p><code>MISSING</code>: Indicates that an attribute is completely absent in the data.</p>
<p><strong>True or false: One must define a schema for data prior to using SQL++.</strong> </p>
<p>False:</p>
<ul>
<li>SQL++ supports <strong>schema-optional</strong> and <strong>schema-less</strong> data formats, such as JSON.</li>
<li>While schemas can improve query optimization and validation, SQL++ can process data without requiring predefined schemas, making it highly flexible for semi-structured data use cases.</li>
</ul>
<p><strong>How does the I lease prevent a thundering herd?</strong></p>
<p>The I lease (Inhibit Lease) prevents a thundering herd problem by ensuring that only one read session at a time is allowed to query the RDBMS for a missing key-value pair in the Key-Value Store (KVS). Here’s how it works:</p>
<ol>
<li><p><strong>Thundering Herd Problem</strong>:</p>
<ul>
<li><p>When a key-value pair is not found in the KVS (a <strong>KVS miss</strong>), multiple read sessions might simultaneously query the RDBMS to fetch the value.</p>
</li>
<li><p>This can overload the RDBMS and degrade performance under high concurrency.</p>
</li>
</ul>
</li>
<li><p><strong>Role of the I Lease</strong>:</p>
<ul>
<li><p>When the first read session encounters a KVS miss, it requests an I lease for the key.</p>
</li>
<li><p>Once the I lease is granted, the KVS prevents other read sessions from querying the RDBMS for the same key.</p>
</li>
<li><p>All other read sessions must “back off” and wait for the value to be updated in the KVS by the session holding the I lease.</p>
</li>
</ul>
</li>
<li><p><strong>Result</strong>:</p>
<ul>
<li><p>The session with the I lease queries the RDBMS, retrieves the value, and populates the KVS.</p>
</li>
<li><p>Subsequent read sessions observe a <strong>KVS hit</strong> and do not need to access the RDBMS.</p>
</li>
<li><p>This mechanism avoids simultaneous RDBMS queries, effectively solving the thundering herd problem.</p>
</li>
</ul>
</li>
</ol>
<p><strong>What is the difference between invalidate and refresh&#x2F;refill for maintaining the cache consistent with the database management system?</strong></p>
<ul>
<li><strong>Invalidate</strong>: Deletes stale cache entries to prevent incorrect reads, but at the cost of forcing subsequent queries to access the RDBMS.</li>
<li><strong>Refresh&#x2F;Refill</strong>: Proactively updates the cache with new data, ensuring consistent reads while reducing future load on the RDBMS at the expense of immediate computation.</li>
</ul>
<p><strong>Describe how CAMP inserts a key-value pair in the cache.</strong></p>
<p><strong>Check Cache Capacity</strong></p>
<ul>
<li>If there is <strong>enough memory</strong> to store the new key-value pair:<ul>
<li>The pair is inserted directly into the appropriate <strong>priority group</strong> based on its cost-to-size ratio.</li>
<li>L is not updated.</li>
</ul>
</li>
<li>If the cache is <strong>full</strong>:<ul>
<li>CAMP selects one or more key-value pairs to <strong>evict</strong> based on their H(p) values.</li>
<li>It removes the pair(s) with the <strong>lowest H(p)</strong> values until there is sufficient space for the new pair.</li>
</ul>
</li>
</ul>
<p><strong>Insert the New Pair</strong></p>
<ul>
<li>The new key-value pair p is added to the cache, and its H(p) value is computed and recorded.</li>
<li>The pair is placed in the appropriate priority queue based on its cost-to-size ratio.</li>
</ul>
<p><strong>How does BG compute the SoAR of a database management system?</strong> </p>
<ol>
<li>Define the SLA.</li>
<li>Run a series of experiments with increasing numbers of threads (T) to find the peak throughput while ensuring SLA compliance.</li>
</ol>
<p>Reference: <a href="https://escholarship.org/content/qt2bj3m590/qt2bj3m590_noSplash_084218340bb4e928c05878f04d01f04d.pdf">https://escholarship.org/content/qt2bj3m590/qt2bj3m590_noSplash_084218340bb4e928c05878f04d01f04d.pdf</a></p>
]]></content>
      <categories>
        <category>数据库系统</category>
      </categories>
  </entry>
  <entry>
    <title>Adaptive Execution of Compiled Queries</title>
    <url>/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/03/02/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20Adaptive%20Execution%20of%20Compiled%20Queries/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>The paper addresses 3 problems:</p>
<ol>
<li>How to reduce the compilation time of complex but fast queries?</li>
<li>How to reduce the compilation time of extremely large queries?</li>
<li>How to reduce the compilation time of the first incoming query?</li>
</ol>
<p>The paper provides 2 novel ideas and elaborates the proposed approach for the implementation of each idea:</p>
<ol>
<li>Adaptive execution: For a specific query pipeline, the paper tracks the progress of worker threads, predicts the remaining workload duration under 3 execution modes based on the overall state of all threads in the pipeline, and finally selects the mode with the shortest duration to apply to all threads.</li>
<li>Fast bytecode interpretation: Based on register machine, the paper implements linear-time liveness computation through processing basic blocks in intervals and using algorithms such as the disjoint set and path compression, which optimizes register allocation further. Furthermore, the bytecode interpreter behaves equivalently to the generated machine code, ensuring seamless switching between interpretation and machine code.</li>
</ol>
<p>Analytical and experimental findings: For different scale factors, adaptive execution can switch to the mode with the optimal performance, ensuring the lowest execution time compared to other static mode selections. For action, adaptive execution can immediately start to process pipeline morsels on all available worker threads and dynamically switch modes for pipelines with heavy workloads, allowing it to finish queries 10%, 40%, and 80% faster than its competitors. While interpreted code is slower than compiled code, it is faster than PostgreSQL and scales as well as compiled code when using multiple cores. Furthermore, the byte interpreter scales perfectly and can process the large query in a short time.</p>
<h1 id="Paper-Strength"><a href="#Paper-Strength" class="headerlink" title="Paper Strength"></a>Paper Strength</h1><ul>
<li>The paper provides a clear and detailed description of the problem to be addressed. In Section II, it presents the multi-step process of a SQL query in HyPer through a flowchart, highlighting that the LLVM compilation tasks takes the majority of the overall execution time. By comparing the compilation and execution times of different execution modes on TPC-H Query 1 on scale factor 1, the paper introduces the trade-off between interpreters and compilers, demonstrating that different execution modes can be applied to different parts of the same query. Furthermore, the paper analyzes the largest TPC-H and TPC-DS queries, concluding that compilation time grows super-linearly with the query size.</li>
<li>The paper provides a comprehensive literature review. In Section VI, it references experimental results from other papers on compilation time, concluding that compiling to LLVM IR is faster than compiling to C. Based on the personal experience, the paper highlights that query compilation latency becomes a major problem in production environments, this makes adaptive execution a crucial component for making query compilation truly practical. It then explores the feasibility of integrating adaptive execution into database systems such as MemSQL, LegoBase, and Microsoft Hekaton. Furthermore, the paper demonstrates the advantages of adaptive execution over automatic plan caching, i.e. the ability to re-optimize queries on every execution. Finally, the paper discusses the similarities and differences between adaptive execution and execution engines in programming languages, such as JVM, V8, and CLR.</li>
<li>The paper achieves significant improvements, the linear-time liveness computation. The traditional solution for computing the liveness of each block individually usually takes $O(n^2)$ runtime. However, the paper proposes a linear-time algorithm. The algorithm labels all basic blocks in reverse postorder and organizes them into a dominator tree, which allows interpreter to determine the relationships of basic blocks in $O(1)$ time and paves the way for identifying loops. It identifies the innermost enclosing loop of each basic block by using the disjoint set with path compression, and, based on the the distribution of basic blocks related to the definition and uses of a certain value, determines the lifetime of that value. The low cost of the computation is primarily attributed to the appropriate data structures.</li>
</ul>
<h1 id="Paper-Weakness"><a href="#Paper-Weakness" class="headerlink" title="Paper Weakness"></a>Paper Weakness</h1><ul>
<li>The paper does not provide a clear explanation of the basic concepts. In Section II, it does not explain the meaning of latency and throughput in the context of HyPer, nor the relationship between them, so readers may not realize the significance of the tradeoff and possibly do not understand how performance improvements in later experimental results are achieved. For example, readers might not figure out why interpreters can achieve very low latency by sacrificing throughput. Therefore, the paper should explain these concepts.</li>
<li>The paper’s experiments are not comprehensive enough. As a core component in data processing and storage, adaptive execution framework is only shown to have an advantage in execution time, while experiments demonstrating its physical device utilization, stability and fault recovery performance are lacking. These metrics are also critical in evaluating the overall performance of the framework. For example, for the same set of queries, if the execution time is short but the CPU and memory usage are extremely high, or if the probability of throwing an exception is high and a large amount of time is required for fault recovery, then the framework still has room for improvement. Therefore, the paper should include experimental results for these metrics.</li>
<li>The paper does not provide a detailed explanation of how to translate into VM code. In Section IV-B, the paper mentions that subsumed instructions will not be translated, but it does not specify which types of instructions are subsumed or in what manner they are subsumed. These omissions can confuse readers and hinder their understanding of the translation pseudocode. Therefore, the paper should explain principles behind subsumed instructions.</li>
</ul>
<p>Reference: <a href="https://db.in.tum.de/~leis/papers/adaptiveexecution.pdf">https://db.in.tum.de/~leis/papers/adaptiveexecution.pdf</a></p>
]]></content>
      <categories>
        <category>高级数据存储</category>
      </categories>
  </entry>
  <entry>
    <title>Managing Cold Data in a Memory-Optimized Database</title>
    <url>/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/04/06/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20Managing%20Cold%20Data%20in%20a%20Memory-Optimized%20Database/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>The paper addresses 2 problems:</p>
<ol>
<li>How to migrate records to and from the cold store.</li>
<li>How to read and update records in the cold store in a transactionally consistent manner.</li>
</ol>
<p>The paper provides 3 novel ideas and elaborates the proposed approach for the implementation of each idea:</p>
<ol>
<li>A unified interface that hides the physical location of a record to higher layer: the collaboration among cold store, access filters, private cache, and update memo.</li>
<li>Minimizing the overhead caused by accessing secondary storage: each transaction has its own private cache.</li>
<li>Seamless migration between hot and cold stores: the system performs migration using insert and delete operations in a transaction.</li>
</ol>
<p>Analytical and experimental findings: The paper evaluates the performance of Siberia on both the YCSB Benchmark and Multi-step read&#x2F;update workload, showing the following conclusions. With realistic client delay, the throughput loss is only 3%. Even under extreme cold access data rate, the in-memory Siberia machinery results in a low performance loss. When live migration is active, the system’s performance remains stable. The overhead of accessing the memo is expensive, which means that memo cleaning is important for improving performance. For read-only transactions at realistic cold data access rates of 5% and 10%, the performance losses are 7% and 14% respectively, which are acceptable. For update-only transactions, 5% cold data updates lead to 8% throughput loss, 10% cold data update rates lead to 13% throughput loss, which are also acceptable. For the YCSB workload, as the access skew decreases and the memory to database size ratio increases, performance degrades, and read-heavy workloads exhibit lower abort rates for transactions at higher skew rates compared to write-heavy workloads.</p>
<h1 id="Paper-Strength"><a href="#Paper-Strength" class="headerlink" title="Paper Strength"></a>Paper Strength</h1><ul>
<li>The paper provides a comprehensive literature review. In the HEKATON STORAGE AND INDEXING section, the paper briefly outlines the index and data storage structure of HEKATON and its read and update operations based on MVCC. In the RELATED WORK section, the paper analyzes how existing database systems handle cold data, explaining that Hyper manages cold data using virtual pages, Stoica et al propose separating hot and cold data into different memory locations. Finally, the paper describes the working principle of Anti-caching and highlights its 2 drawbacks: limited space savings and repeated execution.</li>
<li>The paper provides a detailed description of the working principles after integrating SIBERIA into HEKATON. It presents the workflow of 2 transactions used during data migration to the cold store. Insert and update operations ensure that data is placed into the hot store to avoid the overhead of checking the cold store. Delete and read operations utilize notices in the update memo to perform concurrency control and conflict detection. Cold store cleaning is also driven by the update memo, which enables the timely removal of records from the cold store that are no longer visible to any active transactions. Furthermore, validation leverages the notices in the update memo as well and computes TsBoundSer to ensure the correctness of serializable transactions, thereby enhancing phantom detection.</li>
<li>The paper presents the relatively comprehensive experiment. The experiments are based on the YCSB Benchmark and Multi-step read&#x2F;update workload, which allow for testing Siberia’s performance under different workloads. Moreover, the paper evaluates the pure in-memory overhead of Siberia, the overhead of running live migration, and the overhead of the update memo on the path to accessing a cold record. Furthermore, under the YCSB workload, it demonstrates the relationship among workload skew, memory to database size ratios, and workload performance.</li>
</ul>
<h1 id="Paper-Weakness"><a href="#Paper-Weakness" class="headerlink" title="Paper Weakness"></a>Paper Weakness</h1><ul>
<li>The paper does not describe the process of integrating Siberia in HEKATON at the code level, but only mentions integration at the level of data processing and storage mechanisms. It details the cold data migration process and explains how update memo notices in the insert, delete, read, and update operations work in collaboration with HEKATON’s versioning and concurrency control. However, the paper does not address how Siberia is integrated into HEKATON at the code level, such as by identifying core functions, code segments, and the corresponding modifications. This omission makes it difficult for readers to easily re-achieve the Siberia. Therefore, the paper should at least provide an outline of the code modifications.</li>
<li>In the Synthetic End-to-End Workload section, the paper does not discuss the throughput loss under moderate to high cold data access rates. The paper only presents the throughput loss at 5% and 10% cold data access rates, claiming these are realistic cold data access rates. First, the paper fails to explain which report or literature supports that 5% and 10% are “realistic cold data access rates” in the Read-Only Transactions section. Second, we assume the above data is accurate, but the paper does not describes how the throughput loss changes when cold data access rates exceed 10%. This omission prevents readers from gaining a comprehensive understanding of the performance of handling workload in Siberia. Therefore, the paper should explain how the realistic cold data access rates were determined and describe the changes in throughput loss under moderate to high cold data access rates.</li>
<li>The paper does not discuss Siberia’s performance limitations or the conditions under which its performance might degenerate, despite the Experiments section showcasing impressive performance. It is possible that such impressive performance comes at the cost of high hardware utilization such as high CPU usage. Alternatively, while integrating Siberia in HEKATON may enhance the handling of hot and cold data, it could potentially compromise some of HEKATON’s original features. Therefore, the paper should clarify the situations under which Siberia’s performance may degenerate.</li>
</ul>
<p>Reference: <a href="https://www.vldb.org/pvldb/vol7/p931-eldawy.pdf">https://www.vldb.org/pvldb/vol7/p931-eldawy.pdf</a></p>
]]></content>
      <categories>
        <category>高级数据存储</category>
      </categories>
  </entry>
  <entry>
    <title>Orca - A Modular Query Optimizer Architecture for Big Data</title>
    <url>/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/02/11/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20Orca/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>The paper addresses 2 problems:</p>
<ol>
<li>How to design a query optimizer that can handle big data and complex analytical queries while ensuring the generation of efficient query plans.</li>
<li>Exploring the application of advanced query optimization theories in production environments.</li>
</ol>
<p>The paper provides 4 novel ideas and elaborates the proposed approach for the implementation of each idea:</p>
<ol>
<li>Decoupling the optimizer from the DB systems through DXL: Different DB systems need to implement 3 translators, Query2DXL, MD Provider, and DXL2Plan, to support Orca.</li>
<li>Use Memo and Group Hash Tables to optimize: Each group in the Memo stores all logically equivalent group expressions for a given operation, including enforcer operators. The group hash tables record the optimal implementation for each optimization request, i.e., the best group expressions. During query optimization, by retrieving the best group expression (best GExprs) corresponding to a given optimization request for each group and its child groups, these best GExprs are linked together to form the best execution plan.</li>
<li>Implementing the parallel query optimization with jobs dependency graph and job queues: If a group is currently processing an optimization job, other jobs will be placed in a queue to wait. Once the job is completed, its results can be utilized by subsequent jobs.</li>
<li>Developed efficient tools for testing: AMPERe is used to catch errors and generate dump files for later replaying to debug. TAQO samples plans uniformly based on the optimization requests’ linkage structure and evaluates the optimizer’s accuracy by calculating the correlation score between the ranking of sampled plans based on estimated costs and their ranking based on actual costs.</li>
</ol>
<p>Analytical and experimental findings: Based on the TPC-DS Benchmark, a limited set of queries was used to test GPDB legacy query optimizer (111 queries, MPP) and Orca, as well as Impala (31 queries, Hadoop), Stinger (19 queries, Hadoop), Presto (12 queries, Hadoop) and HAWQ. The results concluded that Orca matched or outperformed the GPDB optimizer in 80% of query executions. For 14 queries, Orca achieved a speed-up ratio of at least 1000× compared to the GPDB optimizer. Presto failed to process any TPC-DS queries under all test conditions, and query execution performance on HAWQ was generally superior to that on Impala and Stinger.</p>
<h1 id="Paper-Strength"><a href="#Paper-Strength" class="headerlink" title="Paper Strength"></a>Paper Strength</h1><ul>
<li>The paper provides a comprehensive literature review and covers the prerequisite knowledge needed to understand Orca. In the PRELIMINARIES, the paper briefly analyzes GPDB and explains its 3 core components: master, segments, and the interconnect layer, explains how SQL and query optimizers have been integrated into big data processing components such as Hadoop. Furthermore, it analyzes the advantages of the HAWQ architecture which is optimized by Orca compared to Impala and Presto. In the RELATED WORK, the paper introduces Volcano and Cascades, highlighting that Cascades offers greater flexibility than Volcano, then discusses various query optimizer implementations for big data in MPP databases, such as PDW and SCOPE. Finally, the paper reviews existing efforts to integrate SQL with Hadoop, such as converting queries into MapReduce jobs (Hive) and co-location of DBMS and Hadoop technologies (Hadapt).</li>
<li>The paper presents highly valuable solutions for query optimization on big data: DXL and Parallel Query Optimization. Different DBs only need to implement their own Query2DXL and DXL2Plan translators to achieve compatibility with Orca, giving Orca the potential to be adapted to any existing database system. To satisfy the core requirement of big data, concurrent processing, Orca constructs an optimization jobs dependency graph to determine the dependencies between jobs. Parent jobs can only be executed after their child jobs are completed, while independent jobs can run in parallel. For handling resource contention, Orca places incoming jobs in a job queue, where they wait until the running job is finished. These waiting jobs can then leverage the results generated by the completed jobs.</li>
<li>The paper elaborates steps of Orca’s optimization in the QUERY OPTIMIZATION section. In the Exploration phase, Orca creates logically equivalent expressions and deduplicates them using the Memo. In the Statistics Derivation phase, Orca estimates the cardinality and data skew for Memo groups. In the Implementation phase, Orca generates physical implementations of logical expressions. In the Optimization phase, multiple execution plans are generated, incorporating enforcer operators when necessary. The cost model is then used to select the execution plan with the lowest cost. These details effectively help readers gain a high-level understanding of Orca’s working principles.</li>
</ul>
<h1 id="Paper-Weakness"><a href="#Paper-Weakness" class="headerlink" title="Paper Weakness"></a>Paper Weakness</h1><ul>
<li>The paper lacks a description of how execution plan costs are computed during the Optimization phase. Specifically, there is a complete absence of discussion on the cost model, which should be a core functionality of Orca. This is particularly crucial when dealing with big data and a shared-nothing architecture, where the cost model here may differ from the Selinger’s cost model by incorporating coordination and communication across multiple worker nodes and need to account for network bandwidth. I recommend that the paper include a detailed description of the cost model and discuss its behavior in both monolithic and distributed DB systems.</li>
<li>The paper’s experimental evaluation for MPP databases is based on a limited dataset. Orca was only compared against the GPDB legacy query optimizer using 119 queries. Both the number of queries and the number of MPP database optimizer being compared are insufficient to convincingly demonstrate Orca’s advantages over other MPP database optimizers. Therefore, the experiment should conduct comparisons with a broader range of MPP databases, such as Amazon Redshift and Teradata Aster, to provide a more comprehensive evaluation.</li>
<li>The paper’s experiments do not reflect Orca’s hardware utilization, such as CPU usage, memory consumption, etc. Orca will possibly be used in a shared-nothing architecture, it will run across multiple servers. However, in the production environment, these servers will be not dedicated solely to Orca, other processes including databases that Orca will optimize is going to be probably running on the same server where Orca will be. If Orca has excessively high CPU or memory usage, it could negatively impact those databases’ query execution and the performance of other applications. This effect could accumulate across multiple servers, leading to significant performance degradation. Therefore, the paper should also include an evaluation of Orca’s hardware resource consumption.</li>
</ul>
<p>Reference: <a href="https://dl.acm.org/doi/10.1145/2588555.2595637">https://dl.acm.org/doi/10.1145/2588555.2595637</a></p>
]]></content>
      <categories>
        <category>高级数据存储</category>
      </categories>
  </entry>
  <entry>
    <title>TicToc</title>
    <url>/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/2025/03/18/%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/Review%20Report%20-%20TicToc/</url>
    <content><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>The paper addresses 1 problem:</p>
<ol>
<li>How to design an efficient concurrency control algorithm to improve scalability of OLTP DBMSs in the multi-core environment?</li>
</ol>
<p>The paper provides 4 novel ideas and elaborates the proposed approach for the implementation of each idea:</p>
<ol>
<li>TicToc algorithm: Implemented on top of OCC, it ensures a transaction’s timestamp will be calculated lazily at the commit time based on tuples this transaction process, which will improve the parallelism as well.</li>
<li>No-wait locking in validation phase: If a transaction fails to acquire a lock for a tuple in the write set, the validation phase will be aborted immediately. TicToc will restarts this phase after a period of time.</li>
<li>Preemptive aborts: Based on an approximate commit timestamp together with the local rts and wts, it is possible to determine whether to abort a transaction before locking the tuples in its write set.</li>
<li>Timestamp history: When a read tuple’s local rts is lower than the commit_ts and its wts differs from the latest wts, further inspection of the tuple’s history buffer is conducted to decide whether to start the validation phase.</li>
</ol>
<p>Analytical and experimental findings: The paper compares five algorithms: TicToc, Silo, HEKATON, DL_DETECT, and NO_WAIT. In the TPC-C results, with 4 warehouses, TicToc achieves the highest throughput and a lower abort rate than Silo. As the number of warehouses increases, TicToc’s throughput is eventually surpassed by Silo around 80 warehouses, but its abort rate remains lower than that of Silo. In the YCSB results, when processing read-only transactions, TicToc’s throughput is close to Silo’s and higher than that of the other algorithms; for read-write transactions under medium contention, TicToc maintains throughput similar to SIlo’s while its abort rate is significantly lower than those of Silo, HEKATON, and NO_WAIT; under high contention conditions, TicToc’s throughput far surpasses that of Silo, although its abort rate becomes more close to Silo’s. Tests for optimization indicate that most of the performance improvements come from the no-wait and preemptive aborts. Furthermore, TicToc’s timestamp growth rate is substantially lower than that of TS_ALLOC. When the isolation level is lower, TicToc shows improved throughput and a reduced abort rate, but the degree of these changes is not as pronounced compared to the other algorithms.</p>
<h1 id="Paper-Strength"><a href="#Paper-Strength" class="headerlink" title="Paper Strength"></a>Paper Strength</h1><ul>
<li>The paper provides a clear description of the background knowledge and the problem to be addressed. It elaborates on the weaknesses of the 2PL strategy and highlights that the T&#x2F;O strategy, such as MVCC and OCC, has gradually become mainstream. It then points out that the centralized timestamp allocator and the CPU’s cache coherence protocol in traditional T&#x2F;O algorithms have led to a timestamp allocation bottleneck. Moreover, all the hardware solutions mentioned in the paper fail to perfectly align with the architecture of most modern CPUs, and their performance remains suboptimal even if implemented. Furthermore, the paper briefly describes the execution phases of OCC and introduces 2 optimized approaches based on OCC, that is DTA and Silo, and highlights that both solutions still suffer from scalability bottlenecks. In order to tackle these problems, the paper presents TicToc algorithm.</li>
<li>The paper provides code, charts, and an example for the core processes of the TicToc algorithm, enabling readers to quickly understand the implementation details and workflow. In Section 3.2, the paper presents pseudocode for the Read Phase, Validation Phase, and Write Phase, which clearly illustrates the design considerations in addressing conflicts in concurrent and parallel scenarios and the decentralized timestamp assignment. In Section 3.6, the paper demonstrates the structure used for storing read and write timestamps and, through pseudocode, effectively presents a solution to the potential overflow problem of the delta attribute. Moreover, in Section 3.3, the paper provides an example of interleaved transaction execution, accompanied by a bar chart, clearly displaying TicToc’s high flexibility and performance in handling concurrency and parallelism challenges.</li>
<li>The paper presents a comprehensive experiment of TicToc on DBx1000. It assesses TicToc’s performance in both TPC-C and YCSB scenarios, comparing throughput and abort rate under various contention levels and different numbers of warehouses, with Silo, HEKATON, DL_DETECT, and NO_WAIT. The paper also evaluates TicToc optimizations, emphasizing the contributions of the no-wait and pre-abort to performance improvements. Moreover, the paper compares TicToc’s timestamp growth rate and linear timestamp growth rate, and the differences in throughput and abort rate between TicToc and other 4 algorithms under different isolation levels.</li>
</ul>
<h1 id="Paper-Weakness"><a href="#Paper-Weakness" class="headerlink" title="Paper Weakness"></a>Paper Weakness</h1><ul>
<li>The paper does not show the process of integrating the TicToc into DBx1000. As a concurrency control algorithm, TicToc must be interfaced with other key components such as transactions, indexes, and logs, which involves a considerable amount of work. However, the paper fails to address this aspect, thereby preventing readers from easily re-achieving the algorithm. Therefore, the paper should at least provide a brief outline of the necessary steps to help readers implement this functionality.</li>
<li>The paper’s experiments fail to demonstrate the general applicability of the TicToc across a range of databases. The evaluation was conducted only in the DBx1000 environment, thereby only substantiating TicToc’s high performance within DBx1000. But many commercially available databases, such as SQL Server and MySQL etc., exhibit distinct characteristics under varying workloads, which could potentially lead to different performance when using TicToc. However, the paper is entirely silent on this aspect. Therefore, the paper should also incorporate integration and testing of TicToc on these mainstream databases.</li>
<li>The paper fails to provide code or explanations for certain key concepts in the critical OPTIMIZATIONS section. According to the experimental results, the no-wait and preemptive aborts lead to significant performance improvements. However, the OPTIMIZATIONS section does not present any relevant code. For instance, in the No-Wait Locking in Validation Phase section, the paper does not clarify what thrashing problems mean in the given context, nor does it showcase the code for the no-wait or highlight its modifications relative to the original TicToc implementation. Therefore, the paper should include the optimization code and explanations for concepts.</li>
</ul>
<p>Reference: <a href="https://people.csail.mit.edu/sanchez/papers/2016.tictoc.sigmod.pdf">https://people.csail.mit.edu/sanchez/papers/2016.tictoc.sigmod.pdf</a></p>
]]></content>
      <categories>
        <category>高级数据存储</category>
      </categories>
  </entry>
</search>

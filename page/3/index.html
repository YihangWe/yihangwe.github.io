<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yihangwe.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
<meta property="og:type" content="website">
<meta property="og:title" content="EthanWeee">
<meta property="og:url" content="https://yihangwe.github.io/page/3/index.html">
<meta property="og:site_name" content="EthanWeee">
<meta property="og:description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Yihang Wei">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yihangwe.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>EthanWeee</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EthanWeee</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/08/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/IQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/08/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/IQ/" class="post-title-link" itemprop="url">IQ</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-11-08 00:00:00" itemprop="dateCreated datePublished" datetime="2024-11-08T00:00:00-08:00">2024-11-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-31 21:00:45" itemprop="dateModified" datetime="2025-05-31T21:00:45-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="What-is-the-IQ-Framework"><a href="#What-is-the-IQ-Framework" class="headerlink" title="What is the IQ Framework?"></a>What is the IQ Framework?</h1><p>The IQ framework is a solution designed for Cache-Augmented SQL (CASQL) systems, which combine relational databases (RDBMS) and key-value stores (KVS) to boost performance by caching database query results. However, CASQL systems often face challenges related to stale data and race conditions. The IQ framework ensures strong consistency while maintaining high performance.</p>
<h1 id="Challenges-in-CASQL-Systems"><a href="#Challenges-in-CASQL-Systems" class="headerlink" title="Challenges in CASQL Systems"></a>Challenges in CASQL Systems</h1><ol>
<li><p><strong>Stale Data in Cache</strong>:</p>
<ul>
<li><p>Cached data in the KVS can become outdated if updates to the RDBMS are not properly synchronized.</p>
</li>
<li><p>For example, if a record in the database is modified, but the corresponding cache entry isn’t updated, subsequent reads might return incorrect values.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Issues</strong>:</p>
<ul>
<li><p>Multiple sessions accessing and modifying the same key in KVS concurrently can lead to inconsistent results.</p>
</li>
<li><p>Example:</p>
<ul>
<li>One session updates a value while another session modifies it based on outdated data.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>RDBMS and Cache Coordination</strong>:</p>
<ul>
<li>While RDBMS ensures transactional consistency, KVS often lacks this capability, making it difficult to synchronize their states.</li>
</ul>
</li>
</ol>
<h1 id="Key-Features-of-the-IQ-Framework"><a href="#Key-Features-of-the-IQ-Framework" class="headerlink" title="Key Features of the IQ Framework"></a>Key Features of the IQ Framework</h1><ol>
<li><strong>Lease Mechanism: Inhibit (I) and Quarantine (Q)</strong>:<ol>
<li><strong>I Lease</strong> (for reads):<ol>
<li>Ensures that only one session can query the RDBMS for a cache miss and update the KVS.</li>
<li>Other sessions attempting to read the same key must “back off” and wait.</li>
</ol>
</li>
<li><strong>Q Lease</strong> (for writes):<ol>
<li>Required for modifying, deleting, or incrementally updating keys in the KVS.</li>
<li>If an I lease exists, the Q lease invalidates it to ensure the write operation’s integrity.</li>
<li>The KVS ignores I’s write operation because this I lease is no longer valid.</li>
</ol>
</li>
</ol>
</li>
<li><strong>Lease Expiry</strong>:<ol>
<li>A lease for a key has a fixed life time and is granted to one KVS connection (thread) at a time.</li>
<li>Expired leases are automatically released, ensuring system availability.</li>
<li>The finite life time enables the KVS to release the lease and continue processing operations in the presence of node failures hosting the application.</li>
</ol>
</li>
<li><strong>Session-based Model</strong>:<ol>
<li>The framework operates through sessions, similar to the <strong>two-phase locking protocol</strong>.</li>
<li>Leases can be acquired either before or during an RDBMS transaction, providing flexibility.</li>
</ol>
</li>
</ol>
<h2 id="Implementing-ACID-Properties"><a href="#Implementing-ACID-Properties" class="headerlink" title="Implementing ACID Properties"></a>Implementing ACID Properties</h2><p>原子性 (Atomicity)： IQ 框架确保事务的操作同时在数据库 (RDBMS) 和缓存 (KVS) 中执行。也就是说，操作不会只在数据库中完成而没有更新缓存。这种设计假设 KVS 中的数据是 RDBMS 数据的一部分，因此如果遇到问题，可以直接删除 KVS 中的数据来保持一致。</p>
<p>一致性 (Consistency)： IQ 框架保证事务在数据库和缓存中的数据状态从一个有效状态变为另一个有效状态。如果数据库的事务回滚 (abort)，那么缓存中的操作也不会被应用，确保不会留下无效的缓存数据。</p>
<p>隔离性 (Isolation)： 即使有多个会话 (session) 同时执行，IQ 框架也让每个会话看起来像是独立执行的，避免了并发问题。例如，即使两个用户同时读写相同的数据，他们看到的结果也是正确且一致的。</p>
<p>持久性 (Durability)： 持久性是由数据库 (RDBMS) 提供的，而缓存 (KVS) 则作为数据库的一部分镜像。KVS 存储的数据是在内存中的副本，但一旦数据库中的事务提交，数据就会被持久保存。</p>
<blockquote>
<p>CAS 操作只能保证单一操作的原子性，但无法在多个并发会话中保证强一致性。 由于数据库和缓存系统中的操作顺序可能不一致，会导致数据不同步。</p>
<p>在并发场景下：CAS 无法感知其他会话在其读取后对数据的更改。 多个会话同时执行 CAS 操作时，可能导致更新丢失或顺序混乱，如本例中 S2 的更新被 S1 覆盖。</p>
<p>Q 租约用于写操作，确保某一时刻只有一个会话能够修改目标键值。如果某个键值已有 Q 租约，其他会话（如 S1）会被要求退避（back off）或中止操作。</p>
</blockquote>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/RDBMS_KVS_OPER.png" alt="img"></p>
<h1 id="Invalidate"><a href="#Invalidate" class="headerlink" title="Invalidate"></a>Invalidate</h1><h2 id="What-is-Snapshot-Isolation"><a href="#What-is-Snapshot-Isolation" class="headerlink" title="What is Snapshot Isolation?"></a>What is Snapshot Isolation?</h2><p>Snapshot isolation is a multi-version concurrency control mechanism commonly used in RDBMS to allow concurrent transactions to execute efficiently. It guarantees:</p>
<ol>
<li><strong>Consistent Snapshot</strong>: All reads in a transaction observe the same consistent state of the database, as it existed at the transaction’s start.</li>
<li><strong>Conflict Detection</strong>: A transaction can only commit if its updates do not conflict with updates made by other transactions since its snapshot was taken.</li>
</ol>
<h3 id="The-Problem"><a href="#The-Problem" class="headerlink" title="The Problem"></a>The Problem</h3><p>Snapshot isolation can cause a race condition between a write session (S1) and a read session (S2) when KVS is involved. The issue unfolds as follows:</p>
<ol>
<li><p><strong>Write Session (S1)</strong>:</p>
<ul>
<li><p>S1 modifies the RDBMS and triggers a delete operation in the KVS to invalidate outdated key-value pairs.</p>
</li>
<li><p>S1 commits the transaction after completing its changes in the RDBMS.</p>
</li>
</ul>
</li>
<li><p><strong>Read Session (S2)</strong>:</p>
<ul>
<li><p>S2 starts after S1’s delete operation in the KVS. It observes a <strong>KVS miss</strong> for a key-value pair because S1 has invalidated it.</p>
</li>
<li><p>S2 queries the RDBMS to recompute the key-value pair. However, because snapshot isolation allows S2 to read an <strong>older snapshot of the database</strong>, it retrieves outdated (stale) data.</p>
</li>
<li><p>S2 inserts this <strong>stale data</strong> back into the KVS before S1 commits its changes to the RDBMS.</p>
</li>
</ul>
</li>
<li><p><strong>Inconsistency</strong>:</p>
<ul>
<li>After both sessions complete, the KVS contains a stale key-value pair inconsistent with the RDBMS, leading to incorrect results for future reads.</li>
</ul>
</li>
</ol>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/invalidate_tbl.png" alt="img"></p>
<p>I Lease (Inhibit Lease):</p>
<ul>
<li>Used by <strong>read sessions</strong> (e.g., S2).</li>
<li>When a read session observes a <strong>KVS miss</strong>, it requests an I lease for the key (<code>k_j</code>) from the KVS server.</li>
<li>The I lease allows the read session to query the RDBMS, compute a value, and insert the computed key-value pair into the KVS.</li>
<li>If a Q lease is already in place, the I lease is denied, and the read session is told to <strong>back off</strong> and retry later.</li>
</ul>
<p>Q Lease (Quarantine Lease):</p>
<ul>
<li>Used by <strong>write sessions</strong> (e.g., S1).</li>
<li>When a write session plans to invalidate a key in the KVS, it requests a Q lease for the key (<code>k_j</code>).</li>
<li>The Q lease prevents other sessions (including those holding I leases) from modifying or inserting the key in the KVS.</li>
<li>Multiple Q leases can be granted for the same key since deleting a key is idempotent (doesn’t create conflicts).</li>
</ul>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><h2 id="The-Problem-1"><a href="#The-Problem-1" class="headerlink" title="The Problem"></a>The Problem</h2><ul>
<li>In the original scenario, <strong>write sessions (e.g., S1)</strong> immediately delete key-value pairs in the KVS as soon as they acquire a Q lease (e.g., Step 1.3 in Figure 3).</li>
<li>This can cause <strong>read sessions (e.g., S2)</strong> to encounter KVS misses, triggering redundant operations like querying the RDBMS, recalculating values, and reinserting them into the KVS.</li>
</ul>
<h2 id="The-Proposed-Optimization"><a href="#The-Proposed-Optimization" class="headerlink" title="The Proposed Optimization"></a>The Proposed Optimization</h2><p><strong>Deferring Key Deletion Until Write Commit</strong></p>
<ol>
<li><p><strong>Key Changes</strong>:</p>
<ul>
<li><p>Instead of deleting the key immediately in Step 1.3, the write session (S1) holds the Q lease and <strong>defers the deletion</strong> until the write session commits (Step 1.5).</p>
</li>
<li><p>While S1 is mid-flight, the invalidated key-value pair remains in the KVS for other read sessions (S2) to observe.</p>
</li>
</ul>
</li>
<li><p><strong>Handling KVS Hits</strong>:</p>
<ul>
<li><p>Read sessions like S2 that encounter a <strong>KVS hit</strong> consume the “stale” key-value pair, treating it as valid.</p>
</li>
<li><p>This is acceptable because S2’s actions can be <strong>serialized to occur before</strong> S1, which is still in progress and has not yet committed its RDBMS changes.</p>
</li>
</ul>
</li>
<li><p><strong>Handling Write Aborts</strong>:</p>
<ul>
<li><p>If a write session (S1) encounters an exception and aborts, the Q lease is released without deleting the key.</p>
</li>
<li><p>The current key-value pair in the KVS remains valid and accessible to other sessions.</p>
</li>
</ul>
</li>
</ol>
<h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><ol>
<li><p><strong>Versioning Concept</strong>:</p>
<ul>
<li><p>The optimization can be conceptualized as maintaining a <strong>temporary version</strong> of the key-value pair for use by all sessions except the one currently invalidating it (S1).</p>
</li>
<li><p>Once S1 commits, the temporary version is removed.</p>
</li>
</ul>
</li>
<li><p><strong>Abort Command</strong>:</p>
<ul>
<li><p>If a write session (S1) aborts due to constraints or exceptions, an <strong>abort command</strong> releases all Q leases held by S1 without deleting the key-value pair.</p>
</li>
<li><p>Without this command, Q leases would expire naturally after a timeout, during which no other session could modify or access the key.</p>
</li>
</ul>
</li>
</ol>
<p><strong>Re-Arrangement Window</strong>:</p>
<ul>
<li>With this optimization, S2 and S1 can be <strong>re-arranged</strong> in a serializable schedule where S2 logically occurs before S1.</li>
<li>Without the optimization, the re-arrangement window shrinks to zero because S2 would have already queried the RDBMS for stale data, violating consistency.</li>
</ul>
<h1 id="Refresh-and-Incremental-Update"><a href="#Refresh-and-Incremental-Update" class="headerlink" title="Refresh and Incremental Update"></a>Refresh and Incremental Update</h1><h2 id="Key-Issues-with-Compare-and-Swap-CAS"><a href="#Key-Issues-with-Compare-and-Swap-CAS" class="headerlink" title="Key Issues with Compare-and-Swap (CAS)"></a>Key Issues with Compare-and-Swap (CAS)</h2><ul>
<li><p><strong>CAS Limitation</strong>:</p>
<ul>
<li>CAS alone cannot ensure strong consistency. It provides atomic updates to a single key-value pair but does not coordinate these updates with RDBMS transactions.</li>
</ul>
</li>
<li><p><strong>Example (Figure 2)</strong>:</p>
<ul>
<li><p>KVS writes can occur either:</p>
<ol>
<li><p><strong>Prior to</strong> the RDBMS transaction, or</p>
</li>
<li><p><strong>As part of</strong> the RDBMS transaction.</p>
</li>
</ol>
</li>
<li><p><strong>Problem</strong>: If the RDBMS transaction aborts, the KVS will retain the modified key-value pair, potentially exposing <strong>dirty reads</strong> to other sessions.</p>
</li>
</ul>
</li>
<li><p><strong>Figure 6 (Dirty Read Problem)</strong>:</p>
<ul>
<li>Write session S1 modifies a key-value pair in KVS.</li>
<li>S1’s transaction later aborts, but the intermediate KVS value is consumed by a read session S2 before the rollback, leading to inconsistencies.</li>
</ul>
</li>
<li><p><strong>Developer Responsibility</strong>:</p>
<ul>
<li>Without additional mechanisms, developers must implement complex logic to restore KVS key-value pairs to their original values when RDBMS transactions abort.</li>
</ul>
</li>
</ul>
<h2 id="Race-Conditions-with-Incremental-Updates-δ-Operations"><a href="#Race-Conditions-with-Incremental-Updates-δ-Operations" class="headerlink" title="Race Conditions with Incremental Updates (δ Operations)"></a>Race Conditions with Incremental Updates (δ Operations)</h2><ul>
<li><p><strong>Figure 7 (Snapshot Isolation with δ Operations)</strong>:</p>
</li>
<li><ul>
<li>Write session S1 updates the RDBMS and KVS using an incremental update (e.g., appending to a value).</li>
<li>Concurrently, read session S2 queries the RDBMS and overwrites the key-value pair in the KVS.</li>
<li><strong>Result</strong>: The KVS reflects inconsistent state, as S2’s overwrite may invalidate S1’s incremental change.</li>
</ul>
</li>
<li><p><strong>Figure 8 (Reordering KVS Operations)</strong>:</p>
</li>
<li><ul>
<li>Delaying KVS updates until after the RDBMS transaction doesn’t solve the problem.</li>
</ul>
</li>
<li><p><strong>Example in Figure 8</strong>:</p>
</li>
<li><ul>
<li>S1 appends a change to a value based on its RDBMS view.</li>
<li>S2 modifies the RDBMS during S1’s execution, which S1 unknowingly incorporates into its KVS update.</li>
<li><strong>Problem</strong>: S2’s modifications are reflected twice in the KVS, introducing inconsistencies.</li>
</ul>
</li>
</ul>
<h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h2><p><strong>Key Concepts in the Solution</strong></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/refresh_tbl.png" alt="img"></p>
<ol>
<li><p><strong>Q Leases for Write Sessions</strong>:</p>
<ul>
<li><p>A <strong>Q lease</strong> must be obtained for each key-value pair that a session intends to update.</p>
</li>
<li><p>This prevents race conditions by locking the key-value pair until the session completes its operations.</p>
</li>
</ul>
</li>
<li><p><strong>Steps for Write Sessions</strong>:</p>
<ul>
<li><p><strong>Step 1</strong>: Obtain Q leases for the keys to be updated before committing the RDBMS transaction. This can happen:</p>
<ul>
<li><p>Before starting the RDBMS transaction.</p>
</li>
<li><p>As part of the RDBMS transaction.</p>
</li>
</ul>
</li>
<li><p><strong>Step 2</strong>: Write the updated key-value pairs to the KVS after committing the RDBMS transaction.</p>
</li>
<li><p><strong>Step 3</strong>: Release the Q leases once the KVS is updated.</p>
</li>
<li><p><strong>Automatic Cleanup</strong>: If a Q lease expires, the KVS deletes the associated key-value pair to avoid stale data.</p>
</li>
</ul>
</li>
<li><p><strong>Command Design for Write Operations</strong>:</p>
<ul>
<li><p><strong>QaRead (Quarantine-and-Read)</strong>:</p>
<ul>
<li><p>Acquires a Q lease on the referenced key and reads its value from the KVS.</p>
</li>
<li><p>If a Q lease for the same key is already held by another session, the requesting session receives an <strong>abort message</strong>, must roll back its RDBMS transaction, release all leases, back off, and retry later.</p>
</li>
<li><p>If no value exists in the KVS (a <strong>KVS miss</strong>), the application can:</p>
<ul>
<li>Skip updating the key, or</li>
<li>Query the RDBMS, compute a new value, and insert it using <strong>SaR</strong> (below).</li>
</ul>
</li>
<li><p>If a <strong>QaRead lease</strong> encounters an <strong>I lease</strong> held by a read session, it invalidates the I lease to prevent race conditions.</p>
</li>
</ul>
</li>
<li><p><strong>SaR (Swap-and-Release)</strong>:</p>
<ul>
<li>Updates the value of a key in the KVS with the new value and releases the Q lease.</li>
<li>If the new value is <code>null</code>, the Q lease is simply released without updating the KVS.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Handling-Race-Conditions"><a href="#Handling-Race-Conditions" class="headerlink" title="Handling Race Conditions"></a>Handling Race Conditions</h2><ol>
<li><p><strong>Q Leases for Concurrent Write Sessions</strong>:</p>
<ul>
<li><p>If two write sessions request Q leases for the same key, the KVS resolves the conflict by:</p>
<ul>
<li>Aborting one session.</li>
<li>Ensuring the aborted session retries later, serializing its updates after the session holding the Q lease.</li>
</ul>
</li>
<li><p>This guarantees a valid serial schedule in the RDBMS and KVS.</p>
</li>
</ul>
</li>
<li><p><strong>Read Sessions and I Leases</strong>:</p>
<ul>
<li><p>Read sessions use <strong>I leases</strong> to avoid race conditions when querying the KVS.</p>
</li>
<li><p>If a write session issues a <strong>QaRead</strong> that encounters an existing <strong>I lease</strong>, the <strong>I lease</strong> is invalidated to ensure the KVS reflects the latest updates from the RDBMS.</p>
</li>
</ul>
</li>
</ol>
<h2 id="Integration-with-Two-Phase-Locking"><a href="#Integration-with-Two-Phase-Locking" class="headerlink" title="Integration with Two-Phase Locking"></a>Integration with Two-Phase Locking</h2><ul>
<li><p>The Q lease mechanism resembles <strong>two-phase locking</strong>:</p>
<ol>
<li><p><strong>Growing Phase</strong>: The session acquires all necessary Q leases using <strong>QaRead</strong> before committing its RDBMS transaction.</p>
</li>
<li><p><strong>Shrinking Phase</strong>: The session releases all Q leases using <strong>SaR</strong> after committing its RDBMS transaction.</p>
</li>
</ol>
</li>
<li><p>Flexibility:</p>
<ul>
<li>A session can issue <strong>QaRead</strong> commands either before starting the RDBMS transaction or as part of the transaction.</li>
</ul>
</li>
</ul>
<h2 id="Key-Concepts-of-Incremental-Updates"><a href="#Key-Concepts-of-Incremental-Updates" class="headerlink" title="Key Concepts of Incremental Updates"></a>Key Concepts of Incremental Updates</h2><ol>
<li><p><strong>Incremental Update Command: IQ-δ</strong>:</p>
<ul>
<li><p><strong>Purpose</strong>: Allows a write session to perform an incremental update, such as appending data to an existing key-value pair.</p>
</li>
<li><p><strong>Syntax</strong>: <code>IQ-δ(ki, δi)</code></p>
<ul>
<li><code>ki</code>: The key to be updated.</li>
<li><code>δi</code>: The incremental change to apply (e.g., the value to append).</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Similarities to QaRead</strong>:</p>
<ul>
<li><p><strong>Q Lease Requirement</strong>: Before issuing the <code>IQ-δ</code> command, the session must obtain a <strong>Q lease</strong> for the key <code>ki</code> to ensure exclusive access.</p>
</li>
<li><p><strong>Abort on Conflict</strong>:</p>
<ul>
<li><p>If another session already holds a Q lease on the same key (<code>ki</code>), the <strong>KVS returns an abort message</strong>.</p>
</li>
<li><p>The write session must:</p>
<ol>
<li><p>Release all its leases.</p>
</li>
<li><p>Abort its ongoing RDBMS transaction (if any).</p>
</li>
<li><p>Retry the operation later.</p>
</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="优化关键点总结"><a href="#优化关键点总结" class="headerlink" title="优化关键点总结"></a>优化关键点总结</h1><ol>
<li><p><strong>保留旧版本（Older Version）</strong>：</p>
<ul>
<li><p>当写会话（S1）更新某键值对 (<code>ki-vi</code>) 时，KVS 暂时保留该键值对的旧版本 (<code>ki-vi_old</code>)，直到 S1 提交。</p>
</li>
<li><p>这避免了读会话在写会话更新期间遇到 <strong>KVS miss</strong>。</p>
</li>
</ul>
</li>
<li><p>写会话的更新视图：</p>
<ul>
<li><p>写会话（S1）在更新期间必须能够看到自己的修改结果（<code>ki-vi_new</code>）。</p>
</li>
<li><p>KVS 确保为 S1 提供其最新的更新视图。</p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Why is it acceptable for invalidate to delete cache entries?</strong></p>
<p>Consistency Assurance: The cache entry being invalidated represents stale data that is no longer consistent with the current state of the RDBMS. Deleting it prevents read sessions from accessing outdated information.</p>
<p><strong>How is a lease different than a lock?</strong> </p>
<ul>
<li><strong>Lease</strong>: Has a fixed lifetime and expires automatically after a certain duration. This makes leases useful in distributed systems where failures or delays could otherwise cause indefinite blocking.</li>
<li><strong>Lock</strong>: Typically remains active until explicitly released, which can lead to deadlocks or indefinite resource contention if not managed properly.</li>
</ul>
<p><strong>True or False: IQ leases require changes to the RDBMS software.</strong></p>
<p>False:</p>
<p>IQ leases do not require changes to the RDBMS software.</p>
<p>Instead, they extend the functionality of the Key-Value Store (KVS) by introducing new lease-based commands (e.g., <code>QaRead</code> and <code>SaR</code>) to coordinate operations between the KVS and the RDBMS. This design leverages existing RDBMS features without altering its underlying implementation.</p>
<p><strong>What factors does CAMP consider when selecting a victim?</strong></p>
<p>H(p) &#x3D; L + size(p) &#x2F; cost(p)</p>
<p><strong>What is the definition of cost? Provide an example.</strong></p>
<ul>
<li><strong>Computation Time</strong>: The time required to regenerate or recompute the data if it is evicted from memory.</li>
<li><strong>Access Latency</strong>: The time it would take to fetch the data from disk or another slower storage tier.</li>
<li><strong>Importance</strong>: The priority or weight assigned to the data based on how frequently or critically it is used.</li>
</ul>
<p><strong>How does CAMP insert a key-value pair in memory?</strong></p>
<p>When a new key-value pair p needs to be inserted into memory, CAMP performs the following steps:</p>
<p><strong>1. Check Cache Capacity</strong></p>
<ul>
<li>If there is <strong>enough memory</strong> to store the new key-value pair:</li>
<li><ul>
<li>The pair is inserted directly into the appropriate <strong>priority group</strong> based on its cost-to-size ratio.</li>
<li>L is not updated.</li>
</ul>
</li>
<li>If the cache is <strong>full</strong>:</li>
<li><ul>
<li>CAMP selects one or more key-value pairs to <strong>evict</strong> based on their H(p) values.</li>
<li>It removes the pair(s) with the <strong>lowest H(p)</strong> values until there is sufficient space for the new pair.</li>
</ul>
</li>
</ul>
<p><strong>2. Insert the New Pair</strong></p>
<ul>
<li>The new key-value pair p is added to the cache, and its H(p) value is computed and recorded.</li>
<li>The pair is placed in the appropriate priority queue based on its cost-to-size ratio.</li>
</ul>
<p><strong>With BG, what is the definition of Service Level Agreement, SLA?</strong></p>
<p>SLA, e.g., 95% of requests to observe a response time equal to or faster than 100 msec with at most 0.1% of requests observing unpredictable data for 10 minutes.</p>
<p><strong>Name one reason why a system may produce unpredictable data?</strong></p>
<p>Eventual consistency. Or multiple threads are updating the same data item.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/2663165.2663318">https://dl.acm.org/doi/abs/10.1145/2663165.2663318</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/CAMP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/11/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/CAMP/" class="post-title-link" itemprop="url">CAMP</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-11-01 00:00:00" itemprop="dateCreated datePublished" datetime="2024-11-01T00:00:00-07:00">2024-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-31 20:59:33" itemprop="dateModified" datetime="2025-05-31T20:59:33-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Greedy-Dual-Size-GDS-algorithm"><a href="#Greedy-Dual-Size-GDS-algorithm" class="headerlink" title="Greedy Dual Size (GDS) algorithm"></a>Greedy Dual Size (GDS) algorithm</h1><p>Key Concepts:</p>
<ol>
<li><strong>Variable Size and Cost</strong>:<ul>
<li>Unlike simple algorithms that treat all objects equally, GDS takes into account:<ul>
<li><strong>Size of the object</strong> (<code>size(p)</code>): Larger objects take up more space in memory.</li>
<li><strong>Cost of the object</strong> (<code>cost(p)</code>): This can represent factors like time to retrieve the object, computational effort, or other resource usage.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Score H(p)</strong>:<ul>
<li>Each key-value pair ppp in the cache is assigned a score H(p). This score reflects the <strong>benefit of keeping the object</strong> in memory and is calculated using:<ul>
<li>A <strong>global parameter L</strong>, which adjusts dynamically based on cache state.</li>
<li>The <strong>size(p)</strong> of the object.</li>
<li>The <strong>cost(p)</strong> associated with the object.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Eviction Strategy</strong>:<ul>
<li>When the cache is full, and a new object needs to be added, GDS removes the object with the <strong>lowest score H(p)</strong>. This process continues until there is enough space for the new object.</li>
</ul>
</li>
</ol>
<h2 id="Proposition-1"><a href="#Proposition-1" class="headerlink" title="Proposition 1"></a>Proposition 1</h2><p><strong>L is non-decreasing over time.</strong></p>
<ul>
<li>The global parameter L, which reflects the minimum priority H(p) among all key-value pairs in the KVS, will either stay the same or increase with each operation. This ensures stability and helps prioritize eviction decisions consistently.</li>
</ul>
<p>For any key-value pair ppp in the KVS, the relationship holds:</p>
<p><strong>L ≤ H(p) ≤ L + cost(p) &#x2F; size(p)</strong></p>
<ul>
<li>H(p), the priority of p, always lies between the global minimum L and L + cost(p) &#x2F; size(p), ensuring H(p) reflects both its retrieval cost and size relative to other elements.</li>
</ul>
<p><strong>Intuition Behind Proposition 1:</strong></p>
<ul>
<li>As L increases over time (reflecting the minimum H(p)), less recently used or less “valuable” pairs become increasingly likely to be evicted. This ensures that newer and higher-priority pairs stay in the KVS longer.</li>
</ul>
<p><strong>Key Insights from Proposition 1:</strong></p>
<ol>
<li><strong>Delayed Eviction:</strong><ul>
<li>When p is requested again while in memory, its H(p) increases to L + cost(p) &#x2F; size(p), delaying its eviction.</li>
</ul>
</li>
<li><strong>Impact of Cost-to-Size Ratio:</strong><ul>
<li>Pairs with higher cost(p) &#x2F; size(p) stay longer in the KVS. For example, if one pair’s ratio is c times another’s, it will stay approximately c times longer.</li>
</ul>
</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart3.png" alt="img"></p>
<h2 id="Key-Points-in-the-Diagram"><a href="#Key-Points-in-the-Diagram" class="headerlink" title="Key Points in the Diagram"></a>Key Points in the Diagram</h2><ol>
<li><strong>Cost-to-Size Ratios</strong>:<ol>
<li>Key-value pairs are grouped into <strong>queues</strong> according to their cost-to-size ratio.</li>
<li>Each queue corresponds to a specific cost-to-size ratio.</li>
</ol>
</li>
<li><strong>Grouping by Ratio</strong>:<ol>
<li>Within each queue, key-value pairs are managed using the <strong>Least Recently Used (LRU)</strong> strategy.</li>
</ol>
</li>
<li><strong>Priority Management</strong>:<ol>
<li>The <strong>priority (H-value)</strong> of a key-value pair is based on: <strong>H(p) &#x3D; L + cost(p) &#x2F; size(p)</strong><ol>
<li>L: The global non-decreasing variable.</li>
<li>cost(p) &#x2F; size(p): The cost-to-size ratio of the key-value pair.</li>
</ol>
</li>
</ol>
</li>
<li><strong>Efficient Eviction</strong>:<ol>
<li>CAMP maintains a <strong>heap</strong> that points to the <strong>head of each queue</strong>, storing the minimum H(p) from every queue.</li>
<li>To identify the next key-value pair for eviction:<ol>
<li><strong>The algorithm checks the heap to find the queue with the smallest H(p).</strong></li>
<li><strong>It then evicts the key-value pair at the front of that queue (i.e., the least recently used pair in that cost-to-size group).</strong></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="Rounding-in-CAMP"><a href="#Rounding-in-CAMP" class="headerlink" title="Rounding in CAMP"></a>Rounding in CAMP</h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/bg_bm_rounding.png" alt="img"></p>
<ol>
<li><strong>Purpose</strong>: To improve performance, CAMP <strong>reduces the number of LRU queues</strong> by grouping key-value pairs with <strong>similar cost-to-size ratios</strong> into the same queue.</li>
<li><strong>Key Idea</strong>: Preserve the most significant bits proportional to the value’s size.</li>
</ol>
<h2 id="Proposition-2-Explanation-of-Rounding-and-Distinct-Values"><a href="#Proposition-2-Explanation-of-Rounding-and-Distinct-Values" class="headerlink" title="Proposition 2: Explanation of Rounding and Distinct Values"></a>Proposition 2: Explanation of Rounding and Distinct Values</h2><h3 id="Implications"><a href="#Implications" class="headerlink" title="Implications"></a>Implications</h3><ol>
<li><p><strong>Trade-Off Between Precision and Efficiency</strong>:</p>
<ul>
<li><p>A higher p preserves more precision but increases the number of distinct values (and thus computational complexity).</p>
</li>
<li><p>Lower p reduces the number of distinct values, making CAMP more efficient but less precise.</p>
</li>
</ul>
</li>
<li><p><strong>Rounding Efficiency</strong>:</p>
<ul>
<li>By limiting the number of distinct values, CAMP minimizes the number of LRU queues, reducing overhead while still approximating GDS closely.</li>
</ul>
</li>
</ol>
<h2 id="Proposition-3-Competitive-Ratio-of-CAMP"><a href="#Proposition-3-Competitive-Ratio-of-CAMP" class="headerlink" title="Proposition 3: Competitive Ratio of CAMP"></a>Proposition 3: Competitive Ratio of CAMP</h2><h3 id="Practical-Implications"><a href="#Practical-Implications" class="headerlink" title="Practical Implications"></a>Practical Implications</h3><ol>
<li><p><strong>Precision ppp</strong>:</p>
<ul>
<li><p>The smaller the ϵ (higher ppp), the closer CAMP approximates GDS.</p>
</li>
<li><p>For sufficiently large p, CAMP performs nearly as well as GDS.</p>
</li>
</ul>
</li>
<li><p><strong>Trade-off</strong>:</p>
<ul>
<li>Higher p increases precision but also increases the number of distinct cost-to-size ratios and computational overhead.</li>
</ul>
</li>
</ol>
<h3 id="CAMP’s-Improvement-Over-GDS"><a href="#CAMP’s-Improvement-Over-GDS" class="headerlink" title="CAMP’s Improvement Over GDS:"></a>CAMP’s Improvement Over GDS:</h3><ol>
<li><strong>Approximation:</strong> CAMP simplifies H(p) by <strong>rounding</strong> the cost-to-size ratio, reducing the precision but making the algorithm more efficient.</li>
<li><strong>Grouping:</strong> Key-value pairs are <strong>grouped</strong> by similar cost-to-size ratios, reducing the number of queues and simplifying priority management.</li>
<li><strong>Tie-Breaking:</strong> CAMP uses <strong>LRU within each group</strong> to determine the eviction order, making it computationally cheaper.</li>
</ol>
<h3 id="Figure-4-Heap-Node-Visits"><a href="#Figure-4-Heap-Node-Visits" class="headerlink" title="Figure 4: Heap Node Visits"></a>Figure 4: Heap Node Visits</h3><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart4.png" alt="img"></p>
<p>This figure compares the number of heap node visits for GDS and CAMP as a function of cache size:</p>
<ol>
<li><p><strong>GDS</strong>:</p>
<ul>
<li><p><strong>Heap size equals the total number of key-value pairs in the cache.</strong></p>
</li>
<li><p>Every heap update (insertion, deletion, or priority change) requires visiting O(log⁡n) nodes, where n is the number of cache entries.</p>
</li>
<li><p>As cache size increases, GDS’s overhead grows significantly.</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>:</p>
<ul>
<li><p><strong>Heap size equals the number of non-empty LRU queues, which is much smaller than the total number of cache entries.</strong></p>
</li>
<li><p>Heap updates occur only when:</p>
<ul>
<li><p>The priority of the head of an LRU queue changes.</p>
</li>
<li><p>A new LRU queue is created.</p>
</li>
</ul>
</li>
<li><p>As cache size increases, the number of non-empty LRU queues remains relatively constant, resulting in fewer heap updates.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart5.png" alt="img"></p>
<p><strong>(a) Cost-Miss Ratio vs Precision</strong></p>
<ul>
<li><strong>横轴</strong>：精度（Precision），从低到高。</li>
<li><strong>纵轴</strong>：成本未命中比（Cost-Miss Ratio）。</li>
<li><strong>结果</strong>：<ul>
<li>不同的缓存大小比（0.01、0.1 和 0.3）在较低精度下表现一致。</li>
<li>提高精度后，成本未命中比没有显著变化。</li>
<li>说明即使使用较低精度，CAMP 的成本未命中比也能接近 GDS（标准实现）。</li>
</ul>
</li>
</ul>
<p><strong>(b) LRU Queues vs Precision</strong></p>
<ul>
<li><strong>横轴</strong>：精度（Precision）。</li>
<li><strong>纵轴</strong>：CAMP 维护的非空 LRU 队列数量。</li>
<li><strong>结果</strong>：<ul>
<li><strong>低精度</strong>（1-5）：CAMP 维持稳定的少量 LRU 队列（约 5 个）。</li>
<li><strong>高精度</strong>（&gt;10）：队列数增加，尤其是在较大的缓存大小比（如 1.0）下。</li>
<li><strong>结论</strong>：<ul>
<li>在较低精度下，CAMP 能保持较低的计算开销，同时维持高效的队列管理。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>(c) Cost-Miss Ratio vs Cache Size Ratio</strong></p>
<ul>
<li><strong>横轴</strong>：缓存大小比（Cache Size Ratio），即缓存大小与 trace 文件中唯一键值对总大小的比值。</li>
<li><strong>纵轴</strong>：成本未命中比（Cost-Miss Ratio）。</li>
<li><strong>结果</strong>：<ul>
<li><strong>CAMP</strong>：<ul>
<li>在所有缓存大小下，成本未命中比最低。</li>
<li>说明 CAMP 在高成本键值对管理上更具效率。</li>
</ul>
</li>
<li><strong>Pooled LRU</strong>：<ul>
<li>在较小缓存下表现稍差，但随着缓存增加，接近 CAMP。</li>
</ul>
</li>
<li><strong>LRU</strong>：<ul>
<li>成本未命中比始终最高。</li>
</ul>
</li>
<li><strong>结论</strong>：<ul>
<li>CAMP 优于 LRU 和 Pooled LRU，尤其是在小缓存下。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>(d) Miss Rate vs Cache Size Ratio</strong></p>
<ul>
<li><strong>横轴</strong>：缓存大小比（Cache Size Ratio）。</li>
<li><strong>纵轴</strong>：未命中率（Miss Rate）。</li>
<li><strong>结果</strong>：<ul>
<li><strong>CAMP</strong>：<ul>
<li>未命中率显著低于 LRU 和 Pooled LRU，尤其在小缓存下表现最优。</li>
</ul>
</li>
<li><strong>Pooled LRU</strong>：<ul>
<li>未命中率随着缓存增大而下降，但始终高于 CAMP。</li>
<li>最低成本池（cheapest pool）未命中率接近 100%，次低成本池未命中率达到 65%。</li>
</ul>
</li>
<li><strong>LRU</strong>：<ul>
<li>始终高于 CAMP 和 Pooled LRU。</li>
</ul>
</li>
<li><strong>结论</strong>：<ul>
<li>CAMP 在多种缓存大小下都保持较低的未命中率，且比 Pooled LRU 更均衡。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="CAMP-的适应能力：访问模式变化的分析"><a href="#CAMP-的适应能力：访问模式变化的分析" class="headerlink" title="CAMP 的适应能力：访问模式变化的分析"></a>CAMP 的适应能力：访问模式变化的分析</h1><p>实验设置：</p>
<ul>
<li>使用 10 个不同的 trace 文件，每个文件包含 400 万个键值对引用。</li>
<li>每个 trace 文件（如 TF1、TF2 等）中的请求在其结束后不会再被引用，模拟访问模式的突然变化。</li>
<li>访问模式具有倾斜分布（如 Zipf 分布），每个 trace 文件中的高成本对象可能在下一次访问中完全无效。</li>
</ul>
<p>目标：</p>
<ul>
<li>比较 <strong>CAMP</strong>、<strong>Pooled LRU</strong> 和 <strong>LRU</strong> 在不同缓存大小下对访问模式突变的适应能力。</li>
<li>评估三种算法在突然变化后清除旧的高成本键值对的效率，以及对总体性能（如成本未命中比和未命中率）的影响。</li>
</ul>
<p>不同算法的行为分析</p>
<ol>
<li><p><strong>LRU</strong>：</p>
<ul>
<li><p>按最近使用排序，当新请求的总大小超过缓存大小时清除旧数据。</p>
</li>
<li><p>当缓存大小比为 1 时，清除 TF1 数据的时间点对应于 TF3 开始请求的第一个键值对。</p>
</li>
</ul>
</li>
<li><p><strong>Pooled LRU</strong>：</p>
<ul>
<li><p>将键值对按成本分组，每组分配固定比例的缓存空间。</p>
</li>
<li><p>高成本池占据 99% 的缓存空间，因此在每个新 trace 开始时会突然清除一批旧数据。</p>
</li>
<li><p>对于缓存大小比 2&#x2F;3 或更高的情况，直到 TF4（约 800 万请求后）才会清除所有 TF1 数据。</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>：</p>
<ul>
<li><p>对每个成本-大小比维护 LRU 队列，这些队列的大小可以动态调整。</p>
</li>
<li><p><strong>优先淘汰较低优先级的数据，但高成本数据即使来自旧 trace，也具有一定保留优先级。</strong></p>
</li>
<li><p><strong>当新数据的总大小超过缓存时，旧 trace 的高成本数据才会被逐步清除。</strong></p>
</li>
</ul>
</li>
</ol>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart6.png" alt="img"></p>
<p><strong>图 6c：缓存比 0.25（小缓存）</strong></p>
<ol>
<li><p><strong>LRU</strong>：</p>
<ul>
<li><p>清除最快，仅需 <strong>2.1 万次请求</strong> 即完全清除 Trace 1 的所有键值对。</p>
</li>
<li><p>由于 LRU 优先淘汰最久未使用的数据，小缓存下表现最佳。</p>
</li>
</ul>
</li>
<li><p><strong>Pooled LRU</strong>：</p>
<ul>
<li><p>清除速度较慢，需要 <strong>13.1 万次请求</strong>。</p>
</li>
<li><p>原因：Pooled LRU 按成本对键值对分组，高成本池占用较多缓存空间，导致清除滞后。</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>：</p>
<ul>
<li><p>初期清除速度比 Pooled LRU 更快，但最后完全清除所有键值对需到 <strong>TF3 结束（770 万次请求）</strong>。</p>
</li>
<li><p>然而，这些未被清除的 Trace 1 数据仅占缓存的 <strong>2%</strong>，说明 CAMP 优先保留了高成本键值对。</p>
</li>
</ul>
</li>
</ol>
<p><strong>图 6d：缓存比 0.75（大缓存）</strong></p>
<ol>
<li><p><strong>LRU</strong>：</p>
<ul>
<li><p>同样清除最快，几乎在 Trace 2 开始时就清除掉大部分 Trace 1 的数据。</p>
</li>
<li><p>说明即使缓存较大，LRU 仍然倾向淘汰旧数据。</p>
</li>
</ul>
</li>
<li><p><strong>Pooled LRU</strong>：</p>
<ul>
<li><p>清除延迟显著，需要 <strong>730 万次请求</strong>，接近 TF3 结束。</p>
</li>
<li><p>原因：高成本池占用过多缓存空间，延迟清除低成本和无用数据。</p>
</li>
</ul>
</li>
<li><p><strong>CAMP</strong>：</p>
<ul>
<li><p>大部分 Trace 1 数据在较早阶段被淘汰，仅保留少量最昂贵的键值对（占缓存比小于 <strong>0.6%</strong>）。</p>
</li>
<li><p>即使在 <strong>4000 万次请求</strong>后，这些高成本键值对仍在缓存中，但对整体缓存利用影响极小。</p>
</li>
</ul>
</li>
</ol>
<p>针对不同大小但成本相同的键值对，CAMP 优先保留较小的键值对，从而降低未命中率和成本未命中比。</p>
<p>针对相同大小但成本不同的键值对，CAMP 优先保留高成本键值对，在成本未命中比上显著优于其他算法。</p>
<p>与其他算法的对比：</p>
<ul>
<li><p>LRU：适用于简单场景，但无法处理成本差异。</p>
</li>
<li><p>Pooled LRU：小缓存情况下表现不错，但静态分区策略限制了其大缓存场景的效率。</p>
</li>
</ul>
<p>CAMP 的适应性：在处理多样化的成本分布时，通过动态调整和四舍五入策略，CAMP 在复杂负载下表现出更高的灵活性和效率。</p>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>What is the time complexity of LRU to select a victim?</strong></p>
<p><strong>O(1)</strong> because the least recently used item is always at the tail of the list.</p>
<p><strong>What is the time complexity of CAMP to select a victim?</strong></p>
<p><strong>O(logk)</strong> CAMP identifies the key-value pair with the smallest priority from the heap, deletes it and then <strong>heapifies</strong>.</p>
<p><strong>Why does CAMP do rounding using the high order bits?</strong></p>
<ul>
<li>CAMP rounds cost-to-size ratios to <strong>reduce the number of distinct ratios (or LRU queues)</strong>.</li>
<li>High-order bits are retained because they represent the <strong>most significant portion of the value</strong>, ensuring that <strong>approximate prioritization is maintained</strong>.</li>
</ul>
<p><strong>How does BG generate social networking actions that are always valid?</strong></p>
<p><strong>Pre-Validation of Actions:</strong></p>
<ul>
<li>Before generating an action, BG <strong>checks</strong> the current state of the database to ensure the action is valid. For instance:<ul>
<li>A friend request is only generated if the two users are not already friends or in a “pending” relationship.</li>
<li>A comment can only be posted on a resource if the resource exists.</li>
</ul>
</li>
</ul>
<p><strong>Avoiding Concurrent Modifications:</strong></p>
<ul>
<li>BG <strong>prevents multiple threads from concurrently modifying the same user’s state</strong>.</li>
</ul>
<p><strong>How does BG scale to a large number of nodes?</strong></p>
<p>BG employs <strong>a shared-nothing architecture</strong> with the following mechanisms to scale effectively:</p>
<ol>
<li><p><strong>Partitioning Members and Resources:</strong></p>
<ul>
<li><p>BGCoord <strong>partitions</strong> the database into <strong>logical fragments</strong>, each containing <strong>a unique subset</strong> of members, their resources, and relationships.</p>
</li>
<li><p>These fragments are assigned to individual BGClients.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple BGClients:</strong></p>
<ul>
<li><p>Each BGClient operates <strong>independently</strong>, generating workloads for its assigned logical fragment.</p>
</li>
<li><p>By running <strong>multiple</strong> BGClients <strong>in parallel</strong> across different nodes, BG can scale horizontally to handle millions of requests.</p>
</li>
</ul>
</li>
<li><p><strong>D-Zipfian Distribution:</strong></p>
<ul>
<li><p>To ensure realistic and scalable workloads, BG uses a decentralized Zipfian distribution (D-Zipfian) that <strong>dynamically assigns</strong> requests to BGClients based on node performance.</p>
</li>
<li><p>Faster nodes receive a larger share of the logical fragments, ensuring even workload distribution.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Control:</strong></p>
<ul>
<li>BG <strong>prevents simultaneous threads from issuing actions for the same user</strong>, maintaining the integrity of modeled user interactions and avoiding resource contention.</li>
</ul>
</li>
</ol>
<p><strong>True or False: BG quantifies the amount of unpredictable data produced by a data store?</strong></p>
<p>True.</p>
<p>This is achieved through:</p>
<ul>
<li><strong>Validation Phase:</strong><ul>
<li>BG uses <strong>read and write log records</strong> to detect instances where a read operation observes a value <strong>outside the acceptable range</strong>, classifying it as “unpredictable data.”</li>
</ul>
</li>
<li><strong>Metrics Collection:</strong><ul>
<li>The percentage of requests that observe unpredictable data (τ) is a key metric used to evaluate the data store’s consistency.</li>
</ul>
</li>
</ul>
<p><strong>How is BG’s SoAR different than its Socialites rating?</strong></p>
<p>SoAR (Social Action Rating): Represents the <strong>maximum throughput</strong> (actions per second) a data store can achieve while meeting a given SLA.</p>
<p>Socialites Rating: Represents the <strong>maximum number of concurrent threads</strong> <strong>(users)</strong> a data store can support while meeting the SLA.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=d6f9678772a09ca29101f5efce583960ecf53745">https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=d6f9678772a09ca29101f5efce583960ecf53745</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/25/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/BG%20Benchmark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/25/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/BG%20Benchmark/" class="post-title-link" itemprop="url">BG Benchmark</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-25 00:00:00" itemprop="dateCreated datePublished" datetime="2024-10-25T00:00:00-07:00">2024-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-31 20:59:14" itemprop="dateModified" datetime="2025-05-31T20:59:14-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Data-Model-and-Performance-Metrics"><a href="#Data-Model-and-Performance-Metrics" class="headerlink" title="Data Model and Performance Metrics"></a>Data Model and Performance Metrics</h1><ol>
<li>ER Diagram and Database Design</li>
</ol>
<ul>
<li>ER Diagram (Figure 1.a):Represents entities and relationships in the BG system.<ul>
<li>Member Entity:<ul>
<li>Represents users with a registered profile, including a unique ID and a set of adjustable-length string attributes to create records of varying sizes.</li>
<li>Each user can have up to two images:<ul>
<li>Thumbnail Image: Small (in KBs), used for displaying in friend lists.</li>
<li>High-Resolution Image: Larger (hundreds of KBs or MBs), displayed when visiting a user profile.</li>
<li>Using thumbnails significantly reduces system load compared to larger images.</li>
</ul>
</li>
</ul>
</li>
<li>Friend Relationship:<ul>
<li>Captures relationships or friend requests between users. An attribute differentiates between invitations and confirmed friendships.</li>
</ul>
</li>
<li>Resource Entity:<ul>
<li>Represents user-owned items like images, questions, or documents. Resources must belong to a user and can be posted on their profile or another user’s profile.</li>
</ul>
</li>
<li>Manipulation Relationship:<ul>
<li>Manages comments and restrictions (e.g., only friends can comment on a resource).</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li>BG Workload and SLA (Service-Level Agreement)</li>
</ol>
<ul>
<li><p>Workload: BG supports defining workloads at the granularity of:</p>
<ul>
<li>Actions: Single operations like “view profile” or “list friends.”</li>
<li>Sessions: A sequence of related actions (e.g., browsing a profile, sending a friend request).</li>
<li>Mixed Workloads: A combination of actions and sessions.</li>
</ul>
</li>
<li><p>Service-Level Agreement (SLA):</p>
<ul>
<li>Goal: Ensures the system provides reliable performance under specified conditions.</li>
<li>Example SLA Requirements: SLA, e.g., 95% of requests to observe a response time equal to or faster than 100 msec with at most 0.1% of requests observing unpredictable data for 10 minutes.</li>
</ul>
</li>
<li><p>Metrics:</p>
<ul>
<li><strong>SoAR (Social Action Rating): Measures the highest number of actions per second that meet the SLA.</strong></li>
<li><strong>Socialites: Measures the maximum number of concurrent threads that meet the SLA, reflecting the system’s multithreading capabilities.</strong></li>
</ul>
</li>
</ul>
<ol start="3">
<li>Performance Evaluation Example</li>
</ol>
<ul>
<li>SQL-X System Performance:SQL-X is a relational database with strict ACID compliance.<ul>
<li>Initially, throughput increases with more threads.</li>
<li>Beyond a certain threshold (e.g., 4 threads), request queuing causes response times to increase, reducing SLA compliance.</li>
<li>With 32 threads, 99.94% of requests exceed the 100-millisecond SLA limit, indicating significant performance degradation.</li>
</ul>
</li>
</ul>
<ol start="4">
<li>Concurrency and Optimization in BG</li>
</ol>
<ul>
<li><p><strong>Concurrency Management:</strong></p>
<ul>
<li><strong>BG prevents two threads from emulating the same user simultaneously to realistically simulate user behavior.</strong></li>
</ul>
</li>
<li><p><strong>Unpredictable Data Handling:</strong></p>
<ul>
<li><strong>Definition: Data that is stale, inconsistent, or invalid due to system limitations or race conditions.</strong></li>
<li><strong>Validation:</strong><ul>
<li><strong>BG uses offline validation to analyze read and write logs.</strong></li>
<li><strong>It determines acceptable value ranges for data and flags any reads that fall outside these ranges as unpredictable.</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>If SoAR is zero, the data store fails to meet SLA requirements, even with a single-threaded BGClient issuing requests.</p>
</blockquote>
<h1 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h1><h2 id="Performance-Analysis-of-View-Profile"><a href="#Performance-Analysis-of-View-Profile" class="headerlink" title="Performance Analysis of View Profile"></a>Performance Analysis of View Profile</h2><p>Performance of VP is influenced by whether profile images are included and their sizes.</p>
<p><strong>Experiment Setup</strong>:</p>
<ul>
<li>Profile data tested with:</li>
<li><ul>
<li><strong>No images</strong>.</li>
<li><strong>2 KB thumbnails</strong> combined with profile images of <strong>2 KB, 12 KB, and 500 KB</strong> sizes.</li>
</ul>
</li>
<li>Metrics: SoAR (Social Action Rating) measures the number of VP actions per second that meet the SLA (response time ≤ 100 ms).</li>
</ul>
<p><strong>Results</strong>:</p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/experiment1.png" alt="img"></p>
<ol>
<li><p><strong>No Images</strong>:</p>
<ul>
<li>MongoDB performed the best, outperforming SQL-X and CASQL by almost 2x.</li>
</ul>
</li>
<li><p><strong>12 KB Images</strong>:</p>
<ul>
<li>SQL-X’s SoAR dropped significantly, from thousands of actions per second to only hundreds.</li>
</ul>
</li>
<li><p><strong>500 KB Images</strong>:</p>
<ul>
<li><p><strong>SQL-X failed to meet the SLA (SoAR &#x3D; 0) because transmitting large images caused significant delays.</strong></p>
</li>
<li><p>MongoDB and CASQL also experienced a decrease in SoAR but performed better than SQL-X.</p>
</li>
</ul>
</li>
</ol>
<p><strong>Role of CASQL</strong>:</p>
<ul>
<li><p><strong>CASQL outperformed SQL-X due to its caching layer (memcached):</strong></p>
<ul>
<li><p>During a warm-up phase, 500,000 requests populate the cache with key-value pairs for member profiles.</p>
</li>
<li><p>Most requests are serviced by memcached instead of SQL-X, significantly improving performance with larger images (12 KB and 500 KB).</p>
</li>
</ul>
</li>
</ul>
<h2 id="Performance-Analysis-of-List-Friends"><a href="#Performance-Analysis-of-List-Friends" class="headerlink" title="Performance Analysis of List Friends"></a><strong>Performance Analysis of List Friends</strong></h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/experiment2.png" alt="img"></p>
<p><strong>1. SQL-X</strong></p>
<ul>
<li><strong>Process</strong>:<ul>
<li>Joins the <code>Friends</code> table with the <code>Members</code> table to fetch the friend list.</li>
<li>Friendship between two members is represented as a single record in the <code>Friends</code> table.</li>
</ul>
</li>
<li><strong>Performance</strong>:<ul>
<li><strong>When ϕ (number of friends) is 1000, SQL-X struggles due to the overhead of joining large tables and fails to meet SLA requirements.</strong></li>
</ul>
</li>
</ul>
<p><strong>2. CASQL</strong></p>
<ul>
<li><strong>Process</strong>:<ul>
<li>Uses a memcached caching layer to store and retrieve results of the LF action.</li>
<li>Results are cached as key-value pairs.</li>
</ul>
</li>
<li><strong>Performance</strong>:<ul>
<li>Outperforms SQL-X when ϕ is 50 or 100 by a small margin (&lt;10% improvement).</li>
<li><strong>At ϕ&#x3D;1000, memcached’s key-value size limit (1 MB) causes failures, as the data exceeds this limit.</strong></li>
<li>Adjusting memcached to support larger key-value pairs (e.g., 2 MB for 1000 friends with 2 KB thumbnails) could improve performance.</li>
</ul>
</li>
</ul>
<p><strong>3. MongoDB</strong></p>
<ul>
<li><strong>Process</strong>:<ul>
<li>Retrieves the <code>confirmedFriends</code> array from the referenced member’s document.</li>
<li>Can fetch friends’ profile documents one by one or as a batch.</li>
</ul>
</li>
<li><strong>Performance</strong>:<ul>
<li>Performs no joins, but its SLA compliance is poor for larger friend counts.</li>
<li>SoAR is zero for ϕ&#x3D;50,100,1000, as it fails to meet the 100 ms response time requirement.</li>
<li>For smaller friend lists (ϕ&#x3D;10), MongoDB achieves a SoAR of 6 actions per second.</li>
</ul>
</li>
</ul>
<h2 id="Mix-of-Read-and-Write-Actions"><a href="#Mix-of-Read-and-Write-Actions" class="headerlink" title="Mix of Read and Write Actions"></a><strong>Mix of Read and Write Actions</strong></h2><ul>
<li><strong>Purpose</strong>: Evaluates the performance of data stores under different ratios of read and write operations.</li>
<li><strong>Categories</strong>:<ul>
<li><strong>Read actions</strong>: Include operations like View Profile (VP), List Friends (LF), and View Friend Requests (VFR).</li>
<li><strong>Write actions</strong>: Modify friendship relationships and invalidate cached key-value pairs (e.g., Invite Friend, Accept Friend Request).</li>
</ul>
</li>
<li><strong>Mix Variations</strong>:<ul>
<li><strong>Very low writes (0.1%)</strong>: Dominantly read-heavy workloads.</li>
<li><strong>Low writes (1%)</strong>: Slightly higher frequency of write actions.</li>
<li><strong>High writes (10%)</strong>: Write-intensive workloads.</li>
</ul>
</li>
</ul>
<p><strong>Performance Analysis (Mix of Read and Write Actions)</strong></p>
<p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/experiment3.png" alt="img"></p>
<ul>
<li><strong>SoAR Comparison</strong>:<ul>
<li><strong>CASQL</strong> consistently achieves the highest SoAR for all write mixes due to its caching mechanism.</li>
<li><strong>MongoDB</strong> outperforms <strong>SQL-X</strong> by a factor of 3 across all workloads.</li>
</ul>
</li>
</ul>
<p><strong>Observations by Write Percentage:</strong></p>
<ol>
<li><p><strong>0.1% Writes (Read-Dominant)</strong>:</p>
<ul>
<li><p>CASQL significantly outperforms MongoDB due to efficient use of cached key-value pairs.</p>
</li>
<li><p>SQL-X lags due to the overhead of processing read actions directly from the RDBMS.</p>
</li>
</ul>
</li>
<li><p><strong>1% Writes</strong>:</p>
<ul>
<li><p>CASQL remains the best performer but shows sensitivity to increasing writes as it invalidates cached data, redirecting more queries to the RDBMS.</p>
</li>
<li><p>MongoDB maintains a consistent performance advantage over SQL-X.</p>
</li>
</ul>
</li>
<li><p><strong>10% Writes (Write-Heavy)</strong>:</p>
<ul>
<li><p><strong>CASQL slightly outperforms MongoDB, but the gap narrows due to the higher frequency of cache invalidations.</strong></p>
</li>
<li><p>SQL-X continues to struggle with write-heavy workloads due to its lack of caching.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h1><blockquote>
<p>Definition: A session is a sequence of actions performed by a socialite (user) in the social network.</p>
<p>Key Concepts:</p>
<ol>
<li>Think Time: Delay between consecutive actions within a session.</li>
<li>Inter-Arrival Time: Delay between sessions initiated by different socialites.</li>
</ol>
</blockquote>
<p><strong>Key Considerations</strong></p>
<ol>
<li><p><strong>Dependencies</strong>:</p>
<ul>
<li><p>Some sessions rely on specific database states (e.g., friends or pending requests).</p>
</li>
<li><p>For example, if m_i has no friends or pending requests, certain sessions terminate early.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Handling</strong>:</p>
<ul>
<li><p>BG uses in-memory data structures to simulate database states and prevent conflicts (e.g., multiple threads deleting the same comment).</p>
</li>
<li><p>Ensures integrity by managing semaphores and detecting unpredictable data.</p>
</li>
</ul>
</li>
<li><p><strong>Extensibility</strong>:</p>
<ul>
<li>BG allows developers to define new sessions by combining different mixes of actions.</li>
</ul>
</li>
</ol>
<h1 id="Parallelism"><a href="#Parallelism" class="headerlink" title="Parallelism"></a>Parallelism</h1><h2 id="BG’s-Scalable-Benchmarking-Framework"><a href="#BG’s-Scalable-Benchmarking-Framework" class="headerlink" title="BG’s Scalable Benchmarking Framework"></a>BG’s Scalable Benchmarking Framework</h2><p>To address these limitations, BG employs <strong>a shared-nothing architecture</strong> with the following components:</p>
<p><strong>1. BGCoord (Coordinator)</strong></p>
<ul>
<li><strong>Role</strong>: Oversees and coordinates the benchmarking process.</li>
<li><strong>Responsibilities</strong>:<ul>
<li><strong>Computes SoAR and Socialites ratings.</strong></li>
<li><strong>Assigns workloads to BGClients and monitors their progress.</strong></li>
<li><strong>Aggregates results (e.g., response times, throughput) for visualization.</strong></li>
</ul>
</li>
<li><strong>Process</strong>:<ul>
<li><strong>Splits the workload among N BGClients.</strong></li>
<li><strong>Ensures each BGClient works independently to prevent resource contention.</strong></li>
</ul>
</li>
</ul>
<p><strong>2. BGClient</strong></p>
<ul>
<li><strong>Role</strong>: Executes tasks assigned by BGCoord.</li>
<li><strong>Responsibilities</strong>:<ul>
<li><strong>Creates a database based on BG specifications.</strong></li>
<li><strong>Simulates workload actions and computes metrics like unpredictable data volume.</strong></li>
<li><strong>Periodically reports metrics to BGCoord for aggregation.</strong></li>
</ul>
</li>
</ul>
<p><strong>3. Visualization Deck</strong></p>
<ul>
<li><strong>Role</strong>: Provides a user interface for monitoring and controlling the benchmarking process.</li>
<li><strong>Features</strong>:<ul>
<li>Allows users to configure parameters (e.g., SLA, workloads).</li>
<li>Visualizes the ratings (SoAR, Socialites) and progress of the benchmarking.</li>
</ul>
</li>
</ul>
<p><strong>Scaling with BGClients</strong></p>
<ul>
<li><strong>Fragmentation</strong>:<ul>
<li><strong>The database is split into N logical fragments, each assigned to a BGClient.</strong></li>
<li><strong>Each fragment includes unique members, friendships, and resources, ensuring no overlap between BGClients.</strong></li>
</ul>
</li>
<li><strong>Decentralized D-Zipfian Distribution</strong>:<ul>
<li><strong>Used to balance workloads across nodes with different processing speeds.</strong></li>
<li><strong>Faster nodes handle larger fragments, ensuring equal workload completion times.</strong></li>
</ul>
</li>
</ul>
<h1 id="Unpredictable-Data"><a href="#Unpredictable-Data" class="headerlink" title="Unpredictable Data"></a>Unpredictable Data</h1><p><strong>Definition</strong>: <strong>Data that is stale, inconsistent, or invalid, produced due to race conditions, dirty reads, or eventual consistency.</strong></p>
<h2 id="BG’s-Validation-Process"><a href="#BG’s-Validation-Process" class="headerlink" title="BG’s Validation Process"></a>BG’s Validation Process</h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart1.png" alt="img"></p>
<h2 id="Validation-Implementation"><a href="#Validation-Implementation" class="headerlink" title="Validation Implementation"></a>Validation Implementation</h2><ol>
<li><strong>Log Generation</strong>:<ul>
<li><strong>BG generates read log records (observed values) and write log records (new or delta values).</strong></li>
</ul>
</li>
<li><strong>Offline Validation</strong>:<ul>
<li><strong>For each read log entry:</strong><ul>
<li><strong>BG computes a range of valid values using overlapping write logs.</strong></li>
<li><strong>If the observed value is outside this range, it is flagged as unpredictable.</strong></li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Impact-of-Time-to-Live-TTL-on-Unpredictable-Data"><a href="#Impact-of-Time-to-Live-TTL-on-Unpredictable-Data" class="headerlink" title="Impact of Time-to-Live (TTL) on Unpredictable Data"></a>Impact of Time-to-Live (TTL) on Unpredictable Data</h2><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/chart2.png" alt="img"></p>
<p><strong>Results</strong>:</p>
<ol>
<li><p><strong>Higher TTL Increases Stale Data</strong>:</p>
<ul>
<li><p>A higher TTL (e.g., 120 seconds) results in more stale key-value pairs, increasing the percentage of unpredictable data.</p>
</li>
<li><p>For T&#x3D;100T &#x3D; 100T&#x3D;100, unpredictable data is:</p>
<ul>
<li>~79.8% with TTL &#x3D; 30 seconds.</li>
<li>~98.15% with TTL &#x3D; 120 seconds.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Performance Trade-off</strong>:</p>
<ul>
<li><p><strong>A higher TTL improves performance (fewer cache invalidations) but increases stale data.</strong></p>
</li>
<li><p><strong>Lower TTL reduces stale data but impacts cache performance.</strong></p>
</li>
</ul>
</li>
</ol>
<h1 id="Heuristic-Search-for-Rating"><a href="#Heuristic-Search-for-Rating" class="headerlink" title="Heuristic Search for Rating"></a>Heuristic Search for Rating</h1><p><strong>Why Use Heuristic Search?</strong></p>
<ul>
<li>Exhaustive search starting from T&#x3D;1 to the maximum T is time-consuming.</li>
<li>MongoDB with T&#x3D;1000 and Δ&#x3D;10 minutes would take 7 days for exhaustive testing.</li>
</ul>
<p><strong>Steps in Heuristic Search</strong>:</p>
<ol>
<li><p><strong>Doubling Strategy</strong>:</p>
<ul>
<li><p><strong>Start with T&#x3D;1, double T after each successful experiment.</strong></p>
</li>
<li><p><strong>Stop when SLA fails, narrowing down T to an interval.</strong></p>
</li>
</ul>
</li>
<li><p><strong>Binary Search</strong>:</p>
<ul>
<li><p><strong>Identify the T corresponding to max throughput within the interval.</strong></p>
</li>
<li><p><strong>Used for both SoAR (peak throughput) and Socialites (maximum concurrent threads).</strong></p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>What system metrics does BG quantify?</strong></p>
<p><strong>SoAR (Social Action Rating):</strong></p>
<ul>
<li>The highest throughput (actions per second) that satisfies a given SLA, ensuring at least α% of requests meet the response time β, with at most τ% of requests observing unpredictable data.</li>
</ul>
<p><strong>Socialites Rating:</strong></p>
<ul>
<li>The maximum number of simultaneous threads (or users) that a data store can support while still meeting the SLA requirements.</li>
</ul>
<p><strong>Throughput</strong>:</p>
<ul>
<li>Total number of completed actions per unit of time.</li>
</ul>
<p><strong>Response Time:</strong></p>
<ul>
<li>Average or percentile-based latency for each action.</li>
</ul>
<p><strong>Unpredictable Data:</strong></p>
<ul>
<li>The percentage of actions that observe stale, inconsistent, or invalid data during execution.</li>
</ul>
<p><strong>How does BG scale to generate a large number of requests?</strong></p>
<p>BG employs <strong>a shared-nothing architecture</strong> with the following mechanisms to scale effectively:</p>
<ol>
<li><p><strong>Partitioning Members and Resources:</strong></p>
<ul>
<li><p>BGCoord <strong>partitions</strong> the database into <strong>logical fragments</strong>, each containing <strong>a unique subset</strong> of members, their resources, and relationships.</p>
</li>
<li><p>These fragments are assigned to individual BGClients.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple BGClients:</strong></p>
<ul>
<li><p>Each BGClient operates <strong>independently</strong>, generating workloads for its assigned logical fragment.</p>
</li>
<li><p>By running <strong>multiple</strong> BGClients <strong>in parallel</strong> across different nodes, BG can scale horizontally to handle millions of requests.</p>
</li>
</ul>
</li>
<li><p><strong>D-Zipfian Distribution:</strong></p>
<ul>
<li><p>To ensure realistic and scalable workloads, BG uses a decentralized Zipfian distribution (D-Zipfian) that <strong>dynamically assigns</strong> requests to BGClients based on node performance.</p>
</li>
<li><p>Faster nodes receive a larger share of the logical fragments, ensuring even workload distribution.</p>
</li>
</ul>
</li>
<li><p><strong>Concurrency Control:</strong></p>
<ul>
<li>BG <strong>prevents simultaneous threads from issuing actions for the same user</strong>, maintaining the integrity of modeled user interactions and avoiding resource contention.</li>
</ul>
</li>
</ol>
<p><strong>If two modeled users, A and B, are already friends, does BG generate a friend request from A to B?</strong></p>
<p>No, BG does not generate a friend request from A to B if they are already friends.</p>
<p>Before generating a friend request, BG <strong>validates</strong> whether the relationship between A and B is pending or already confirmed. For example, in the <code>InviteFrdSession</code>, BG only selects users who have no existing “friend” or “pending” relationship with the requester to receive a new friend request.</p>
<p>Reference: <a target="_blank" rel="noopener" href="https://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper93.pdf">https://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper93.pdf</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/OCC%20%E5%92%8C%20MVCC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/OCC%20%E5%92%8C%20MVCC/" class="post-title-link" itemprop="url">OCC 和 MVCC</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-11 00:00:00" itemprop="dateCreated datePublished" datetime="2024-10-11T00:00:00-07:00">2024-10-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-31 01:18:09" itemprop="dateModified" datetime="2025-05-31T01:18:09-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Time-Stamp-Based-Protocols"><a href="#Time-Stamp-Based-Protocols" class="headerlink" title="Time-Stamp Based Protocols"></a>Time-Stamp Based Protocols</h1><p>Suppose transaction Ti issues read(Q):</p>
<ul>
<li>If TS(Ti) &lt; W-TimeStamp(Q), then Ti needs to read the value of Q which was already overwritten. Hence the read request is rejected and Ti is rolled back.</li>
<li>If TS(Ti) &gt;&#x3D; W-TimeStamp(Q), then the read operation is executed and the R-timeStamp(Q) is set to the maximum of R-TimeStamp(Q) and TS(Ti).</li>
</ul>
<p>Suppose transaction Ti issues write(Q):</p>
<ul>
<li>If TS(Ti) &lt; R-TimeStamp(Q), then this implies that some transaction has already consumed the value of Q and Ti should have produced a value before that transaction read it. Thus, the write request is rejected and Ti is rolled back.</li>
<li>If TS(Ti) &lt; W-TimeStamp(Q), then Ti is trying to write an obsolete value of Q. Hence reject Ti’s request and roll it back. &#x2F; Ignore this write operation. (Tomas’s Write Rule)</li>
<li>Otherwise, execute the write(Q) operation and update W-TimeStamp(Q) to TS(Ti).</li>
</ul>
<h1 id="OCC"><a href="#OCC" class="headerlink" title="OCC"></a>OCC</h1><p>Each transaction Ti has three phases:</p>
<ul>
<li>Read phase: reads the value of data items and copies its contents to variables local to Ti. All writes are performed on the temporary local variables.</li>
<li>Validation phase: Ti determines whether the local variables whose values have been overwritten can be copied to the database. If not, then abort. Otherwise, proceed to Write phase.</li>
<li><ul>
<li>When validating transaction Tj, for all transactions Ti with TS(Ti) &lt; TS(Tj), one of the following must hold:</li>
<li><ul>
<li>Finish(Ti) &lt; Start(Tj), OR</li>
<li>Set of data items written by Ti does not intersect with the set of data items read by Tj, and Ti completes its write phase before Tj starts its validation phase.</li>
</ul>
</li>
</ul>
</li>
<li>Write phase: The values stored in local variables overwrite the value of the data items in the database.</li>
</ul>
<p>A transaction has three time stamps:</p>
<ul>
<li>Start(Ti): When Ti started its execution.</li>
<li>Validation(Ti): When Ti finished its read phase and started its validation.</li>
<li>Finish(Ti): Done with the write phase.</li>
</ul>
<h1 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h1><p>Assume that transaction Ti issues either a read(Q) or a write(Q) operation.</p>
<p>Let Qk denote the version of Q whose write timestamp is the largest write timestamp less than TS(Ti), i.e., W-TimeStamp(Qk) &lt; TS(Ti).</p>
<ul>
<li>If Ti issues a Read(Q), then return the value of Qk.</li>
<li>If Ti issues a write(Q), and TS(Ti) &lt; R-TimeStamp(Qk), then Ti is rolled back.</li>
<li>Otherwise, a new version of Qk is created.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yihangwe.github.io/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/DynamoDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yihang Wei">
      <meta itemprop="description" content="聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EthanWeee">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/2024/10/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/DynamoDB/" class="post-title-link" itemprop="url">DynamoDB</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-04 00:00:00" itemprop="dateCreated datePublished" datetime="2024-10-04T00:00:00-07:00">2024-10-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-31 20:59:45" itemprop="dateModified" datetime="2025-05-31T20:59:45-07:00">2025-05-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="index"><span itemprop="name">数据库系统</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>Dynamo has the ability of incremental scalability and predictable high performance, but it carries the operational complexity of self-managed large database systems.</p>
<p>SimpleDB is easy to administrate a cloud service, consistency, and a table-based data model, but it has limitations that tables have a small capacity in terms of storage and of request throughput, and that a unpredictable query and write latency.</p>
<p>DynamoDB &#x3D; Dynamo + SImpleDB</p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/dynamodb_arch.png" alt="img"></p>
<ol>
<li><strong>DynamoDB Tables and Primary Keys</strong>: A DynamoDB table is a collection of items, each uniquely identified by a primary key. The primary key can be a partition key or a composite key (partition key + sort key).</li>
<li><strong>Secondary Indexes</strong>: DynamoDB supports secondary indexes, allowing queries using alternate keys in addition to the primary key, enhancing query capabilities.</li>
<li><strong>Partitions and Replication Groups</strong>: DynamoDB tables are divided into multiple partitions, with each partition managing a contiguous key range. Each partition has multiple replicas across different Availability Zones for high availability and durability. A <strong>Multi-Paxos</strong> consensus is used for leader election within the replication group, and the leader handles writes and strongly consistent reads.</li>
<li><strong>Write-Ahead Logs and Consistency</strong>: The leader replica generates a write-ahead log for write requests. A write is acknowledged once a quorum of replicas persists the log. DynamoDB supports both strongly consistent and eventually consistent reads.</li>
<li><strong>Failure Detection and Leader Election</strong>: If the current leader is detected as unhealthy, other replicas initiate a new election. The new leader can only start serving writes or consistent reads after the old leader’s lease expires.</li>
<li><strong>Autoadmin Service</strong>: The autoadmin service monitors the health of the fleet and partitions, scaling tables and replacing unhealthy replicas or hardware to maintain system stability. It automatically detects and resolves issues, ensuring a stable and healthy infrastructure.</li>
</ol>
<h1 id="Journey-from-provisioned-to-on-demand"><a href="#Journey-from-provisioned-to-on-demand" class="headerlink" title="Journey from provisioned to on-demand"></a>Journey from provisioned to on-demand</h1><ul>
<li><p><strong>Bursting:</strong> To address the issue of uneven workload distribution across partitions, DynamoDB introduced the concept of <strong>bursting</strong>. Bursting allows an application to utilize unused capacity at the partition level when its provisioned throughput is exhausted, helping to <strong>handle short-term spikes</strong> in workload. DynamoDB retains unused capacity in a partition for <strong>up to 300 seconds</strong>, which can be tapped into when the consumed capacity exceeds the provisioned capacity. This reserved capacity is referred to as <strong>burst capacity</strong>.</p>
<p><strong>How It Works</strong>: DynamoDB manages throughput using multiple <strong>token buckets</strong>:</p>
<ul>
<li><p><strong>Each partition has two token buckets</strong>: one for allocated capacity and another for burst capacity. <strong>Each storage node has a token bucket</strong> that controls the overall load across partitions hosted on that node.</p>
</li>
<li><p>When a read or write request arrives at a storage node, the system first checks the partition’s token bucket. If the allocated capacity has been exhausted, burst capacity can be used, but only if there are available tokens at both the burst token level and the node level.</p>
</li>
<li><p><strong>Additional Check for Write Requests</strong>: When using burst capacity for write requests, an additional check is performed to ensure that other replica nodes for the partition also have sufficient capacity. This ensures that the write operation can be completed safely and consistently across all replicas. The leader replica periodically gathers information about the node-level capacity of other members in the replication group to facilitate this process.</p>
</li>
</ul>
</li>
<li><p><strong>Adaptive (deprecated):</strong> DynamoDB launched adaptive capacity to better absorb <strong>longlived</strong> spikes that cannot be absorbed by the burst capacity. Adaptive capacity allowed DynamoDB to better absorb workloads that had heavily skewed access patterns across partitions. Adaptive capacity actively monitored the provisioned and consumed capacity of all the tables.</p>
<ul>
<li><p>If a table experienced throttling and the table level throughput was not exceeded, then it would automatically increase (boost) the allocated throughput of the partitions of the table using a proportional control algorithm.</p>
</li>
<li><p>If the table was consuming more than its provisioned capacity then capacity of the partitions which received the boost would be decreased. The autoadmin system ensured that partitions receiving boost were relocated to an appropriate node that had the capacity to serve the increased throughput.</p>
</li>
</ul>
</li>
<li><p>GAC, how does it work:</p>
<ul>
<li><p><strong>Global Throughput Tracking and Management</strong></p>
<ul>
<li><p><strong>Global Token Management</strong>: GAC uses a <strong>token bucket system</strong> to manage the overall throughput (RCUs and WCUs) of a DynamoDB table.</p>
</li>
<li><p><strong>Token Buckets</strong>: Each request router maintains a <strong>local token bucket</strong> to handle requests. When tokens are depleted locally, the router requests more tokens from GAC, which manages the global distribution of these tokens across partitions.</p>
</li>
</ul>
</li>
<li><p><strong>Dynamic Token Allocation</strong></p>
<ul>
<li><p><strong>Periodic Replenishment</strong>: GAC regularly communicates with the request routers every few seconds to replenish their token buckets. The amount of tokens allocated is based on the overall resource consumption of the table, particularly when certain partitions are experiencing high traffic.</p>
</li>
<li><p><strong>Handling Hot Partitions</strong>: When specific partitions become hot, GAC dynamically allocates additional tokens to those partitions.</p>
</li>
</ul>
</li>
<li><p><strong>Capacity Limits and Isolation</strong></p>
<ul>
<li><p><strong>Global Throughput Limits</strong>: GAC ensures that the total number of tokens allocated to partitions does not exceed the <strong>provisioned capacity</strong> for the entire table.</p>
</li>
<li><p><strong>Node-Level Limits</strong>: Although GAC allocates tokens globally, each partition is subject to the <strong>maximum throughput capacity of its storage node</strong>. This ensures that no single partition can consume more than its node’s allowable resources.</p>
</li>
</ul>
</li>
<li><p><strong>Stateless and Distributed Design</strong></p>
<ul>
<li><p><strong>Stateless Operation</strong>: GAC operates in a <strong>stateless manner</strong>, meaning it calculates token allocations in real-time based on incoming client requests. It doesn’t rely on long-term stored states, so GAC servers can be restarted or stopped without affecting the system’s overall operation.</p>
</li>
<li><p><strong>Distributed Architecture</strong>: GAC uses a distributed architecture, where multiple GAC instances coordinate using a <strong>hash ring</strong>. This allows GAC to scale horizontally and handle requests from multiple routers efficiently.</p>
</li>
</ul>
</li>
<li><p><strong>Defense-in-Depth with Partition-Level Token Buckets</strong></p>
<ul>
<li><p><strong>Partition-Level Control</strong>: Even though GAC manages tokens globally, DynamoDB still retains <strong>partition-level token buckets</strong> for additional protection. These buckets ensure that no single partition consumes excessive resources, offering a secondary layer of isolation and control.</p>
</li>
<li><p><strong>Resource Isolation</strong>: Partition-level token buckets prevent any single application or partition from monopolizing the resources of the storage node.</p>
</li>
</ul>
</li>
<li><p><strong>Token Consumption and Replenishment Process</strong></p>
<ul>
<li><p>When a request is made, the request router checks its local token bucket for available tokens. If enough tokens are present, the request is processed.</p>
</li>
<li><p>If the local tokens are depleted, the request router asks GAC for more tokens.</p>
</li>
<li><p>GAC calculates the global consumption of tokens for the table and allocates more tokens to the router based on overall resource usage.</p>
</li>
<li><p>Once tokens are used up or expire, the process repeats, with the router requesting new tokens from GAC.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Proactive load balancing mechanism</p>
<ul>
<li><p>Independent Monitoring: Each storage node independently monitors the total throughput (read&#x2F;write requests) and data size of all the partition replicas it hosts.</p>
</li>
<li><p>Threshold Detection: When the throughput or data size of a partition replica approaches or exceeds a predefined threshold of the node’s capacity, that partition replica is flagged as a candidate for migration.</p>
</li>
<li><p>Reporting to Autoadmin Service: The storage node reports the list of over-utilized partition replicas to the Autoadmin service, which manages the load balancing process.</p>
</li>
<li><p>Automatic Migration: The Autoadmin service finds a new storage node, usually located in a different Availability Zone, that can accommodate the migrating partition replica. This new node must have enough spare capacity to handle the increased load.</p>
</li>
<li><p>GAC 和 bursting 都擅长处理 短期或临时的高负载，但如果某个分区长期处于高负载状态（例如一个分区持续有热点键），这些机制可能无法完全消除该分区对特定节点的影响。在这种情况下，自动迁移分区副本是更长期有效的解决方案。</p>
</li>
</ul>
</li>
<li><p>Even with GAC and partition bursting capacity, DynamoDB tables may still experience throttling if traffic is heavily concentrated on a specific set of items. When the throughput for a partition exceeds a certain threshold, the system splits the partition according to the observed key distribution, rather than simply splitting the key range in the middle. These smaller partitions are typically distributed to different storage nodes. However, some workloads may not benefit from this mechanism, such as:</p>
<ul>
<li><p>A partition where traffic is concentrated on a single item.</p>
</li>
<li><p>A partition where the key range is accessed sequentially.</p>
</li>
</ul>
</li>
<li><p>DynamoDB’s <strong>on-demand tables</strong> eliminate the need for customers to manually set throughput. The system automatically adjusts resources based on actual read and write requests, enabling it to quickly adapt to sudden traffic increases. Specifically:</p>
<ul>
<li><p>DynamoDB automatically scales up to <strong>twice the previous peak traffic</strong> to handle more requests instantly.</p>
</li>
<li><p>If traffic continues to increase, DynamoDB further allocates more resources to prevent throttling and maintain performance.</p>
</li>
</ul>
<p>The scaling mechanism for on-demand tables is achieved through <strong>partition splitting</strong>, where partitions are split based on traffic patterns to ensure each partition has sufficient resources. At the same time, <strong>GAC (Global Admission Control)</strong> monitors the system to prevent any single application from consuming too many resources, maintaining overall system stability.</p>
</li>
</ul>
<p><strong>机制之间的关联总结：</strong></p>
<ul>
<li><strong>On-demand tables</strong> 依赖 <strong>GAC</strong> 和 <strong>bursting</strong> 来动态扩展资源，处理流量波动。</li>
<li><strong>GAC</strong> 管理整个系统的全局资源分配，确保突发和按需扩展时不影响其他应用，同时在必要时与 <strong>proactive load balancing</strong> 机制配合，进行分区迁移。</li>
<li><strong>Bursting</strong> 提供短期解决方案，而当负载持续增加时，系统会通过 <strong>主动负载平衡</strong> 来长期调整资源分配，防止系统瓶颈。</li>
</ul>
<h1 id="Durability-and-correctness"><a href="#Durability-and-correctness" class="headerlink" title="Durability and correctness"></a>Durability and correctness</h1><h2 id="Hardware-failures"><a href="#Hardware-failures" class="headerlink" title="Hardware failures"></a>Hardware failures</h2><p>The write-ahead logs in DynamoDB are crucial for ensuring data durability and crash recovery. Each partition has three replicas that store the write-ahead logs. To enhance durability, the logs are periodically archived to Amazon S3. The unarchived logs typically amount to a few hundred megabytes.</p>
<p>In large-scale systems, hardware failures such as memory or disk failures are common. When a node fails, all replication groups hosted on that node are reduced to two copies. The process of repairing a storage replica can take several minutes, as it involves copying both the B-tree and the write-ahead logs.</p>
<p>When the system detects an unhealthy replica, the leader of the replication group adds a log replica to ensure data durability is not compromised. Since only the recent write-ahead logs need to be copied without the B-tree, adding the log replica takes just a few seconds. This quick addition helps restore the affected replication group, ensuring that the most recent writes remain highly durable.</p>
<h2 id="Silent-data-errors"><a href="#Silent-data-errors" class="headerlink" title="Silent data errors"></a>Silent data errors</h2><p>Hardware failures can cause incorrect data storage: In DynamoDB, errors may occur due to issues with storage media, CPU, or memory, and these errors are often difficult to detect.</p>
<p>Extensive use of checksums: DynamoDB maintains checksums for every log entry, message, and log file to detect silent errors and ensure data integrity during each data transfer. When messages are transmitted between nodes, checksums verify whether errors occurred during transmission.</p>
<p>Log archiving and validation: Each log file archived to S3 has a manifest that records details such as the table, partition, and data markers. Before uploading, the archiving agent performs various checks, including checksum validation, verifying that the log belongs to the correct table and partition, and ensuring that there are no gaps in the sequence numbers.</p>
<p>Multiple replica log archiving: Log archiving agents run on all three replicas. If one agent finds that a log file has already been archived, it downloads the file and compares it with the local write-ahead log to verify data integrity.</p>
<p>Checksum validation during S3 upload: Every log file and manifest file is uploaded to S3 with a content checksum. S3 verifies this checksum during the upload process to catch any errors in data transmission.</p>
<h2 id="Continuous-verification"><a href="#Continuous-verification" class="headerlink" title="Continuous verification"></a>Continuous verification</h2><p>Continuous Data Integrity Verification: DynamoDB continuously verifies data at rest to detect silent data errors and bit rot, which can occur due to hardware failures or data corruption. This is a critical defense mechanism for maintaining data reliability.</p>
<p>Scrub Process: The scrub process is central to detecting unforeseen errors. It checks two main aspects:</p>
<ul>
<li><strong>Replica Consistency</strong>: Ensures that all three replicas in a replication group have identical data.</li>
<li><strong>Archived Log Reconstruction</strong>: Rebuilds an offline replica using archived write-ahead logs from S3 and verifies that it matches the live replica.</li>
</ul>
<p>Verification Mechanism: Scrub computes checksums for the live replicas and compares them with those generated from replicas built using archived logs.</p>
<p>Defense in Depth: This mechanism ensures that live storage replicas and those rebuilt from historical logs remain consistent, providing confidence in the system’s integrity and reliability.</p>
<h2 id="Backups-and-restores"><a href="#Backups-and-restores" class="headerlink" title="Backups and restores"></a>Backups and restores</h2><p>Backup and Restore Mechanism: DynamoDB supports backup and restore to protect against logical corruption caused by bugs in customer applications. Backups and restores are built using write-ahead logs stored in S3 and do not affect table performance or availability.</p>
<p>Backup Consistency: Backups are full copies of DynamoDB tables, consistent across multiple partitions to the nearest second, and stored in Amazon S3. Data can be restored to a new DynamoDB table at any time.</p>
<p>Point-in-Time Restore: DynamoDB supports point-in-time restore, allowing customers to restore a table to any point within the last 35 days. This feature creates periodic snapshots of table partitions and stores them in S3.</p>
<p>Snapshots and Write-Ahead Logs: For point-in-time restore, DynamoDB identifies the closest snapshots to the requested time, applies the corresponding write-ahead logs, and restores the table to the desired state.</p>
<h1 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a>Availability</h1><h2 id="Write-and-consistent-read-availability"><a href="#Write-and-consistent-read-availability" class="headerlink" title="Write and consistent read availability"></a>Write and consistent read availability</h2><p>Write Availability: DynamoDB partition write availability depends on having a healthy leader and a healthy write quorum. A write quorum in DynamoDB requires two out of three replicas across different Availability Zones (AZs) to be healthy.</p>
<p>Handling Write Quorum Failures: <strong>If one replica becomes unresponsive, the leader adds a log replica, which is the fastest way to meet the quorum requirement and minimize write disruptions caused by an unhealthy quorum.</strong></p>
<p>Consistent Reads: Consistent reads are served by the leader replica. <strong>If the leader fails, other replicas detect the failure and elect a new leader to minimize disruptions to consistent read availability.</strong></p>
<p>Impact of Log Replicas: The introduction of log replicas was a significant system change. The use of the formally proven Paxos protocol provided confidence to safely implement this change, increasing system availability. DynamoDB can run millions of Paxos groups with log replicas in a single region.</p>
<p>Eventually Consistent Reads: Eventually consistent reads can be served by any of the replicas.</p>
<h2 id="Failure-detection"><a href="#Failure-detection" class="headerlink" title="Failure detection"></a>Failure detection</h2><p>New Leader Waits for Lease Expiry: A newly elected leader must wait for the old leader’s lease to expire before handling traffic, causing a few seconds of disruption where no new writes or consistent reads can be processed.</p>
<p>Importance of Leader Failure Detection: Quick and robust leader failure detection is crucial for minimizing disruptions. False positives in failure detection can lead to unnecessary leader elections, further disrupting availability.</p>
<p>Impact of Gray Network Failures: Gray network failures, such as communication issues between nodes or routers, can result in false or missed failure detections. These failures can trigger unnecessary leader elections, causing availability interruptions.</p>
<p>Improved Failure Detection Algorithm: To address the availability issues caused by gray failures, DynamoDB’s failure detection algorithm was improved. <strong>When a follower attempts to trigger a failover, it first checks with other replicas to see if they can still communicate with the leader. If they report the leader is healthy, the follower cancels the failover attempt.</strong> This change significantly reduced false leader elections and minimized availability disruptions.</p>
<h2 id="Metadata-availability"><a href="#Metadata-availability" class="headerlink" title="Metadata availability"></a>Metadata availability</h2><p>Metadata Needs for Request Routers: DynamoDB’s request routers require metadata mapping between table primary keys and storage nodes. Initially, this metadata was stored in DynamoDB, and the routers cached it locally. Although the cache hit rate was high, cache misses or cold starts caused metadata lookup traffic spikes, potentially destabilizing the system.</p>
<p>Caching Challenges: When caches failed or during cold starts, request routers frequently queried the metadata service, putting immense pressure on it and leading to cascading failures in other parts of the system.</p>
<p>Introduction of MemDS: <strong>To reduce reliance on local caches, DynamoDB introduced MemDS, a distributed in-memory data store for storing and replicating metadata.</strong> MemDS scales horizontally to handle all incoming requests and stores data in a compressed format. It uses a Perkle tree structure, combining Patricia and Merkle tree features for efficient key lookups and range queries.</p>
<p>Perkle Tree Operations: MemDS supports efficient key lookups, range queries, and special operations like floor (find the largest key ≤ given key) and ceiling (find the smallest key ≥ given key) for metadata retrieval.</p>
<p>New Partition Map Cache: DynamoDB implemented a new cache on request routers, addressing the issues of bimodal behavior. Even when a cache hit occurs, an asynchronous call is made to MemDS to refresh the cache. This ensures that MemDS consistently handles a steady volume of traffic, preventing reliance on cache hit ratios and avoiding cascading failures when caches become ineffective.</p>
<p>Partition Membership Updates: DynamoDB storage nodes, the authoritative source of partition membership data, push updates to MemDS. If a request router queries an incorrect storage node due to outdated information, the node provides updated membership data or triggers a new MemDS lookup.</p>
<h1 id="Programming-Interface"><a href="#Programming-Interface" class="headerlink" title="Programming Interface"></a>Programming Interface</h1><ol>
<li><p><strong>Key-Value Store</strong></p>
<p>DynamoDB allows users to create tables that can grow almost indefinitely. Each table is a collection of items, and each item is a collection of attributes. Each item is uniquely identified by a primary key, ensuring uniqueness within the table. DynamoDB provides a simple interface to store or retrieve items from a table or an index.</p>
</li>
<li><p><strong>Read and Write Operations</strong></p>
<p>DynamoDB operates as a key-value store, and the most common operations used by applications involve reading and writing data. These operations include:</p>
<ul>
<li><p><strong>GetItem</strong>: Retrieves an item with a given primary key.</p>
</li>
<li><p><strong>PutItem</strong>: Inserts a new item or replaces an existing one.</p>
</li>
<li><p><strong>UpdateItem</strong>: Updates an existing item, or adds it if it doesn’t exist.</p>
</li>
<li><p><strong>DeleteItem</strong>: Deletes an item from the table based on the primary key.</p>
</li>
</ul>
<p>These last three operations (PutItem, UpdateItem, and DeleteItem) are collectively referred to as writes. A write operation can optionally include conditions that must be satisfied for the operation to be executed successfully. For instance, you could specify that a PutItem operation should only succeed if the item doesn’t already exist.</p>
</li>
<li><p><strong>Transactional Operations</strong></p>
<p>DynamoDB supports transactions through two key operations:</p>
<ul>
<li><p><strong>TransactGetItems</strong>: Used for reading multiple items atomically. It retrieves the latest versions of items from one or more tables at a single point in time, ensuring consistency. If any conflicting operation is modifying an item that’s being read, the transaction will be rejected.</p>
</li>
<li><p><strong>TransactWriteItems</strong>: This is used for performing atomic writes across multiple items and tables. It allows you to create, update, or delete multiple items in one or more tables within a single atomic transaction. This ensures that either all changes happen, or none do. The operation is synchronous and idempotent (meaning it can be retried without causing duplicate effects). TransactWriteItems can include conditions on the current values of the items, and the operation is rejected if these conditions aren’t met.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Transactions"><a href="#Transactions" class="headerlink" title="Transactions"></a>Transactions</h1><p><img src="/../../images/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/dynamodb_txn.png" alt="img"></p>
<p><strong>Request Router (RR)</strong>:</p>
<ul>
<li>The <strong>Request Router</strong> is the first major component that handles incoming requests after they pass through the network.</li>
<li><strong>Authentication and Authorization</strong>: RR typically interacts with an <strong>Authentication System</strong> to ensure that the request is valid and the user has the proper permissions to access or modify the data.</li>
<li><strong>Routing Requests</strong>: Once a request is authenticated, the RR determines which <strong>Storage Nodes</strong> the request should be forwarded to. It uses the <strong>Metadata System</strong> to map the key(s) involved in the request to the correct storage nodes, as the data is distributed across many nodes.</li>
<li><strong>Forwarding Requests</strong>: Depending on whether the operation is a simple read&#x2F;write or part of a larger transaction, the RR may route the request directly to storage nodes or to the Transaction Coordinator.</li>
</ul>
<p><strong>Transaction Coordinator (TC)</strong>:</p>
<ul>
<li>The <strong>Transaction Coordinator</strong> plays a central role in handling transactions that span multiple items or storage nodes.</li>
<li><strong>Transaction Management</strong>: For requests that involve multiple storage nodes or require consistency (e.g., multi-item writes in a transaction), the RR forwards the request to the TC. The TC is responsible for breaking down the transaction into individual operations and coordinating these operations across the necessary storage nodes.</li>
<li><strong>Distributed Transaction Execution</strong>: The TC ensures that the operations follow the appropriate protocol (e.g., two-phase commit) to guarantee atomicity and consistency, ensuring that all parts of the transaction are either completed successfully or rolled back.</li>
<li><strong>Timestamp Assignment and Conflict Resolution</strong>: In a timestamp-based system like DynamoDB, the TC may assign timestamps to ensure the correct ordering of operations and manage any potential conflicts between concurrent transactions.</li>
</ul>
<p>In summary:</p>
<ul>
<li><strong>Request Router (RR)</strong> handles initial authentication and routing of requests to the appropriate storage nodes or transaction coordinator.</li>
<li><strong>Transaction Coordinator (TC)</strong> manages distributed transactions, ensuring data consistency and handling multi-node operations.</li>
</ul>
<h1 id="Transaction-execution"><a href="#Transaction-execution" class="headerlink" title="Transaction execution"></a>Transaction execution</h1><ol>
<li><p><strong>Transaction Routing</strong></p>
<p>All operation requests first reach a set of frontend hosts known as request routers. These routers are responsible for authenticating the requests and routing them to the appropriate storage nodes. Storage nodes are mapped based on key ranges. For transaction management, the routers forward transaction operations to transaction coordinators.</p>
<p>Transaction coordinators break down the transaction into multiple operations targeting different items and coordinate the execution of these operations across the storage nodes using a distributed protocol.</p>
</li>
<li><p><strong>Timestamp Ordering</strong></p>
<p>Each transaction is assigned a timestamp that defines its logical execution order. Multiple transaction coordinators operate in parallel, and different coordinators assign timestamps to different transactions. As long as transactions execute in the assigned order, serializability is maintained.</p>
<p>The storage nodes are responsible for ensuring that the operations on the items they manage are executed in the correct order and rejecting transactions that cannot be properly ordered.</p>
</li>
<li><p><strong>Write Transaction Protocol</strong></p>
<p>DynamoDB uses a two-phase commit protocol to ensure that the write operations within a transaction are atomic and executed in the correct order. In the prepare phase, the coordinator prepares all the write operations. If all storage nodes accept the operations, the transaction is committed; otherwise, it is canceled.</p>
<p>The storage nodes record the timestamp and metadata of each item involved in the transaction to ensure the transaction is handled correctly.</p>
</li>
<li><p><strong>Read Transaction Protocol</strong></p>
<p>Read transactions also use a two-phase protocol, but it differs from the write transaction protocol. DynamoDB designed a two-phase read protocol without write operations to avoid adding latency and costs to reads.</p>
<p>In the first phase, the coordinator reads all the items involved in the transaction, along with their Log Sequence Numbers (LSN). In the second phase, if the LSN has not changed, the read is successful; otherwise, the read is rejected.</p>
</li>
<li><p><strong>Recovery and Fault Tolerance</strong></p>
<p>If a storage node fails, the leadership role transfers to another storage node within the same replication group, with transaction metadata persistently stored and replicated across the nodes.</p>
<p>Transaction coordinator failures are more complex. Coordinators maintain a persistent record of each transaction to ensure atomicity and completeness. Recovery managers periodically scan these transaction records, looking for incomplete transactions, and reassign them to new coordinators to resume execution.</p>
</li>
</ol>
<h1 id="Two-phase-commit-2PC"><a href="#Two-phase-commit-2PC" class="headerlink" title="Two-phase commit (2PC)"></a>Two-phase commit (2PC)</h1><ol>
<li><p><strong>Prepare Phase</strong></p>
<p>In the prepare phase, the transaction coordinator (TC) is responsible for sending the transaction’s write operations to all the participating storage nodes. The coordinator breaks down the transaction into individual operations targeting specific data items and sends a prepare message to each storage node involved. This message includes:</p>
<ul>
<li><p>The transaction’s timestamp.</p>
</li>
<li><p>The transaction’s unique identifier (ID).</p>
</li>
<li><p>The specific operation to be performed on the data item (such as insert, update, or delete).</p>
</li>
</ul>
<p>Upon receiving the prepare message, each storage node evaluates whether it can accept the transaction. The storage node will accept the transaction’s write operation if all of the following conditions are met:</p>
<ul>
<li><p><strong>Preconditions</strong> are satisfied (e.g., a condition might be that the item must exist, or that it has a certain value).</p>
</li>
<li><p>The write operation does not violate any <strong>system restrictions</strong> (e.g., exceeding the maximum item size).</p>
</li>
<li><p>The transaction’s timestamp is <strong>greater than</strong> the item’s last write timestamp, indicating that this operation is the most recent.</p>
</li>
<li><p>There are no <strong>ongoing transactions</strong> attempting to write to the same item.</p>
</li>
</ul>
<p>If all participating storage nodes accept the transaction during the prepare phase, the coordinator moves to the commit phase. If any node rejects the transaction (e.g., due to a failed precondition or timestamp conflict), the transaction is canceled.</p>
</li>
<li><p><strong>Commit Phase</strong></p>
<p>Once the transaction has been accepted by all storage nodes during the prepare phase, the coordinator enters the commit phase. During this phase, the coordinator sends a commit message to all the storage nodes, instructing them to apply the write operations. Each storage node then:</p>
<ul>
<li><p>Applies the prepared write operations to the local items.</p>
</li>
<li><p>Updates the <strong>timestamp</strong> of the item to reflect the transaction’s timestamp.</p>
</li>
<li><p>Updates the timestamps of any items where preconditions were checked, even if no write operation was performed.</p>
</li>
</ul>
<p>If any node rejects the transaction during the prepare phase, the coordinator sends a cancel message to all storage nodes, instructing them to discard any prepared changes. No writes are applied, ensuring atomicity.</p>
</li>
</ol>
<h1 id="Adapting-timestamp-ordering-for-key-value-operations"><a href="#Adapting-timestamp-ordering-for-key-value-operations" class="headerlink" title="Adapting timestamp ordering for key-value operations"></a>Adapting timestamp ordering for key-value operations</h1><ol>
<li><p><strong>Individual Item Read Operations</strong></p>
<p>In DynamoDB, even if there is a prepared transaction attempting to read to a particular data item, the system still allows read operations on that item. Specifically:</p>
<ul>
<li><p><strong>Bypassing the transaction coordinator</strong>: Non-transactional <code>GetItem</code> operations are routed directly to the storage node responsible for the item, bypassing the transaction coordinator. This avoids potential transaction locks or delays.</p>
</li>
<li><p><strong>Returning the latest data immediately</strong>: The storage node immediately returns the latest committed value of the item, regardless of whether a prepared transaction may later update it.</p>
</li>
<li><p><strong>Timestamp assignment</strong>: This read operation is assigned a timestamp that is after the last write operation’s timestamp but before the prepared transaction’s commit timestamp. This ensures the read operation is serializable, meaning it is placed between the last completed write and the pending write.</p>
</li>
</ul>
</li>
<li><p><strong>Individual Item Write Operations</strong></p>
<p>In most cases, DynamoDB allows individual item write operations to be executed immediately, often before prepared transactions:</p>
<ul>
<li><p><strong>Directly routed to the storage node</strong>: Non-transactional <code>PutItem</code> and other modification operations are routed directly to the storage node, bypassing the transaction coordinator.</p>
</li>
<li><p><strong>Timestamp ordering</strong>: The storage node assigns a timestamp to the write operation that is typically earlier than any prepared transactions (since those have not yet written).</p>
</li>
<li><p><strong>Exceptions</strong>: If a prepared transaction includes a condition check on the item (e.g., checking a bank account balance), the system will not allow a new write to bypass the prepared transaction. For example, if a transaction is checking that there are enough funds to withdraw $100, a new transaction cannot make a withdrawal or delete the item during that check.</p>
</li>
</ul>
</li>
<li><p><strong>Delayed Execution of Write Operations</strong></p>
<p>In certain scenarios, the system can delay write operations instead of rejecting them:</p>
<ul>
<li><p><strong>Buffering writes</strong>: If a new write operation conflicts with a prepared transaction’s conditions (e.g., by modifying the item’s state), the storage node can buffer the write operation in a queue until the prepared transaction is complete. This prevents the need to reject the write and require the client to resubmit it.</p>
</li>
<li><p><strong>Processing buffered writes after the transaction completes</strong>: Once the prepared transaction completes (committed or canceled), the buffered write can be assigned a new timestamp and executed. Typically, the delay caused by waiting for the transaction to complete is short, so this strategy doesn’t significantly increase latency.</p>
</li>
<li><p><strong>Unconditional writes</strong>: If the storage node receives a <code>PutItem</code> or <code>DeleteItem</code> operation without any preconditions, these operations can be executed immediately. They are assigned a timestamp later than any prepared transactions, ensuring the correctness of transactions. If a previously prepared transaction is committed with an earlier timestamp, its write operations will be ignored.</p>
</li>
</ul>
</li>
<li><p><strong>Write Transactions with Older Timestamps</strong></p>
<p>DynamoDB supports accepting write transactions with older timestamps:</p>
<ul>
<li><p><strong>Handling after already committed writes</strong>: If a write transaction with an older timestamp arrives at a storage node where a later write has already been processed, the node can still accept the older transaction and mark it as prepared. If the transaction is eventually committed, its write will be ignored, as the earlier write has already been overwritten by the newer one.</p>
</li>
<li><p><strong>Exceptions for partial updates</strong>: This rule applies to full overwrites of data items (like <code>PutItem</code>), but not to partial updates (like <code>UpdateItem</code>). If the last write was a partial update, the operations must be executed in strict timestamp order to ensure correctness.</p>
</li>
</ul>
</li>
<li><p><strong>Multiple Transactions Writing to the Same Item</strong></p>
<p>DynamoDB allows multiple transactions to simultaneously prepare to write the same data item:</p>
<ul>
<li><p><strong>Simultaneous transaction preparation</strong>: For a given item, a series of transactions can enter the prepared state simultaneously, without waiting for the previous transaction to commit. This increases concurrency and allows multiple transactions to proceed in parallel.</p>
</li>
<li><p><strong>Order of transaction commits</strong>: If the write operations are full item overwrites (like <code>PutItem</code> or <code>DeleteItem</code>), the transactions can be committed in any order, as long as the last <code>PutItem</code> or <code>DeleteItem</code> operation (with the latest timestamp) is the final one executed.</p>
</li>
<li><p><strong>Restrictions for partial updates</strong>: For transactions performing partial updates (like <code>UpdateItem</code>), the transactions must be executed in timestamp order, as the final state of the item depends on the sequence of updates.</p>
</li>
</ul>
</li>
<li><p><strong>Optimized Single-Phase Read Transactions</strong></p>
<p>DynamoDB introduces optimizations for read transactions, allowing certain read transactions to be completed in a single phase without requiring a two-phase commit protocol:</p>
<ul>
<li><p><strong><code>GetItemWithTimestamp</code></strong>: Assuming storage nodes support the <code>GetItemWithTimestamp</code> operation, it allows a read timestamp to be passed as a parameter. This operation returns the latest value of the item, provided its last write timestamp is earlier than the given read timestamp and any prepared transactions have timestamps later than the read timestamp; otherwise, the request is rejected.</p>
</li>
<li><p><strong>Single-phase completion of read transactions</strong>: When a read transaction involves multiple items, the transaction coordinator issues <code>GetItemWithTimestamp</code> requests for each item and buffers the returned values. If all storage nodes accept the requests without conflict, the coordinator can return the buffered values to the client, completing the transaction. If any node rejects the request, the read transaction fails.</p>
</li>
<li><p><strong>Serialization issues</strong>: This optimization is optimistic but can lead to potential serialization issues. If a storage node later accepts a write with a timestamp earlier than a previously executed read transaction, it may cause the transaction to be non-serializable. To avoid this, storage nodes need to track both the last read and write timestamps for each item. Future write transactions must ensure that their timestamps are later than the last read&#x2F;write timestamps of all the items they modify.</p>
</li>
</ul>
</li>
<li><p><strong>Optimizations for Single-Partition Write Transactions</strong></p>
<p>DynamoDB further optimizes write transactions that involve multiple items within a single partition, allowing them to be completed in a single phase without a two-phase commit protocol:</p>
<ul>
<li><p><strong>Single-partition transaction processing</strong>: If all the items being written in a transaction reside within the same partition (and thus are stored on the same storage node), there is no need for separate prepare and commit phases. The storage node can perform all the necessary precondition checks and immediately execute the write operations.</p>
</li>
<li><p><strong>Reduced communication overhead</strong>: This approach significantly reduces the communication overhead between the transaction coordinator and storage nodes, especially in highly concurrent environments, improving system performance.</p>
</li>
</ul>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><p><strong>Why does DynmoDB not use the two-phase locking protocol?</strong> </p>
<p>While two-phase locking is used traditionally to prevent concurrent transactions from reading and writing the same data items, it has drawbacks. Locking <strong>restricts concurrency</strong> and can lead to <strong>deadlocks</strong>. Moreover, it requires <strong>a recovery mechanism</strong> to release locks when an application fails after acquiring locks as part of a transaction but before that transaction commits. To simplify the design and take advantage of low-contention workloads, DynamoDB uses an optimistic concurrency control scheme that avoids locking altogether.</p>
<p><strong>With DynamoDB, what is the role of a transaction coordinator?</strong></p>
<ul>
<li>The <strong>Transaction Coordinator</strong> plays a central role in handling transactions that span multiple items or storage nodes. The TC is responsible for</li>
<li><ul>
<li>breaking down the transaction into individual operations and coordinating these operations across the necessary storage nodes.</li>
<li>ensuring that the operations follow two-phase commit and all parts of the transaction are either completed successfully or rolled back.</li>
<li>assigning timestamps to ensure the correct ordering of operations and managing any potential conflicts between concurrent transactions.</li>
</ul>
</li>
</ul>
<p><strong>Is DynamoDB a relational database management system?</strong></p>
<p>No, DynamoDB is not a relational database management system (RDBMS). It is a NoSQL database, specifically a key-value and document store. Here’s how it differs from an RDBMS:</p>
<ol>
<li><strong>Data Model</strong>: DynamoDB does not use tables with fixed schemas like relational databases. Instead, it stores data as key-value pairs or documents (JSON-like structure). Each item can have different attributes, and there’s no need for predefined schemas.</li>
<li><strong>Relationships</strong>: Relational databases focus on managing relationships between data (using joins, foreign keys, etc.), while DynamoDB is optimized for storing large amounts of data without complex relationships between the data items.</li>
<li><strong>Querying</strong>: RDBMSs typically use <strong>SQL</strong> for querying data, which allows for complex joins and aggregations. DynamoDB uses its own API for querying and does not support SQL natively. While it allows querying by primary key and secondary indexes, it doesn’t support joins.</li>
<li><strong>Consistency and Transactions</strong>: DynamoDB supports <strong>eventual consistency</strong> or <strong>strong consistency</strong> for reads, while traditional relational databases typically ensure strong consistency through ACID transactions. DynamoDB has introduced <strong>transactions</strong>, but they work differently compared to those in relational databases.</li>
<li><strong>Scalability</strong>: DynamoDB is designed for horizontal scalability across distributed systems, allowing it to handle very large amounts of traffic and data by automatically partitioning data. In contrast, RDBMSs are typically vertically scaled and are not as naturally distributed.</li>
</ol>
<p><strong>How is DynamoDB’s transaction coordinator different than Gamma’s scheduler?</strong> </p>
<ul>
<li>DynamoDB’s transaction coordinator uses Optimistic Concurrency Control (OCC) to manage distributed transactions, ensuring atomicity without 2PC, focusing on scalability and performance in a globally distributed system.</li>
<li>Gamma’s scheduler, on the other hand, uses the traditional Two-Phase Locking (2PL) protocol to guarantee strong consistency in a distributed environment, prioritizing strict coordination across nodes.</li>
</ul>
<p><strong>Name one difference between FoundationDB and DynamoDB?</strong></p>
<p>FoundationDB: FoundationDB is a multi-model database that offers a core key-value store as its foundation, but it allows you to build other data models (such as documents, graphs, or relational) on top of this key-value layer. It’s highly flexible and provides transactional support for different types of data models via layers.</p>
<p>DynamoDB: DynamoDB is a NoSQL key-value and document store with a fixed data model designed specifically for highly scalable, distributed environments. It does not offer the flexibility of building different models on top of its architecture and is focused on high-performance operations with automatic scaling.</p>
<p><strong>What partitioning strategy does FoundationDB use to distribute key-value pairs across its StorageServers?</strong></p>
<p>FoundationDB uses a range-based partitioning strategy to distribute key-value pairs across its StorageServers.</p>
<p>Here’s how it works:</p>
<ol>
<li><strong>Key Ranges</strong>: FoundationDB partitions the key-value pairs by dividing the key space into <strong>contiguous ranges</strong>. Each range of keys is assigned to a specific <strong>StorageServer</strong>.</li>
<li><strong>Dynamic Splitting</strong>: The key ranges are <strong>dynamically split</strong> and adjusted based on data distribution and load. If a particular range grows too large or becomes a hotspot due to frequent access, FoundationDB will automatically split that range into smaller sub-ranges and distribute them across multiple <strong>StorageServers</strong> to balance the load.</li>
<li><strong>Data Movement</strong>: When a key range is split or needs to be rebalanced, the corresponding data is migrated from one <strong>StorageServer</strong> to another without manual intervention, ensuring even distribution of data and load across the system.</li>
</ol>
<p><strong>Why do systems such as Nova-LSM separate storage of data from its processing?</strong> </p>
<ul>
<li><strong>Independent Scaling</strong>: Storage and processing resources can scale independently to meet varying load demands.</li>
<li><strong>Resource Optimization</strong>: Storage nodes focus on data persistence and I&#x2F;O performance, while processing nodes handle computation, improving overall resource efficiency.</li>
<li><strong>Fault Tolerance</strong>: Data remains safe in storage even if processing nodes fail, ensuring high availability.</li>
</ul>
<p>Reference: </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/atc23-idziorek.pdf">https://www.usenix.org/system/files/atc23-idziorek.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/atc22-elhemali.pdf">https://www.usenix.org/system/files/atc22-elhemali.pdf</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yihang Wei</p>
  <div class="site-description" itemprop="description">聚焦于个人的学习与成长历程，旨在系统记录在后端开发、数据库等领域的探索与实践。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yihang Wei</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '50%',
  right: '32px',
  left: 'unset',
  time: '0.5s',
  mixColor: '#fff',
  backgroundColor: '#fff',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '明暗',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

</body>
</html>
